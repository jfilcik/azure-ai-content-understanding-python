{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhance Your Analyzer with Labeled Data\n",
    "\n",
    "\n",
    "> #################################################################################\n",
    ">\n",
    "> Note: Currently, this feature is only available when the analyzer scenario is set to `document`.\n",
    ">\n",
    "> #################################################################################\n",
    "\n",
    "Labeled data consists of samples that have been tagged with one or more labels to add context or meaning. This additional information is used to improve the analyzer's performance.\n",
    "\n",
    "In your own projects, you can use [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/quickstart/use-ai-foundry) to annotate your data with the labeling tool.\n",
    "\n",
    "This notebook demonstrates how to create an analyzer using your labeled data and how to analyze your files afterward.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "1. Ensure your Azure AI service is configured by following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. Set environment variables related to training data by following the steps in [Set env for training data](../docs/set_env_for_training_data_and_reference_doc.md) and adding them to the [.env](./.env) file.\n",
    "   - You can either set `TRAINING_DATA_SAS_URL` directly with the SAS URL for your Azure Blob container,\n",
    "   - Or set both `TRAINING_DATA_STORAGE_ACCOUNT_NAME` and `TRAINING_DATA_CONTAINER_NAME` to generate the SAS URL automatically during later steps.\n",
    "   - Also set `TRAINING_DATA_PATH` to specify the folder path within the container where the training data will be uploaded.\n",
    "3. Install the packages required to run the sample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer Template and Local Training Folder Setup\n",
    "In this sample, we define a template for receipts.\n",
    "\n",
    "The training folder should contain a flat (one-level) directory of labeled receipt documents. Each document includes:\n",
    "- The original file (e.g., PDF or image).\n",
    "- A corresponding `labels.json` file with labeled fields.\n",
    "- A corresponding `result.json` file with OCR results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_docs_folder = \"../data/document_training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure Content Understanding Client\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class that contains helper functions. Before the official release of the Content Understanding SDK, please consider it a lightweight SDK.\n",
    ">\n",
    "> Fill in the constants **AZURE_AI_ENDPOINT**, **AZURE_AI_API_VERSION**, and **AZURE_AI_API_KEY** with the information from your Azure AI Service.\n",
    "\n",
    "> ‚ö†Ô∏è Important:\n",
    "You must update the code below to match your Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments and modify those sections accordingly.\n",
    "If you skip this step, the sample may not run correctly.\n",
    "\n",
    "> ‚ö†Ô∏è Note: While using a subscription key works, using a token provider with Azure Active Directory (AAD) is safer and highly recommended for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from typing import Any, Optional\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.storage.blob import ContainerSasPermissions\n",
    "# Add the parent directory to the Python path to import the sample_helper module\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'python'))\n",
    "from content_understanding_client import AzureContentUnderstandingClient\n",
    "from extension.document_processor import DocumentProcessor\n",
    "from extension.sample_helper import save_json_to_file \n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key; only one is required\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set it in your \".env\" file if not using token authentication\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "API_VERSION = \"2025-11-01\"\n",
    "\n",
    "# Create token provider for Azure AD authentication\n",
    "def token_provider():\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    return token.token\n",
    "\n",
    "# Create the Content Understanding client\n",
    "try:\n",
    "    client = AzureContentUnderstandingClient(\n",
    "        endpoint=AZURE_AI_ENDPOINT,\n",
    "        api_version=API_VERSION,\n",
    "        subscription_key=AZURE_AI_API_KEY,\n",
    "        token_provider=token_provider if not AZURE_AI_API_KEY else None,\n",
    "        x_ms_useragent=\"azure-ai-content-understanding-python-sample-ga\"    # The user agent is used for tracking sample usage and does not provide identity information. You can change this if you want to opt out of tracking.\n",
    "    )\n",
    "    credential_type = \"Subscription Key\" if AZURE_AI_API_KEY else \"Azure AD Token\"\n",
    "    print(f\"‚úÖ Client created successfully\")\n",
    "    print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "    print(f\"   Credential: {credential_type}\")\n",
    "    print(f\"   API Version: {API_VERSION}\")\n",
    "except Exception as e:\n",
    "    credential_type = \"Subscription Key\" if AZURE_AI_API_KEY else \"Azure AD Token\"\n",
    "    print(f\"‚ùå Failed to create client\")\n",
    "    print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "    print(f\"   Credential: {credential_type}\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    processor = DocumentProcessor(client)\n",
    "    print(\"‚úÖ DocumentProcessor created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create DocumentProcessor: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model Deployments for Prebuilt Analyzers\n",
    "\n",
    "> **üí° Note:** This step is only required **once per Azure Content Understanding resource**, unless the GPT deployment has been changed. You can skip this section if:\n",
    "> - This configuration has already been run once for your resource, or\n",
    "> - Your administrator has already configured the model deployments for you\n",
    "\n",
    "Before using prebuilt analyzers, you need to configure the default model deployment mappings. This tells Content Understanding which model deployments to use.\n",
    "\n",
    "**Model Requirements:**\n",
    "- **GPT-4.1** - Required for most prebuilt analyzers (e.g., `prebuilt-invoice`, `prebuilt-receipt`, `prebuilt-idDocument`)\n",
    "- **GPT-4.1-mini** - Required for RAG analyzers (e.g., `prebuilt-documentSearch`, `prebuilt-audioSearch`, `prebuilt-videoSearch`)\n",
    "- **text-embedding-3-large** - Required for all prebuilt analyzers that use embeddings\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Deploy **GPT-4.1**, **GPT-4.1-mini**, and **text-embedding-3-large** models in Azure AI Foundry\n",
    "2. Set `GPT_4_1_DEPLOYMENT`, `GPT_4_1_MINI_DEPLOYMENT`, and `TEXT_EMBEDDING_3_LARGE_DEPLOYMENT` in your `.env` file with the deployment names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model deployment names from environment variables\n",
    "GPT_4_1_DEPLOYMENT = os.getenv(\"GPT_4_1_DEPLOYMENT\")\n",
    "GPT_4_1_MINI_DEPLOYMENT = os.getenv(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "TEXT_EMBEDDING_3_LARGE_DEPLOYMENT = os.getenv(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "# Check if required deployments are configured\n",
    "missing_deployments = []\n",
    "if not GPT_4_1_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_DEPLOYMENT\")\n",
    "if not GPT_4_1_MINI_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "if not TEXT_EMBEDDING_3_LARGE_DEPLOYMENT:\n",
    "    missing_deployments.append(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "if missing_deployments:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing required model deployment configuration(s):\")\n",
    "    for deployment in missing_deployments:\n",
    "        print(f\"   - {deployment}\")\n",
    "    print(\"\\n   Prebuilt analyzers require GPT-4.1, GPT-4.1-mini, and text-embedding-3-large deployments.\")\n",
    "    print(\"   Please:\")\n",
    "    print(\"   1. Deploy all three models in Azure AI Foundry\")\n",
    "    print(\"   2. Add the following to notebooks/.env:\")\n",
    "    print(\"      GPT_4_1_DEPLOYMENT=<your-gpt-4.1-deployment-name>\")\n",
    "    print(\"      GPT_4_1_MINI_DEPLOYMENT=<your-gpt-4.1-mini-deployment-name>\")\n",
    "    print(\"      TEXT_EMBEDDING_3_LARGE_DEPLOYMENT=<your-text-embedding-3-large-deployment-name>\")\n",
    "    print(\"   3. Restart the kernel and run this cell again\")\n",
    "else:\n",
    "    print(f\"üìã Configuring default model deployments...\")\n",
    "    print(f\"   GPT-4.1 deployment: {GPT_4_1_DEPLOYMENT}\")\n",
    "    print(f\"   GPT-4.1-mini deployment: {GPT_4_1_MINI_DEPLOYMENT}\")\n",
    "    print(f\"   text-embedding-3-large deployment: {TEXT_EMBEDDING_3_LARGE_DEPLOYMENT}\")\n",
    "    \n",
    "    try:\n",
    "        # Update defaults to map model names to your deployments\n",
    "        result = client.update_defaults({\n",
    "            \"gpt-4.1\": GPT_4_1_DEPLOYMENT,\n",
    "            \"gpt-4.1-mini\": GPT_4_1_MINI_DEPLOYMENT,\n",
    "            \"text-embedding-3-large\": TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Default model deployments configured successfully\")\n",
    "        print(f\"   Model mappings:\")\n",
    "        for model, deployment in result.get(\"modelDeployments\", {}).items():\n",
    "            print(f\"     {model} ‚Üí {deployment}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to configure defaults: {e}\")\n",
    "        print(f\"   This may happen if:\")\n",
    "        print(f\"   - One or more deployment names don't exist in your Azure AI Foundry project\")\n",
    "        print(f\"   - You don't have permission to update defaults\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Labeled Data\n",
    "In this step, we will:\n",
    "- Use the environment variables `TRAINING_DATA_PATH` and SAS URL related variables set in the Prerequisites step.\n",
    "- Attempt to get the SAS URL from the environment variable `TRAINING_DATA_SAS_URL`.\n",
    "- If `TRAINING_DATA_SAS_URL` is not set, try generating it automatically using `TRAINING_DATA_STORAGE_ACCOUNT_NAME` and `TRAINING_DATA_CONTAINER_NAME` environment variables.\n",
    "- Verify that each document file in the local folder has corresponding `.labels.json` and `.result.json` files.\n",
    "- Upload these files to the Azure Blob storage container specified by the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference storage configuration from environment\n",
    "training_data_path = os.getenv(\"TRAINING_DATA_PATH\") or f\"training_data_{uuid.uuid4().hex[:8]}\"\n",
    "training_data_sas_url = os.getenv(\"TRAINING_DATA_SAS_URL\")\n",
    "\n",
    "print(f\"üìã Configuration:\")\n",
    "print(f\"   Training Data Path: {training_data_path}\")\n",
    "print(f\"   Training Data SAS URL: {'<set>' if training_data_sas_url else '<not set>'}\")\n",
    "\n",
    "if not training_data_path.endswith(\"/\"):\n",
    "    training_data_path += \"/\"\n",
    "\n",
    "if not training_data_sas_url:\n",
    "    training_data_storage_account_name = os.getenv(\"TRAINING_DATA_STORAGE_ACCOUNT_NAME\")\n",
    "    training_data_container_name = os.getenv(\"TRAINING_DATA_CONTAINER_NAME\")\n",
    "    \n",
    "    print(f\"   Storage Account Name: {training_data_storage_account_name or '<not set>'}\")\n",
    "    print(f\"   Container Name: {training_data_container_name or '<not set>'}\")\n",
    "\n",
    "    if training_data_storage_account_name and training_data_container_name:\n",
    "        print(f\"\\nüîë Generating SAS URL...\")\n",
    "        # We require \"Write\" permission to upload, modify, or append blobs\n",
    "        training_data_sas_url = processor.generate_container_sas_url(\n",
    "            account_name=training_data_storage_account_name,\n",
    "            container_name=training_data_container_name,\n",
    "            permissions=ContainerSasPermissions(read=True, write=True, list=True),\n",
    "            expiry_hours=1,\n",
    "        )\n",
    "        print(f\"‚úÖ SAS URL generated successfully\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Storage account name or container name not set. Cannot generate SAS URL.\")\n",
    "\n",
    "if training_data_sas_url:\n",
    "    print(f\"\\nüì§ Uploading training data from '{training_docs_folder}'...\")\n",
    "    \n",
    "    # The generate_training_data_on_blob method is async, so we need to run it in an event loop\n",
    "    import asyncio\n",
    "    \n",
    "    # For Jupyter notebooks, we need to handle the event loop properly\n",
    "    try:\n",
    "        # Try to get the current event loop\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # We're in a Jupyter notebook with a running loop\n",
    "            # Use asyncio.ensure_future and wait for it\n",
    "            task = asyncio.ensure_future(processor.generate_training_data_on_blob(\n",
    "                training_docs_folder, training_data_sas_url, training_data_path))\n",
    "            # Wait for the task to complete\n",
    "            await task\n",
    "        else:\n",
    "            # No running loop, use asyncio.run()\n",
    "            asyncio.run(processor.generate_training_data_on_blob(\n",
    "                training_docs_folder, training_data_sas_url, training_data_path))\n",
    "    except RuntimeError:\n",
    "        # No event loop exists, create one\n",
    "        asyncio.run(processor.generate_training_data_on_blob(\n",
    "            training_docs_folder, training_data_sas_url, training_data_path))\n",
    "    \n",
    "    print(f\"‚úÖ Training data upload completed!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error: No SAS URL available. Please set TRAINING_DATA_SAS_URL or provide storage account credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Analyzer with Defined Schema\n",
    "Before creating the analyzer, fill in the constant `ANALYZER_ID` with a relevant name for your task. In this example, we generate a unique suffix so that this cell can be run multiple times to create different analyzers.\n",
    "\n",
    "We use **TRAINING_DATA_SAS_URL** and **TRAINING_DATA_PATH** as set in the [.env](./.env) file and used in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_id = f\"notebooks_sample_analyzer_training_{int(time.time())}\"\n",
    "\n",
    "# Build knowledge sources if we have training data\n",
    "knowledge_sources = None\n",
    "if training_data_sas_url and training_data_path:\n",
    "    print(f\"üìö Configuring knowledge sources with labeled training data...\")\n",
    "    print(f\"   Container SAS URL: <provided>\")\n",
    "    print(f\"   Storage Prefix: {training_data_path}\")\n",
    "    \n",
    "    # Build knowledge source configuration\n",
    "    knowledge_source_config = {\n",
    "        \"kind\": \"labeledData\",\n",
    "        \"containerUrl\": training_data_sas_url,\n",
    "        \"prefix\": training_data_path\n",
    "    }\n",
    "    \n",
    "    # Optionally add file list path if specified\n",
    "    file_list_path = os.getenv(\"CONTENT_UNDERSTANDING_FILE_LIST_PATH\", \"\")\n",
    "    if file_list_path:\n",
    "        knowledge_source_config[\"fileListPath\"] = file_list_path\n",
    "        print(f\"   File List Path: {file_list_path}\")\n",
    "    \n",
    "    knowledge_sources = [knowledge_source_config]\n",
    "    print(f\"‚úÖ Knowledge source configured\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  No training data available - creating analyzer without knowledge sources\")\n",
    "\n",
    "# Define the analyzer as a dictionary\n",
    "content_analyzer = {\n",
    "    \"baseAnalyzerId\": \"prebuilt-document\",\n",
    "    \"description\": \"Extract useful information from receipt with labeled training data\",\n",
    "    \"config\": {\n",
    "        \"returnDetails\": True,\n",
    "        \"enableLayout\": True,\n",
    "        \"enableFormula\": False,\n",
    "        \"estimateFieldSourceAndConfidence\": True\n",
    "    },\n",
    "    \"fieldSchema\": {\n",
    "        \"name\": \"receipt schema\",\n",
    "        \"description\": \"Schema for receipt\",\n",
    "        \"fields\": {\n",
    "            \"MerchantName\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"extract\",\n",
    "                \"description\": \"Name of the merchant\"\n",
    "            },\n",
    "            \"Items\": {\n",
    "                \"type\": \"array\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"List of items purchased\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"method\": \"extract\",\n",
    "                    \"description\": \"Individual item details\",\n",
    "                    \"properties\": {\n",
    "                        \"Quantity\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"method\": \"extract\",\n",
    "                            \"description\": \"Quantity of the item\"\n",
    "                        },\n",
    "                        \"Name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"method\": \"extract\",\n",
    "                            \"description\": \"Name of the item\"\n",
    "                        },\n",
    "                        \"Price\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"method\": \"extract\",\n",
    "                            \"description\": \"Price of the item\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"TotalPrice\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"extract\",\n",
    "                \"description\": \"Total price on the receipt\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"tags\": {\"demo_type\": \"analyzer_training\"},\n",
    "    \"models\": {\n",
    "        \"completion\": \"gpt-4.1\",\n",
    "        \"embedding\": \"text-embedding-3-large\"  # Required when using knowledge sources\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add knowledge sources if available\n",
    "if knowledge_sources:\n",
    "    content_analyzer[\"knowledgeSources\"] = knowledge_sources\n",
    "\n",
    "print(f\"\\nüîß Creating custom analyzer '{analyzer_id}'...\")\n",
    "print(f\"   With knowledge sources: {'Yes' if knowledge_sources else 'No'}\")\n",
    "\n",
    "response = client.begin_create_analyzer(\n",
    "    analyzer_id=analyzer_id,\n",
    "    analyzer_template=content_analyzer,\n",
    ")\n",
    "\n",
    "# Wait for the analyzer to be created\n",
    "print(f\"‚è≥ Waiting for analyzer creation to complete...\")\n",
    "client.poll_result(response)\n",
    "print(f\"‚úÖ Analyzer '{analyzer_id}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Created Analyzer to Extract Document Content\n",
    "After the analyzer is successfully created, you can use it to analyze your input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/receipt.png\"\n",
    "print(f\"üìÑ Reading document file: {file_path}\")\n",
    "\n",
    "# Begin document analysis operation\n",
    "print(f\"üîç Starting document analysis with analyzer '{analyzer_id}'...\")\n",
    "analysis_response = client.begin_analyze_binary(\n",
    "    analyzer_id=analyzer_id,\n",
    "    file_location=file_path,\n",
    ")\n",
    "\n",
    "# Wait for analysis completion\n",
    "print(f\"‚è≥ Waiting for document analysis to complete...\")\n",
    "analysis_result = client.poll_result(analysis_response)\n",
    "print(f\"‚úÖ Document analysis completed successfully!\")\n",
    "\n",
    "# Display results\n",
    "if analysis_result and \"result\" in analysis_result:\n",
    "    result = analysis_result[\"result\"]\n",
    "    contents = result.get(\"contents\", [])\n",
    "    \n",
    "    if contents:\n",
    "        first_content = contents[0]\n",
    "        \n",
    "        # Display markdown content\n",
    "        print(\"\\nüìÑ Markdown Content:\")\n",
    "        print(\"=\" * 50)\n",
    "        markdown = first_content.get(\"markdown\", \"\")\n",
    "        print(markdown[:500] + \"...\" if len(markdown) > 500 else markdown)\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Display extracted fields\n",
    "        print(f\"\\nüìä Analyzer Training Results:\")\n",
    "        fields = first_content.get(\"fields\", {})\n",
    "        if fields:\n",
    "            for field_name, field_value in fields.items():\n",
    "                field_type = field_value.get(\"type\")\n",
    "                print(f\"\\n{field_name}:\")\n",
    "                if field_type == \"string\":\n",
    "                    print(f\"  Value: {field_value.get('valueString')}\")\n",
    "                elif field_type == \"number\":\n",
    "                    print(f\"  Value: {field_value.get('valueNumber')}\")\n",
    "                elif field_type == \"array\":\n",
    "                    print(f\"  Array with {len(field_value.get('valueArray', []))} items:\")\n",
    "                    for idx, item in enumerate(field_value.get('valueArray', []), 1):\n",
    "                        if item.get('type') == 'object':\n",
    "                            print(f\"    Item {idx}:\")\n",
    "                            for key, val in item.get('valueObject', {}).items():\n",
    "                                if val.get('type') == 'string':\n",
    "                                    print(f\"      {key}: {val.get('valueString')}\")\n",
    "                                elif val.get('type') == 'number':\n",
    "                                    print(f\"      {key}: {val.get('valueNumber')}\")\n",
    "        else:\n",
    "            print(\"No fields extracted\")\n",
    "        \n",
    "        # Display content metadata\n",
    "        print(f\"\\nüìã Content Metadata:\")\n",
    "        print(f\"   Category: {first_content.get('category', 'N/A')}\")\n",
    "        print(f\"   Start Page Number: {first_content.get('startPageNumber', 'N/A')}\")\n",
    "        print(f\"   End Page Number: {first_content.get('endPageNumber', 'N/A')}\")\n",
    "        \n",
    "        # Check if this is document content to access document-specific properties\n",
    "        if first_content.get(\"kind\") == \"document\":\n",
    "            print(f\"\\nüìö Document Information:\")\n",
    "            start_page = first_content.get(\"startPageNumber\", 0)\n",
    "            end_page = first_content.get(\"endPageNumber\", 0)\n",
    "            print(f\"Start page: {start_page}\")\n",
    "            print(f\"End page: {end_page}\")\n",
    "            print(f\"Total pages: {end_page - start_page + 1}\")\n",
    "\n",
    "            # Check for pages\n",
    "            pages = first_content.get(\"pages\")\n",
    "            if pages:\n",
    "                print(f\"\\nüìÑ Pages ({len(pages)}):\")\n",
    "                for page in pages:\n",
    "                    unit = first_content.get(\"unit\", \"units\")\n",
    "                    print(f\"  Page {page.get('pageNumber')}: {page.get('width')} x {page.get('height')} {unit}\")\n",
    "\n",
    "            # Check if there are tables in the document\n",
    "            tables = first_content.get(\"tables\")\n",
    "            if tables:\n",
    "                print(f\"\\nüìä Tables ({len(tables)}):\")\n",
    "                for idx, table in enumerate(tables, 1):\n",
    "                    row_count = table.get(\"rowCount\", 0)\n",
    "                    col_count = table.get(\"columnCount\", 0)\n",
    "                    print(f\"  Table {idx}: {row_count} rows x {col_count} columns\")\n",
    "        else:\n",
    "            print(\"\\nüìö Document Information: Not available for this content type\")\n",
    "    else:\n",
    "        print(\"No contents available in analysis result\")\n",
    "    \n",
    "    # Save the analysis result to a file\n",
    "    saved_file_path = save_json_to_file(analysis_result, filename_prefix=\"analyzer_training_result\")\n",
    "    # Print the full analysis result as a JSON string\n",
    "    print(json.dumps(analysis_result, indent=2))\n",
    "else:\n",
    "    print(\"No analysis result available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Existing Analyzer in Content Understanding Service\n",
    "This snippet is optional and is included to prevent test analyzers from remaining in your service. Without deletion, the analyzer will stay in your service and may be reused in subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üóëÔ∏è  Deleting analyzer '{analyzer_id}'...\")\n",
    "client.delete_analyzer(analyzer_id=analyzer_id)\n",
    "print(f\"‚úÖ Analyzer '{analyzer_id}' deleted successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
