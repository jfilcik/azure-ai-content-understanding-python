{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Custom Fields from Your File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use analyzers to extract custom fields from your input files.\n",
    "\n",
    "Content Understanding provides **extensive prebuilt analyzers** ready to use without training. Always start with prebuilt analyzers before building custom solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Ensure Azure AI service is configured following [steps](../README.md#configure-azure-ai-service-resource)\n",
    "2. Install the required packages to run the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure AI Content Understanding Client\n",
    "\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class containing functions to interact with the Content Understanding API. Before the official release of the Content Understanding SDK, it can be regarded as a lightweight SDK. Fill the constant **AZURE_AI_ENDPOINT**, **AZURE_AI_API_VERSION**, **AZURE_AI_API_KEY** with the information from your Azure AI Service.\n",
    "\n",
    "> ‚ö†Ô∏è Important:\n",
    "You must update the code below to match your Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments and modify those sections accordingly.\n",
    "If you skip this step, the sample may not run correctly.\n",
    "\n",
    "> ‚ö†Ô∏è Note: Using a subscription key works, but using a token provider with Azure Active Directory (AAD) is much safer and is highly recommended for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Add the parent directory to the Python path to import the sample_helper module\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'python'))\n",
    "from content_understanding_client import AzureContentUnderstandingClient\n",
    "from extension.document_processor import DocumentProcessor\n",
    "from extension.sample_helper import save_json_to_file \n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key; only one is required\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set it in your \".env\" file if not using token authentication\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "API_VERSION = \"2025-11-01\"\n",
    "\n",
    "# Create token provider for Azure AD authentication\n",
    "def token_provider():\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    return token.token\n",
    "\n",
    "# Create the Content Understanding client\n",
    "client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_ENDPOINT,\n",
    "    api_version=API_VERSION,\n",
    "    subscription_key=AZURE_AI_API_KEY,\n",
    "    token_provider=token_provider if not AZURE_AI_API_KEY else None\n",
    ")\n",
    "print(\"‚úÖ ContentUnderstandingClient created successfully\")\n",
    "\n",
    "try:\n",
    "    processor = DocumentProcessor(client)\n",
    "    print(\"‚úÖ DocumentProcessor created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create DocumentProcessor: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model Deployments for Prebuilt Analyzers\n",
    "\n",
    "> **üí° Note:** This step is only required **once per Azure Content Understanding resource**, unless the GPT deployment has been changed. You can skip this section if:\n",
    "> - This configuration has already been run once for your resource, or\n",
    "> - Your administrator has already configured the model deployments for you\n",
    "\n",
    "Before using prebuilt analyzers, you need to configure the default model deployment mappings. This tells Content Understanding which model deployments to use.\n",
    "\n",
    "**Model Requirements:**\n",
    "- **GPT-4.1** - Required for most prebuilt analyzers (e.g., `prebuilt-invoice`, `prebuilt-receipt`, `prebuilt-idDocument`)\n",
    "- **GPT-4.1-mini** - Required for RAG analyzers (e.g., `prebuilt-documentSearch`, `prebuilt-audioSearch`, `prebuilt-videoSearch`)\n",
    "- **text-embedding-3-large** - Required for all prebuilt analyzers that use embeddings\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Deploy **GPT-4.1**, **GPT-4.1-mini**, and **text-embedding-3-large** models in Azure AI Foundry\n",
    "2. Set `GPT_4_1_DEPLOYMENT`, `GPT_4_1_MINI_DEPLOYMENT`, and `TEXT_EMBEDDING_3_LARGE_DEPLOYMENT` in your `.env` file with the deployment names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model deployment names from environment variables\n",
    "GPT_4_1_DEPLOYMENT = os.getenv(\"GPT_4_1_DEPLOYMENT\")\n",
    "GPT_4_1_MINI_DEPLOYMENT = os.getenv(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "TEXT_EMBEDDING_3_LARGE_DEPLOYMENT = os.getenv(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "# Check if required deployments are configured\n",
    "missing_deployments = []\n",
    "if not GPT_4_1_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_DEPLOYMENT\")\n",
    "if not GPT_4_1_MINI_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "if not TEXT_EMBEDDING_3_LARGE_DEPLOYMENT:\n",
    "    missing_deployments.append(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "if missing_deployments:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing required model deployment configuration(s):\")\n",
    "    for deployment in missing_deployments:\n",
    "        print(f\"   - {deployment}\")\n",
    "    print(\"\\n   Prebuilt analyzers require GPT-4.1, GPT-4.1-mini, and text-embedding-3-large deployments.\")\n",
    "    print(\"   Please:\")\n",
    "    print(\"   1. Deploy all three models in Azure AI Foundry\")\n",
    "    print(\"   2. Add the following to notebooks/.env:\")\n",
    "    print(\"      GPT_4_1_DEPLOYMENT=<your-gpt-4.1-deployment-name>\")\n",
    "    print(\"      GPT_4_1_MINI_DEPLOYMENT=<your-gpt-4.1-mini-deployment-name>\")\n",
    "    print(\"      TEXT_EMBEDDING_3_LARGE_DEPLOYMENT=<your-text-embedding-3-large-deployment-name>\")\n",
    "    print(\"   3. Restart the kernel and run this cell again\")\n",
    "else:\n",
    "    print(f\"üìã Configuring default model deployments...\")\n",
    "    print(f\"   GPT-4.1 deployment: {GPT_4_1_DEPLOYMENT}\")\n",
    "    print(f\"   GPT-4.1-mini deployment: {GPT_4_1_MINI_DEPLOYMENT}\")\n",
    "    print(f\"   text-embedding-3-large deployment: {TEXT_EMBEDDING_3_LARGE_DEPLOYMENT}\")\n",
    "    \n",
    "    try:\n",
    "        # Update defaults to map model names to your deployments\n",
    "        result = client.update_defaults({\n",
    "            \"gpt-4.1\": GPT_4_1_DEPLOYMENT,\n",
    "            \"gpt-4.1-mini\": GPT_4_1_MINI_DEPLOYMENT,\n",
    "            \"text-embedding-3-large\": TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Default model deployments configured successfully\")\n",
    "        print(f\"   Model mappings:\")\n",
    "        for model, deployment in result.get(\"modelDeployments\", {}).items():\n",
    "            print(f\"     {model} ‚Üí {deployment}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to configure defaults: {e}\")\n",
    "        print(f\"   This may happen if:\")\n",
    "        print(f\"   - One or more deployment names don't exist in your Azure AI Foundry project\")\n",
    "        print(f\"   - You don't have permission to update defaults\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Using Prebuilt Analyzers (Recommended Starting Point)\n",
    "\n",
    "## Why Start with Prebuilt Analyzers?\n",
    "\n",
    "Azure AI Content Understanding provides **70+ production-ready prebuilt analyzers** that cover common scenarios across finance, healthcare, legal, tax, and business domains. These analyzers are:\n",
    "\n",
    "- **Immediately Available** - No training, configuration, or customization needed  \n",
    "- **Battle-Tested** - Built on rich knowledge bases of thousands of real-world document examples  \n",
    "- **Continuously Improved** - Regularly updated by Microsoft to handle document variations  \n",
    "- **Cost-Effective** - Save development time and resources by using proven solutions  \n",
    "- **Comprehensive Coverage** - Extensive support for Financial documents (invoices, receipts, bank statements, credit cards), Identity documents (passports, driver licenses, ID cards, health insurance), Tax documents (40+ US tax forms including 1040, W-2, 1099 variants), Mortgage documents (applications, appraisals, disclosures), Business documents (contracts, purchase orders, procurement), and many more specialized scenarios\n",
    "\n",
    "> **Best Practice**: Always explore prebuilt analyzers first. Build custom analyzers only when prebuilt options don't meet your specific requirements.\n",
    "\n",
    "### Complete List of Prebuilt Analyzer Categories\n",
    "\n",
    "**Content Extraction & RAG**\n",
    "- `prebuilt-read`, `prebuilt-layout` - OCR and layout analysis\n",
    "- `prebuilt-documentSearch`, `prebuilt-imageSearch`, `prebuilt-audioSearch`, `prebuilt-videoSearch` - RAG-optimized\n",
    "\n",
    "**Financial Documents**\n",
    "- `prebuilt-invoice`, `prebuilt-receipt`, `prebuilt-creditCard`, `prebuilt-bankStatement.us`, `prebuilt-check.us`, `prebuilt-creditMemo`\n",
    "\n",
    "**Identity & Healthcare**  \n",
    "- `prebuilt-idDocument`, `prebuilt-idDocument.passport`, `prebuilt-healthInsuranceCard.us`\n",
    "\n",
    "**Tax Documents (US)**\n",
    "- 40+ tax form analyzers including `prebuilt-tax.us.1040`, `prebuilt-tax.us.w2`, all 1099 variants, 1098 series, and more\n",
    "\n",
    "**Mortgage Documents (US)**\n",
    "- `prebuilt-mortgage.us.1003`, `prebuilt-mortgage.us.1004`, `prebuilt-mortgage.us.1005`, `prebuilt-mortgage.us.closingDisclosure`\n",
    "\n",
    "**Legal & Business**\n",
    "- `prebuilt-contract`, `prebuilt-procurement`, `prebuilt-purchaseOrder`, `prebuilt-marriageCertificate.us`\n",
    "\n",
    "**Other Specialized**\n",
    "- `prebuilt-utilityBill`, `prebuilt-payStub.us`, and more\n",
    "\n",
    "> **Learn More**: [Complete Prebuilt Analyzers Documentation](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/concepts/prebuilt-analyzers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Analyzers (When Needed)\n",
    "\n",
    "Create custom analyzers only when prebuilt ones don't meet your needs:\n",
    "- Extract fields specific to your business\n",
    "- Process proprietary document types\n",
    "- Customize extraction logic for unique requirements\n",
    "\n",
    "**This notebook demonstrates both approaches:**\n",
    "1. **Part 1**: Using prebuilt analyzers (receipts, invoices)\n",
    "2. **Part 2**: Creating custom analyzers when prebuilt options aren't sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Invoice Field Extraction with Prebuilt Analyzer\n",
    "\n",
    "Let's demonstrate using `prebuilt-invoice` to extract structured data from an invoice PDF. This analyzer automatically identifies vendor information, invoice numbers, dates, line items, totals, taxes, and payment details without any configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_path = '../data/invoice.pdf'\n",
    "invoice_analyzer_id = \"prebuilt-invoice\"\n",
    "\n",
    "print(f\"üîç Analyzing {sample_file_path} with {invoice_analyzer_id}...\")\n",
    "\n",
    "analysis_response = client.begin_analyze_binary(\n",
    "    analyzer_id=invoice_analyzer_id,\n",
    "    file_location=sample_file_path,\n",
    ")\n",
    "\n",
    "# Wait for analysis completion\n",
    "print(f\"‚è≥ Waiting for document analysis to complete...\")\n",
    "analysis_result = client.poll_result(analysis_response)\n",
    "print(f\"‚úÖ Document analysis completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Invoice Analysis Results**\n",
    "\n",
    "Let's examine the extracted fields from the invoice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive results\n",
    "if analysis_result and \"result\" in analysis_result:\n",
    "    result = analysis_result[\"result\"]\n",
    "    contents = result.get(\"contents\", [])\n",
    "    \n",
    "    if contents:\n",
    "        first_content = contents[0]\n",
    "        \n",
    "        # Display extracted fields\n",
    "        fields = first_content.get(\"fields\", {})\n",
    "        print(\"üìä Extracted Fields:\")\n",
    "        print(\"-\" * 80)\n",
    "        if fields:\n",
    "            for field_name, field_value in fields.items():\n",
    "                field_type = field_value.get(\"type\")\n",
    "                if field_type == \"string\":\n",
    "                    print(f\"{field_name}: {field_value.get('valueString')}\")\n",
    "                elif field_type == \"number\":\n",
    "                    print(f\"{field_name}: {field_value.get('valueNumber')}\")\n",
    "                elif field_type == \"date\":\n",
    "                    print(f\"{field_name}: {field_value.get('valueDate')}\")\n",
    "                elif field_type == \"array\":\n",
    "                    print(f\"{field_name} (array with {len(field_value.get('valueArray', []))} items):\")\n",
    "                    for idx, item in enumerate(field_value.get('valueArray', []), 1):\n",
    "                        if item.get('type') == 'object':\n",
    "                            print(f\"  Item {idx}:\")\n",
    "                            for key, val in item.get('valueObject', {}).items():\n",
    "                                if val.get('type') == 'string':\n",
    "                                    print(f\"    {key}: {val.get('valueString')}\")\n",
    "                                elif val.get('type') == 'number':\n",
    "                                    print(f\"    {key}: {val.get('valueNumber')}\")\n",
    "                                # Display confidence and source for nested fields\n",
    "                                if val.get('confidence') is not None:\n",
    "                                    print(f\"      Confidence: {val.get('confidence'):.3f}\")\n",
    "                                if val.get('source'):\n",
    "                                    print(f\"      Bounding Box: {val.get('source')}\")\n",
    "                elif field_type == \"object\":\n",
    "                    print(f\"{field_name}: {field_value.get('valueObject')}\")\n",
    "                \n",
    "                # Display confidence and bounding box for the field\n",
    "                confidence = field_value.get('confidence')\n",
    "                if confidence is not None:\n",
    "                    print(f\"  Confidence: {confidence:.3f}\")\n",
    "                source = field_value.get('source')\n",
    "                if source:\n",
    "                    print(f\"  Bounding Box: {source}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"No fields extracted\")\n",
    "        print()\n",
    "        \n",
    "        # Display content metadata\n",
    "        print(\"üìã Content Metadata:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Kind: {first_content.get('kind')}\")\n",
    "        if first_content.get(\"kind\") == \"document\":\n",
    "            start_page = first_content.get(\"startPageNumber\", 0)\n",
    "            end_page = first_content.get(\"endPageNumber\", 0)\n",
    "            print(f\"Pages: {start_page} - {end_page}\")\n",
    "            print(f\"Total pages: {end_page - start_page + 1}\")\n",
    "        print()\n",
    "    \n",
    "    # Save full result to file\n",
    "    saved_file_path = save_json_to_file(analysis_result, filename_prefix=\"prebuilt_invoice_analysis_result\")\n",
    "    print(f\"üíæ Full analysis result saved. Review the complete JSON at: {saved_file_path}\")\n",
    "else:\n",
    "    print(\"No analysis result available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Receipt Field Extraction with Prebuilt Analyzer\n",
    "\n",
    "Let's demonstrate using `prebuilt-receipt` to extract structured data from a receipt image. This analyzer automatically identifies merchant information, items, totals, taxes, and payment details without any configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_path = '../data/receipt.png'\n",
    "receipt_analyzer_id = \"prebuilt-receipt\"\n",
    "\n",
    "print(f\"üîç Analyzing {sample_file_path} with {receipt_analyzer_id}...\")\n",
    "\n",
    "analysis_response = client.begin_analyze_binary(\n",
    "    analyzer_id=receipt_analyzer_id,\n",
    "    file_location=sample_file_path,\n",
    ")\n",
    "\n",
    "# Wait for analysis completion\n",
    "print(f\"‚è≥ Waiting for document analysis to complete...\")\n",
    "analysis_result = client.poll_result(analysis_response)\n",
    "print(f\"‚úÖ Document analysis completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Receipt Analysis Results**\n",
    "\n",
    "Let's examine the extracted fields from the receipt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the analysis result to a file\n",
    "saved_file_path = save_json_to_file(analysis_result, filename_prefix=\"prebuilt_receipt_analysis_result\")\n",
    "# Print the full analysis result as a JSON string\n",
    "print(json.dumps(analysis_result, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Analyzers\n",
    "\n",
    "Now let's explore creating custom analyzers to extract specific fields tailored to your needs. Custom analyzers allow you to define exactly what information you want to extract and how it should be structured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Analyzer Configuration Components:**\n",
    "\n",
    "- **`baseAnalyzerId`**: Specifies which prebuilt analyzer to inherit from. Available base analyzers:\n",
    "  - **`prebuilt-document`** - For document-based custom analyzers (PDFs, images, Office docs)\n",
    "  - **`prebuilt-audio`** - For audio-based custom analyzers\n",
    "  - **`prebuilt-video`** - For video-based custom analyzers\n",
    "  - **`prebuilt-image`** - For image-based custom analyzers\n",
    "\n",
    "- **`fieldSchema`**: Defines the structured data to extract from content:\n",
    "  - **`fields`**: Object defining each field to extract, with field names as keys\n",
    "  - Each field definition includes:\n",
    "    - **`type`**: Data type (`string`, `number`, `boolean`, `date`, `object`, `array`)\n",
    "    - **`description`**: Clear explanation of the field - acts as a prompt to guide extraction accuracy\n",
    "    - **`method`**: Extraction method to use:\n",
    "      - **`\"extract\"`** - Extract values as they appear in content (literal text extraction). Requires `estimateSourceAndConfidence: true`. Only supported for document analyzers.\n",
    "      - **`\"generate\"`** - Generate values using AI based on content understanding (best for complex fields)\n",
    "      - **`\"classify\"`** - Classify values against predefined categories (use with `enum`)\n",
    "    - **`enum`**: (Optional) Fixed list of possible values for classification\n",
    "    - **`items`**: (For arrays) Defines structure of array elements\n",
    "    - **`properties`**: (For objects) Defines nested field structure\n",
    "\n",
    "- **`config`**: Processing options that control analysis behavior:\n",
    "  - **`returnDetails`**: Include confidence scores, bounding boxes, metadata (default: false)\n",
    "  - **`enableOcr`**: Extract text from images/scans (default: true, document only)\n",
    "  - **`enableLayout`**: Extract layout info like paragraphs, structure (default: true, document only)\n",
    "  - **`estimateFieldSourceAndConfidence`**: Return source locations and confidence for extracted fields (document only)\n",
    "  - **`locales`**: Language codes for transcription (audio/video, e.g., `[\"en-US\"]`)\n",
    "  - **`contentCategories`**: Define categories for classification and segmentation\n",
    "  - **`enableSegment`**: Split content into categorized chunks (document/video)\n",
    "\n",
    "- **`models`**: Specifies which AI models to use:\n",
    "  - **`completion`**: Model for extraction/generation tasks (e.g., `\"gpt-4o\"`, `\"gpt-4o-mini\"`)\n",
    "  - **`embedding`**: Model for embedding tasks when using knowledge bases\n",
    "\n",
    "For complete details, see the [Analyzer Reference Documentation](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/concepts/analyzer-reference).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Analysis\n",
    "\n",
    "Let's start with document analysis by extracting fields from invoices and receipts. This modality is excellent for processing structured documents and extracting key information like amounts, dates, vendor details, and line items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Invoice Field Extraction\n",
    "\n",
    "Let's extract fields from an invoice PDF. This analyzer identifies essential invoice elements such as vendor information, amounts, dates, and line items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create and Run Invoice Analyzer**\n",
    "\n",
    "Now let's create the invoice analyzer and process our sample invoice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "invoice_analyzer_id = f\"notebooks_sample_invoice_extraction_{int(time.time())}\"\n",
    "\n",
    "invoice_analyzer = {\n",
    "    \"baseAnalyzerId\": \"prebuilt-document\",\n",
    "    \"description\": \"Sample invoice analyzer that extracts vendor information, line items, and totals from commercial invoices\",\n",
    "    \"config\": {\n",
    "        \"returnDetails\": True,\n",
    "        \"enableOcr\": True,\n",
    "        \"enableLayout\": True,\n",
    "        \"estimateFieldSourceAndConfidence\": True\n",
    "    },\n",
    "    \"fieldSchema\": {\n",
    "        \"name\": \"InvoiceFields\",\n",
    "        \"fields\": {\n",
    "            \"VendorName\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"extract\",\n",
    "                \"description\": \"Name of the vendor or supplier, typically found in the header section\"\n",
    "            },\n",
    "            \"Items\": {\n",
    "                \"type\": \"array\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"List of items or services on the invoice, typically in a table format\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"Description\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Item or service description\"\n",
    "                        },\n",
    "                        \"Amount\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Line total amount for this item\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"completion\": \"gpt-4.1\"\n",
    "    }\n",
    "}\n",
    "print(f\"{json.dumps(invoice_analyzer, indent=2)}\")\n",
    "# Start the analyzer creation operation\n",
    "response = client.begin_create_analyzer(\n",
    "    analyzer_id=invoice_analyzer_id,\n",
    "    analyzer_template=invoice_analyzer,\n",
    ")\n",
    "\n",
    "# Wait for the analyzer to be created\n",
    "print(f\"‚è≥ Waiting for analyzer creation to complete...\")\n",
    "client.poll_result(response)\n",
    "print(f\"‚úÖ Analyzer '{invoice_analyzer_id}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the custom analyzer with a invoice pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_path = '../data/invoice.pdf'\n",
    "\n",
    "# Begin document analysis operation\n",
    "print(f\"üîç Starting document analysis with analyzer '{invoice_analyzer_id}'...\")\n",
    "analysis_response = client.begin_analyze_binary(\n",
    "    analyzer_id=invoice_analyzer_id,\n",
    "    file_location=sample_file_path,\n",
    ")\n",
    "\n",
    "# Wait for analysis completion\n",
    "print(f\"‚è≥ Waiting for document analysis to complete...\")\n",
    "analysis_result = client.poll_result(analysis_response)\n",
    "print(f\"‚úÖ Document analysis completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Invoice Analysis Results**\n",
    "\n",
    "Let's examine the extracted fields from the invoice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive results\n",
    "if analysis_result and \"result\" in analysis_result:\n",
    "    result = analysis_result[\"result\"]\n",
    "    contents = result.get(\"contents\", [])\n",
    "    \n",
    "    if contents:\n",
    "        first_content = contents[0]\n",
    "        \n",
    "        # Display extracted fields\n",
    "        fields = first_content.get(\"fields\", {})\n",
    "        print(\"üìä Extracted Fields:\")\n",
    "        print(\"-\" * 80)\n",
    "        if fields:\n",
    "            for field_name, field_value in fields.items():\n",
    "                field_type = field_value.get(\"type\")\n",
    "                if field_type == \"string\":\n",
    "                    print(f\"{field_name}: {field_value.get('valueString')}\")\n",
    "                elif field_type == \"number\":\n",
    "                    print(f\"{field_name}: {field_value.get('valueNumber')}\")\n",
    "                elif field_type == \"array\":\n",
    "                    print(f\"{field_name} (array with {len(field_value.get('valueArray', []))} items):\")\n",
    "                    for idx, item in enumerate(field_value.get('valueArray', []), 1):\n",
    "                        if item.get('type') == 'object':\n",
    "                            print(f\"  Item {idx}:\")\n",
    "                            for key, val in item.get('valueObject', {}).items():\n",
    "                                if val.get('type') == 'string':\n",
    "                                    print(f\"    {key}: {val.get('valueString')}\")\n",
    "                                elif val.get('type') == 'number':\n",
    "                                    print(f\"    {key}: {val.get('valueNumber')}\")\n",
    "                elif field_type == \"object\":\n",
    "                    print(f\"{field_name}: {field_value.get('valueObject')}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"No fields extracted\")\n",
    "        print()\n",
    "        \n",
    "        # Display content metadata\n",
    "        print(\"üìã Content Metadata:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Kind: {first_content.get('kind')}\")\n",
    "        print(f\"Pages: {first_content.get('startPageNumber')} - {first_content.get('endPageNumber')}\")\n",
    "        print(f\"Unit: {first_content.get('unit')}\")\n",
    "        print()\n",
    "        \n",
    "    \n",
    "    # Save full result to file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = f\"test_output/invoice_analysis_result_{timestamp}.json\"\n",
    "    os.makedirs(\"test_output\", exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(analysis_result, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Full analysis result saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"No analysis result available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean Up Invoice Analyzer**\n",
    "\n",
    "Clean up the analyzer to manage resources (in production, you would typically keep analyzers for reuse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the created analyzer\n",
    "print(f\"üóëÔ∏è  Deleting analyzer '{invoice_analyzer_id}'...\")\n",
    "client.delete_analyzer(analyzer_id=invoice_analyzer_id)\n",
    "print(f\"‚úÖ Analyzer '{invoice_analyzer_id}' deleted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "üéâ **Congratulations!** You've successfully completed the field extraction tutorial for Azure AI Content Understanding!\n",
    "\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Try Other Notebooks**: \n",
    "  - `content_extraction.ipynb` - Multi-modal content extraction (audio, video, images)\n",
    "  - `conversational_field_extraction.ipynb` - Extract fields from audio conversations\n",
    "  - `management.ipynb` - Advanced analyzer management operations\n",
    "- **Read the Documentation**: Visit the [Azure AI Content Understanding documentation](https://learn.microsoft.com/azure/ai-services/content-understanding/) for comprehensive guides and API references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
