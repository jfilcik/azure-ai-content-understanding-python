{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11049ef0",
   "metadata": {},
   "source": [
    "# Generating Video Chapters\n",
    "\n",
    "This notebook demonstrates how to automatically generate video chapters using **Azure Content Understanding**. By analyzing the video content, the LLM model applies its own reasoning to segment the video into meaningful chapters and scenes, providing concise descriptions and timestamps. This approach enables users to quickly create a structured table of contents for any video, powered by advanced AI understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44bdf4",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "1. Please follow the [README](../README.md#configure-azure-ai-service-resource) to create the essential resource needed for this sample.\n",
    "2. Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa60be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece89d8",
   "metadata": {},
   "source": [
    "## Create Azure AI Content Understanding Client\n",
    "\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class providing functions to interact with the Content Understanding API. Before the official release of the Content Understanding SDK, this acts as a lightweight SDK. Please fill in the constants **AZURE_AI_ENDPOINT** and **AZURE_AI_API_VERSION** with your Azure AI Service information. Optionally, you may provide **AZURE_AI_API_KEY** if your setup requires key-based authentication.\n",
    "\n",
    "> ⚠️ Important:\n",
    "Please update the code below to match your Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments and kindly modify those sections accordingly.\n",
    "Skipping this step may cause the sample to not run correctly.\n",
    "\n",
    "> ⚠️ Note: While using a subscription key works, using a token provider with Azure Active Directory (AAD) is safer and highly recommended for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a726d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based authentication or a subscription key; only one method is required.\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "# IMPORTANT: Please replace with your actual subscription key or set it in the \".env\" file if not using token authentication.\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "AZURE_AI_API_VERSION = os.getenv(\"AZURE_AI_API_VERSION\", \"2025-05-01-preview\")\n",
    "\n",
    "# Add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_ENDPOINT,\n",
    "    api_version=AZURE_AI_API_VERSION,\n",
    "    # IMPORTANT: Comment out token_provider if using subscription key\n",
    "    token_provider=token_provider,\n",
    "    # IMPORTANT: Uncomment this if using subscription key\n",
    "    # subscription_key=AZURE_AI_API_KEY,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/video_chapters\",  # This header is used for sample usage telemetry; please comment out this line if you wish to opt out.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7d414",
   "metadata": {},
   "source": [
    "## File to Analyze\n",
    "\n",
    "Use the following variable to specify the file to analyze. For this tutorial, we will examine a short example video. To try your own videos, replace the file path accordingly. For longer videos, you may need to adjust the `timeout` variable in `poll_result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9fb2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE_PATH = Path(\"../data/FlightSimulator.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfcab1",
   "metadata": {},
   "source": [
    "# 1. Automated Chapter Discovery\n",
    "\n",
    "We will first use Content Understanding and allow the service to interact with an LLM model, enabling it to apply its own reasoning to segment the video into meaningful chapters. It will provide concise descriptions with timestamps.\n",
    "\n",
    "The custom analyzer schema template for this approach is [video_chapters_dynamic.json](../analyzer_templates/video_chapters_dynamic.json). This file defines the schema and configuration for a custom video analyzer that uses AI to dynamically generate chapters and scenes based on its understanding of the video content. The service analyzes the video and identifies the most meaningful chapter divisions, providing start and end timestamps for each. The configuration section allows flexible segmentation, enabling the model to reason about the content and organize chapters appropriately.\n",
    "\n",
    "In this example, we will use the utility class `AzureContentUnderstandingClient` to load the analyzer schema from the template file and submit it to the Azure Content Understanding service. Then, we will analyze the video and generate the desired chapter and scene structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d502b2e",
   "metadata": {},
   "source": [
    "### Load Analyzer Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151faa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_template_path = \"../analyzer_templates/video_chapters_dynamic.json\"\n",
    "with open(analyzer_template_path, 'r') as f:\n",
    "    template_content = json.load(f)\n",
    "    print(json.dumps(template_content, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75e230",
   "metadata": {},
   "source": [
    "### Create and Run Video Chapter Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e52230",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_analyzer_id = \"video_scene_chapter\" + \"_\" + str(uuid.uuid4())\n",
    "\n",
    "print(f\"Creating chapter video analyzer: {video_analyzer_id}\")\n",
    "response = client.begin_create_analyzer(video_analyzer_id, analyzer_template_path=analyzer_template_path)\n",
    "result = client.poll_result(response)\n",
    "print(\"✅ Chapter video analyzer created successfully!\")\n",
    "\n",
    "print(f\"Analyzing chapter video: {VIDEO_FILE_PATH}\")\n",
    "print(\"⏳ Note: Video analysis may take significantly longer than document analysis...\")\n",
    "response = client.begin_analyze(video_analyzer_id, file_location=VIDEO_FILE_PATH)\n",
    "result_json = client.poll_result(response, timeout_seconds=360)  # Video analysis may take longer, please adjust timeout as needed\n",
    "\n",
    "print(\"Video Content Understanding result: \")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85cf38",
   "metadata": {},
   "source": [
    "### Extract Video Content from Analyzed Result\n",
    "Use the following utility to display results for your visual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from python.chapters_utility import ChaptersFormatter\n",
    "\n",
    "# For dynamic chaptering, pass dynamic=True\n",
    "full_html = ChaptersFormatter.format_chapters_output(VIDEO_FILE_PATH, result_json, dynamic=True)\n",
    "display(HTML(full_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3dba6",
   "metadata": {},
   "source": [
    "### [Optional] Clean Up Marketing Video Analyzer\n",
    "\n",
    "Note: In production environments, analyzers are typically kept for reuse rather than deleting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_analyzer(video_analyzer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e9193",
   "metadata": {},
   "source": [
    "## 2. Structured Chapter Creation\n",
    "\n",
    "Alternatively, you can define a specific structure for chaptering that Content Understanding will use to guide the model. When your request includes information detailing clear chapter types and organization, the service can instruct the model to segment the video according to your desired structure, ensuring consistent and predictable chapter creation.\n",
    "\n",
    "The custom analyzer schema template for this approach is [video_chapters_structured.json](../analyzer_templates/video_chapters_structured.json). This file defines the schema and configuration for a custom video analyzer. In this example, it specifies how a video should be segmented into chapters and scenes, including three chapter types: **\"Topic Introduction\"**, **\"Details About the Work Done\"**, and **\"Conclusion or Results\"**. Each segment contains a list of scenes, with each scene described by a short description, start timestamp, and end timestamp. The configuration section controls segmentation behavior and other analysis options, while the fieldSchema section outlines the expected structure of the output, ensuring chapters and scenes are clearly organized and non-overlapping.\n",
    "\n",
    "We will also be using the utility class `AzureContentUnderstandingClient` to load the analyzer schema from the template file and submit it to the Azure Content Understanding service. Then, we will analyze the video and generate the desired chapter and scene structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ef148",
   "metadata": {},
   "source": [
    "### Load Analyzer Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558af576",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_template_path = \"../analyzer_templates/video_chapters_structured.json\"\n",
    "with open(analyzer_template_path, 'r') as f:\n",
    "    template_content = json.load(f)\n",
    "    print(json.dumps(template_content, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee269be2",
   "metadata": {},
   "source": [
    "### Create and Run Video Chapter Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_analyzer_id = \"video_scene_chapter\" + \"_\" + str(uuid.uuid4())\n",
    "\n",
    "print(f\"Creating chapter video analyzer: {video_analyzer_id}\")\n",
    "response = client.begin_create_analyzer(video_analyzer_id, analyzer_template_path=analyzer_template_path)\n",
    "result = client.poll_result(response)\n",
    "print(\"✅ Chapter video analyzer created successfully!\")\n",
    "\n",
    "print(f\"Analyzing chapter video: {VIDEO_FILE_PATH}\")\n",
    "print(\"⏳ Note: Video analysis may take significantly longer than document analysis...\")\n",
    "response = client.begin_analyze(video_analyzer_id, file_location=VIDEO_FILE_PATH)\n",
    "result_json = client.poll_result(response, timeout_seconds=360)  # Video analysis may take longer, please adjust the timeout as needed\n",
    "\n",
    "print(\"Video Content Understanding result: \")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f5f191",
   "metadata": {},
   "source": [
    "### Extract Video Content from Analyzed Result\n",
    "Use the following utility to display results for your visual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b303cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from python.chapters_utility import ChaptersFormatter\n",
    "\n",
    "# For structured chaptering, use the default (dynamic=False)\n",
    "full_html = ChaptersFormatter.format_chapters_output(VIDEO_FILE_PATH, result_json)\n",
    "display(HTML(full_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8577b",
   "metadata": {},
   "source": [
    "### [Optional] Clean Up Marketing Video Analyzer\n",
    "\n",
    "Note: In production environments, analyzers are typically kept for reuse rather than deleting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6576c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_analyzer(video_analyzer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8d5e2",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook has demonstrated how **Azure Content Understanding** can automatically generate meaningful chapters and scenes from video content. By leveraging AI-driven analysis, you can quickly create structured, searchable tables of contents for any video, making it easier to navigate and understand complex material. This approach streamlines video processing and enables exciting new possibilities for content organization and discovery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
