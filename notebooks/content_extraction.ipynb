{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Content from Your File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use the Content Understanding API to extract semantic content from multimodal files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Ensure your Azure AI service is configured by following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. Install the required packages to run this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure AI Content Understanding Client\n",
    "\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class that provides functions to interact with the Content Understanding API. Prior to the official release of the Content Understanding SDK, it serves as a lightweight SDK.\n",
    ">\n",
    "> Fill in the constants **AZURE_AI_ENDPOINT**, **AZURE_AI_API_VERSION**, and **AZURE_AI_API_KEY** with the details from your Azure AI Service.\n",
    "\n",
    "> ‚ö†Ô∏è Important:\n",
    "You must update the code below to use your preferred Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments in the code and modify those sections accordingly.\n",
    "Skipping this step may cause the sample to not run correctly.\n",
    "\n",
    "> ‚ö†Ô∏è Note: While using a subscription key is supported, it is strongly recommended to use a token provider with Azure Active Directory (AAD) for enhanced security in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Optional\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Add the parent directory to the Python path to import the sample_helper module\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'python'))\n",
    "from content_understanding_client import AzureContentUnderstandingClient\n",
    "from extension.sample_helper import save_json_to_file \n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key; only one is required\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set it in your \".env\" file if not using token authentication\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "API_VERSION = \"2025-11-01\"\n",
    "\n",
    "# Create token provider for Azure AD authentication\n",
    "def token_provider():\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    return token.token\n",
    "\n",
    "# Create the Content Understanding client\n",
    "try:\n",
    "    client = AzureContentUnderstandingClient(\n",
    "        endpoint=AZURE_AI_ENDPOINT,\n",
    "        api_version=API_VERSION,\n",
    "        subscription_key=AZURE_AI_API_KEY,\n",
    "        token_provider=token_provider if not AZURE_AI_API_KEY else None,\n",
    "        x_ms_useragent=\"azure-ai-content-understanding-python-sample-ga\"    # The user agent is used for tracking sample usage and does not provide identity information. You can change this if you want to opt out of tracking.\n",
    "    )\n",
    "    credential_type = \"Subscription Key\" if AZURE_AI_API_KEY else \"Azure AD Token\"\n",
    "    print(f\"‚úÖ Client created successfully\")\n",
    "    print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "    print(f\"   Credential: {credential_type}\")\n",
    "    print(f\"   API Version: {API_VERSION}\")\n",
    "except Exception as e:\n",
    "    credential_type = \"Subscription Key\" if AZURE_AI_API_KEY else \"Azure AD Token\"\n",
    "    print(f\"‚ùå Failed to create client\")\n",
    "    print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "    print(f\"   Credential: {credential_type}\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model Deployments for Prebuilt Analyzers\n",
    "\n",
    "> **üí° Note:** This step is only required **once per Azure Content Understanding resource**, unless the GPT deployment has been changed. You can skip this section if:\n",
    "> - This configuration has already been run once for your resource, or\n",
    "> - Your administrator has already configured the model deployments for you\n",
    "\n",
    "Before using prebuilt analyzers, you need to configure the default model deployment mappings. This tells Content Understanding which model deployments to use.\n",
    "\n",
    "**Model Requirements:**\n",
    "- **GPT-4.1** - Required for most prebuilt analyzers (e.g., `prebuilt-invoice`, `prebuilt-receipt`, `prebuilt-idDocument`)\n",
    "- **GPT-4.1-mini** - Required for RAG analyzers (e.g., `prebuilt-documentSearch`, `prebuilt-audioSearch`, `prebuilt-videoSearch`)\n",
    "- **text-embedding-3-large** - Required for all prebuilt analyzers that use embeddings\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Deploy **GPT-4.1**, **GPT-4.1-mini**, and **text-embedding-3-large** models in Azure AI Foundry (see README.md for instructions)\n",
    "2. Set `GPT_4_1_DEPLOYMENT`, `GPT_4_1_MINI_DEPLOYMENT`, and `TEXT_EMBEDDING_3_LARGE_DEPLOYMENT` in your `.env` file with the deployment names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model deployment names from environment variables\n",
    "# Get model deployment names from environment variables\n",
    "GPT_4_1_DEPLOYMENT = os.getenv(\"GPT_4_1_DEPLOYMENT\")\n",
    "GPT_4_1_MINI_DEPLOYMENT = os.getenv(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "TEXT_EMBEDDING_3_LARGE_DEPLOYMENT = os.getenv(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "# Check if required deployments are configured\n",
    "missing_deployments = []\n",
    "if not GPT_4_1_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_DEPLOYMENT\")\n",
    "if not GPT_4_1_MINI_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "if not TEXT_EMBEDDING_3_LARGE_DEPLOYMENT:\n",
    "    missing_deployments.append(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "if missing_deployments:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing required model deployment configuration(s):\")\n",
    "    for deployment in missing_deployments:\n",
    "        print(f\"   - {deployment}\")\n",
    "    print(\"\\n   Prebuilt analyzers require GPT-4.1, GPT-4.1-mini, and text-embedding-3-large deployments.\")\n",
    "    print(\"   Please:\")\n",
    "    print(\"   1. Deploy all three models in Azure AI Foundry\")\n",
    "    print(\"   2. Add the following to notebooks/.env:\")\n",
    "    print(\"      GPT_4_1_DEPLOYMENT=<your-gpt-4.1-deployment-name>\")\n",
    "    print(\"      GPT_4_1_MINI_DEPLOYMENT=<your-gpt-4.1-mini-deployment-name>\")\n",
    "    print(\"      TEXT_EMBEDDING_3_LARGE_DEPLOYMENT=<your-text-embedding-3-large-deployment-name>\")\n",
    "    print(\"   3. Restart the kernel and run this cell again\")\n",
    "else:\n",
    "    print(f\"üìã Configuring default model deployments...\")\n",
    "    print(f\"   GPT-4.1 deployment: {GPT_4_1_DEPLOYMENT}\")\n",
    "    print(f\"   GPT-4.1-mini deployment: {GPT_4_1_MINI_DEPLOYMENT}\")\n",
    "    print(f\"   text-embedding-3-large deployment: {TEXT_EMBEDDING_3_LARGE_DEPLOYMENT}\")\n",
    "    \n",
    "    try:\n",
    "        # Update defaults to map model names to your deployments\n",
    "        result = client.update_defaults({\n",
    "            \"gpt-4.1\": GPT_4_1_DEPLOYMENT,\n",
    "            \"gpt-4.1-mini\": GPT_4_1_MINI_DEPLOYMENT,\n",
    "            \"text-embedding-3-large\": TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Default model deployments configured successfully\")\n",
    "        print(f\"   Model mappings:\")\n",
    "        for model, deployment in result.get(\"modelDeployments\", {}).items():\n",
    "            print(f\"     {model} ‚Üí {deployment}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to configure defaults: {e}\")\n",
    "        print(f\"   This may happen if:\")\n",
    "        print(f\"   - One or more deployment names don't exist in your Azure AI Foundry project\")\n",
    "        print(f\"   - You don't have permission to update defaults\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Content\n",
    "\n",
    "The `prebuilt-documentSearch` analyzer transforms unstructured documents into structured, machine-readable data optimized for retrieval-augmented generation (RAG) and automated workflows. It extracts content and layout elements while preserving document structure and semantic relationships.\n",
    "\n",
    "Key capabilities include:\n",
    "1. **Content Analysis:** Extracts text (printed and handwritten), selection marks, barcodes (12+ types), mathematical formulas (LaTeX), hyperlinks, and annotations.\n",
    "2. **Figure Analysis:** Generates descriptions for images/charts/diagrams, converts charts to Chart.js syntax, and diagrams to Mermaid.js syntax.\n",
    "3. **Structure Analysis:** Identifies paragraphs with contextual roles (title, section heading, page header/footer), detects tables with complex layouts (merged cells, multi-page), and maps hierarchical sections.\n",
    "4. **GitHub Flavored Markdown:** Outputs richly formatted markdown that preserves document structure for LLM comprehension and AI-powered analysis.\n",
    "5. **Broad Format Support:** Processes PDFs, images, Office documents (Word, Excel, PowerPoint), text files (HTML, Markdown), structured files (XML, JSON, CSV), and email formats (EML, MSG).\n",
    "\n",
    "For detailed information about document elements and markdown representation, see [Document elements](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/elements) and [Document markdown](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/markdown).\n",
    "\n",
    "> **Note:** Figure analysis (descriptions and chart/diagram analysis) is only supported for PDF and image file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze document from local file\n",
    "analyzer_sample_file = '../data/invoice.pdf'\n",
    "analyzer_id = 'prebuilt-documentSearch'\n",
    "\n",
    "print(f\"üîç Analyzing {analyzer_sample_file} with {analyzer_id}...\")\n",
    "response = client.begin_analyze_binary(\n",
    "    analyzer_id=analyzer_id,\n",
    "    file_location=analyzer_sample_file,\n",
    ")\n",
    "\n",
    "result = client.poll_result(response)\n",
    "\n",
    "print(\"\\nüìÑ Markdown Content:\")\n",
    "print(\"=\" * 50)\n",
    "# Extract markdown from the first content element\n",
    "contents = result.get(\"result\", {}).get(\"contents\", [])\n",
    "if contents:\n",
    "    content = contents[0]\n",
    "    markdown = content.get(\"markdown\", \"\")\n",
    "    print(markdown)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if this is document content to access document-specific properties\n",
    "if content.get(\"kind\") == \"document\":\n",
    "    document_content = content\n",
    "    print(f\"\\nüìö Document Information:\")\n",
    "    print(f\"Start page: {document_content.get('startPageNumber')}\")\n",
    "    print(f\"End page: {document_content.get('endPageNumber')}\")\n",
    "    print(f\"Total pages: {document_content.get('endPageNumber') - document_content.get('startPageNumber') + 1}\")\n",
    "\n",
    "    # Check for pages\n",
    "    pages = document_content.get(\"pages\")\n",
    "    if pages is not None:\n",
    "        print(f\"\\nüìÑ Pages ({len(pages)}):\")\n",
    "        for i, page in enumerate(pages):\n",
    "            unit = document_content.get(\"unit\", \"units\")\n",
    "            print(f\"  Page {page.get('pageNumber')}: {page.get('width')} x {page.get('height')} {unit}\")\n",
    "\n",
    "    # Check if there are tables in the document\n",
    "    tables = document_content.get(\"tables\")\n",
    "    if tables is not None:\n",
    "        print(f\"\\nüìä Tables ({len(tables)}):\")\n",
    "        table_counter = 1\n",
    "        for table in tables:\n",
    "            row_count = table.get(\"rowCount\")\n",
    "            col_count = table.get(\"columnCount\")\n",
    "            print(f\"  Table {table_counter}: {row_count} rows x {col_count} columns\")\n",
    "            table_counter += 1\n",
    "else:\n",
    "    print(\"\\nüìö Document Information: Not available for this content type\")\n",
    "    \n",
    "# Save the result\n",
    "saved_json_path = save_json_to_file(result, filename_prefix=\"content_analyzers_analyze_binary\")\n",
    "print(f\"\\nüìã Full analysis result saved. Review the complete JSON at: {saved_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Documents from URLs\n",
    "\n",
    "You can also analyze documents directly from publicly accessible URLs without downloading them first. This is useful for processing documents hosted on web servers, cloud storage, or GitHub repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze document from URL\n",
    "document_url = 'https://github.com/Azure-Samples/azure-ai-content-understanding-python/raw/refs/heads/main/data/invoice.pdf'\n",
    "analyzer_id = 'prebuilt-documentSearch'\n",
    "\n",
    "print(f\"üîç Analyzing document from URL: {document_url}\")\n",
    "print(f\"üìä Using analyzer: {analyzer_id}\\n\")\n",
    "\n",
    "response = client.begin_analyze_url(\n",
    "    analyzer_id=analyzer_id,\n",
    "    url=document_url,\n",
    ")\n",
    "\n",
    "result = client.poll_result(response)\n",
    "\n",
    "print(\"\\nüìÑ Markdown Content:\")\n",
    "print(\"=\" * 50)\n",
    "# Extract markdown from the first content element\n",
    "contents = result.get(\"result\", {}).get(\"contents\", [])\n",
    "if contents:\n",
    "    content = contents[0]\n",
    "    markdown = content.get(\"markdown\", \"\")\n",
    "    print(markdown)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if this is document content to access document-specific properties\n",
    "if content.get(\"kind\") == \"document\":\n",
    "    document_content = content\n",
    "    print(f\"\\nüìö Document Information:\")\n",
    "    print(f\"Start page: {document_content.get('startPageNumber')}\")\n",
    "    print(f\"End page: {document_content.get('endPageNumber')}\")\n",
    "    print(f\"Total pages: {document_content.get('endPageNumber') - document_content.get('startPageNumber') + 1}\")\n",
    "\n",
    "    # Check for pages\n",
    "    pages = document_content.get(\"pages\")\n",
    "    if pages is not None:\n",
    "        print(f\"\\nüìÑ Pages ({len(pages)}):\")\n",
    "        for i, page in enumerate(pages):\n",
    "            unit = document_content.get(\"unit\", \"units\")\n",
    "            print(f\"  Page {page.get('pageNumber')}: {page.get('width')} x {page.get('height')} {unit}\")\n",
    "\n",
    "    # Check if there are tables in the document\n",
    "    tables = document_content.get(\"tables\")\n",
    "    if tables is not None:\n",
    "        print(f\"\\nüìä Tables ({len(tables)}):\")\n",
    "        table_counter = 1\n",
    "        for table in tables:\n",
    "            row_count = table.get(\"rowCount\")\n",
    "            col_count = table.get(\"columnCount\")\n",
    "            print(f\"  Table {table_counter}: {row_count} rows x {col_count} columns\")\n",
    "            table_counter += 1\n",
    "else:\n",
    "    print(\"\\nüìö Document Information: Not available for this content type\")\n",
    "    \n",
    "# Save the result\n",
    "saved_json_path = save_json_to_file(result, filename_prefix=\"content_analyzers_url_document\")\n",
    "print(f\"\\nüìã Full analysis result saved. Review the complete JSON at: {saved_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Content\n",
    "The `prebuilt-audioSearch` analyzer provides conversation analysis capabilities for audio files. It automatically transcribes audio content, performs speaker diarization to distinguish between speakers, and generates conversation summaries. The analyzer supports multilingual transcription and outputs transcripts in standard WebVTT format.\n",
    "\n",
    "Key features include:\n",
    "1. **Transcription:** Converts conversational audio into searchable text with sentence-level and word-level timestamps.\n",
    "2. **Speaker Diarization:** Distinguishes between speakers in a conversation, attributing parts of the transcript to specific speakers (e.g., \"Speaker 1\", \"Speaker 2\").\n",
    "3. **Timing Information:** Precise timing data in milliseconds (startTimeMs, endTimeMs) for each phrase, crucial for audio-text synchronization.\n",
    "4. **Summary Generation:** Automatically generates a summary of the conversation for quick understanding.\n",
    "5. **Multilingual Support:** Supports automatic language detection and multilingual transcription across multiple locales.\n",
    "6. **Markdown Output:** Structured markdown format with WebVTT transcripts preserving speaker identification and timing.\n",
    "\n",
    "For detailed information about audio markdown format and capabilities, see [Audio overview](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/audio/overview) and [AudioVisual markdown representation](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/video/markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_sample_file = '../data/audio.wav'\n",
    "analyzer_id = 'prebuilt-audioSearch'\n",
    "\n",
    "# Analyze audio file with the created analyzer\n",
    "print(f\"üîç Analyzing audio file from path: {analyzer_sample_file} with analyzer '{analyzer_id}'...\")\n",
    "\n",
    "# Begin audio analysis operation\n",
    "print(f\"üé¨ Starting audio analysis with analyzer '{analyzer_id}'...\")\n",
    "analysis_response = client.begin_analyze_binary(\n",
    "    analyzer_id=analyzer_id,\n",
    "    file_location=analyzer_sample_file,\n",
    ")\n",
    "\n",
    "# Wait for analysis completion\n",
    "print(f\"‚è≥ Waiting for audio analysis to complete...\")\n",
    "analysis_result = client.poll_result(analysis_response)\n",
    "print(f\"‚úÖ Audio analysis completed successfully!\")\n",
    "\n",
    "print(\"\\nüìÑ Markdown Content:\")\n",
    "print(\"=\" * 50)\n",
    "# Extract markdown from the first content element\n",
    "contents = analysis_result.get(\"result\", {}).get(\"contents\", [])\n",
    "if contents:\n",
    "    content = contents[0]\n",
    "    markdown = content.get(\"markdown\", \"\")\n",
    "    print(markdown)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if this is audio-visual content to access audio-visual properties\n",
    "if content.get(\"kind\") == \"audioVisual\":\n",
    "    audio_visual_content = content\n",
    "    print(\"\\nüéôÔ∏è Audio-Visual Information:\")\n",
    "    \n",
    "    # Basic Audio-Visual Details\n",
    "    try:\n",
    "        start_time = audio_visual_content.get(\"startTimeMs\")\n",
    "        end_time = audio_visual_content.get(\"endTimeMs\")\n",
    "        duration_sec = (end_time - start_time) / 1000\n",
    "        print(f\"Start Time: {start_time} ms\")\n",
    "        print(f\"End Time: {end_time} ms\")\n",
    "        print(f\"Duration: {duration_sec:.2f} seconds\")\n",
    "    except (KeyError, TypeError):\n",
    "        print(\"‚ùå Missing basic audio-visual content details.\")\n",
    "\n",
    "    # Transcript Phrases (limit to 10)\n",
    "    transcript_phrases = audio_visual_content.get(\"transcriptPhrases\", [])\n",
    "    if transcript_phrases:\n",
    "        print(f\"\\nüìù Transcript Phrases ({min(len(transcript_phrases), 10)}):\")\n",
    "        for idx, phrase in enumerate(transcript_phrases[:10]):\n",
    "            print(f\"  {idx + 1}. Speaker: {phrase.get('speaker')}\")\n",
    "            print(f\"     Text: {phrase.get('text')}\")\n",
    "            print(f\"     Start: {phrase.get('startTimeMs')} ms, End: {phrase.get('endTimeMs')} ms\")\n",
    "            confidence = phrase.get('confidence', 0)\n",
    "            print(f\"     Confidence: {confidence:.2%}\")\n",
    "            print(f\"     Locale: {phrase.get('locale')}\")\n",
    "        if len(transcript_phrases) > 10:\n",
    "            print(f\"  ... and {len(transcript_phrases) - 10} more.\")\n",
    "    else:\n",
    "        print(\"\\nüìù No transcript phrases available.\")\n",
    "\n",
    "    # Markdown Preview\n",
    "    if markdown:\n",
    "        print(\"\\nüéµ Markdown Content Preview:\")\n",
    "        print(markdown[:200] + (\"...\" if len(markdown) > 200 else \"\"))\n",
    "    else:\n",
    "        print(\"\\nüéµ No Markdown content available.\")\n",
    "else:\n",
    "    print(\"\\nüéôÔ∏è Audio-Visual Information: Not available for this content type.\")\n",
    "\n",
    "# Save the result\n",
    "saved_json_path = save_json_to_file(analysis_result, filename_prefix=\"content_analyzers_audio\")\n",
    "print(f\"\\nüìã Full analysis result saved. Review the complete JSON at: {saved_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Content\n",
    "The `prebuilt-videoSearch` analyzer provides comprehensive analysis of video content, combining visual frame extraction, audio transcription, and AI-powered insights. It transforms raw video into RAG-ready structured output in both Markdown and JSON formats, enabling applications like media asset management, content categorization, and retrieval-augmented generation.\n",
    "\n",
    "Key features include:\n",
    "1. **Transcription with Diarization:** Converts audio to searchable WebVTT transcripts with speaker identification and multilingual support (same language handling as audio).\n",
    "2. **Key Frame Extraction:** Intelligently extracts representative frames (~1 FPS) from each scene, embedded as markdown image references with timestamps (e.g., `![](keyFrame.400.jpg)`).\n",
    "3. **Shot Detection:** Identifies video segment boundaries aligned with camera cuts and scene transitions, providing timestamps in `cameraShotTimesMs`.\n",
    "4. **Segment-based Analysis:** Analyzes multiple frames per segment to identify actions, events, and themes rather than individual frame analysis.\n",
    "5. **Custom Field Extraction:** Define business-specific fields (brands, categories, sentiment) that the generative model extracts from visual and audio content.\n",
    "6. **Structured Output:** Content organized in GitHub Flavored Markdown with precise temporal alignment and JSON with detailed metadata.\n",
    "\n",
    "For detailed information about video capabilities, elements, and markdown format, see [Video overview](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/video/overview), [Video elements](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/video/elements), and [AudioVisual markdown representation](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/video/markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Utility function to save keyframe images\n",
    "def save_keyframe_image_to_file(\n",
    "    image_content: bytes,\n",
    "    keyframe_id: str,\n",
    "    test_name: str,\n",
    "    test_py_file_dir: str,\n",
    "    identifier: Optional[str] = None,\n",
    "    output_dir: str = \"test_output\",\n",
    ") -> str:\n",
    "    \"\"\"Save keyframe image to output file using pytest naming convention.\n",
    "\n",
    "    Args:\n",
    "        image_content: The binary image content to save\n",
    "        keyframe_id: The keyframe ID (e.g., \"keyframes/733\")\n",
    "        test_name: Name of the test case (e.g., function name)\n",
    "        test_py_file_dir: Directory where pytest files are located\n",
    "        identifier: Optional unique identifier to avoid conflicts (e.g., analyzer_id)\n",
    "        output_dir: Directory name to save the output file (default: \"test_output\")\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved image file\n",
    "\n",
    "    Raises:\n",
    "        OSError: If there are issues creating directory or writing file\n",
    "    \"\"\"\n",
    "    # Generate timestamp and frame ID\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # Extract the frame time from the keyframe path (e.g., \"keyframes/733\" -> \"733\")\n",
    "    if \"/\" in keyframe_id:\n",
    "        frame_id = keyframe_id.split(\"/\")[-1]\n",
    "    else:\n",
    "        # Fallback: use as-is if no slash found\n",
    "        frame_id = keyframe_id\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir_path = os.path.join(test_py_file_dir, output_dir)\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "    # Generate output filename with optional identifier to avoid conflicts\n",
    "    if identifier:\n",
    "        output_filename = f\"{test_name}_{identifier}_{timestamp}_{frame_id}.jpg\"\n",
    "    else:\n",
    "        output_filename = f\"{test_name}_{timestamp}_{frame_id}.jpg\"\n",
    "\n",
    "    saved_file_path = os.path.join(output_dir_path, output_filename)\n",
    "\n",
    "    # Write the image content to file\n",
    "    with open(saved_file_path, \"wb\") as image_file:\n",
    "        image_file.write(image_content)\n",
    "\n",
    "    print(f\"Image file saved to: {saved_file_path}\")\n",
    "    return saved_file_path\n",
    "\n",
    "\n",
    "analyzer_sample_file = '../data/FlightSimulator.mp4'\n",
    "analyzer_id = 'prebuilt-videoSearch'\n",
    "\n",
    "# Analyze video file with the created analyzer\n",
    "print(f\"üîç Analyzing video file from path: {analyzer_sample_file} with analyzer '{analyzer_id}'...\")\n",
    "\n",
    "# Begin video analysis operation\n",
    "print(f\"üé¨ Starting video analysis with analyzer '{analyzer_id}'...\")\n",
    "analysis_response = client.begin_analyze_binary(\n",
    "    analyzer_id=analyzer_id,\n",
    "    file_location=analyzer_sample_file,\n",
    ")\n",
    "\n",
    "# Wait for analysis completion\n",
    "print(f\"‚è≥ Waiting for video analysis to complete...\")\n",
    "analysis_result = client.poll_result(analysis_response)\n",
    "print(f\"‚úÖ Video analysis completed successfully!\")\n",
    "\n",
    "print(\"\\nüìÑ Markdown Content:\")\n",
    "print(\"=\" * 50)\n",
    "# Extract markdown from the first content element\n",
    "contents = analysis_result.get(\"result\", {}).get(\"contents\", [])\n",
    "if contents:\n",
    "    content = contents[0]\n",
    "    markdown = content.get(\"markdown\", \"\")\n",
    "    print(markdown)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if this is video-visual content to access video-visual properties\n",
    "if content.get(\"kind\") == \"audioVisual\":\n",
    "    video_visual_content = content\n",
    "    print(\"\\nüé¨ Video-Visual Information:\")\n",
    "\n",
    "    # Basic Video-Visual Details\n",
    "    try:\n",
    "        start_time = video_visual_content.get(\"startTimeMs\")\n",
    "        end_time = video_visual_content.get(\"endTimeMs\")\n",
    "        duration_sec = (end_time - start_time) / 1000\n",
    "        print(f\"Start Time: {start_time} ms\")\n",
    "        print(f\"End Time: {end_time} ms\")\n",
    "        print(f\"Duration: {duration_sec:.2f} seconds\")\n",
    "    except (KeyError, TypeError):\n",
    "        print(\"‚ùå Missing basic audio-visual content details.\")\n",
    "\n",
    "    # Transcript Phrases (limit to 10)\n",
    "    transcript_phrases = video_visual_content.get(\"transcriptPhrases\", [])\n",
    "    if transcript_phrases:\n",
    "        print(f\"\\nüìù Transcript Phrases ({min(len(transcript_phrases), 10)}):\")\n",
    "        for idx, phrase in enumerate(transcript_phrases[:10]):\n",
    "            print(f\"  {idx + 1}. Speaker: {phrase.get('speaker')}\")\n",
    "            print(f\"     Text: {phrase.get('text')}\")\n",
    "            print(f\"     Start: {phrase.get('startTimeMs')} ms, End: {phrase.get('endTimeMs')} ms\")\n",
    "            confidence = phrase.get('confidence', 0)\n",
    "            print(f\"     Confidence: {confidence:.2%}\")\n",
    "            print(f\"     Locale: {phrase.get('locale')}\")\n",
    "        if len(transcript_phrases) > 10:\n",
    "            print(f\"  ... and {len(transcript_phrases) - 10} more.\")\n",
    "    else:\n",
    "        print(\"\\nüìù No transcript phrases available.\")\n",
    "\n",
    "    # Key Frames (support both keyFrameTimesMs and KeyFrameTimesMs for forward compatibility)\n",
    "    key_frame_times_ms = video_visual_content.get(\"keyFrameTimesMs\") or video_visual_content.get(\"KeyFrameTimesMs\", [])\n",
    "    if key_frame_times_ms:\n",
    "        print(f\"\\nüñºÔ∏è Key Frames ({len(key_frame_times_ms)}):\")\n",
    "        for idx, key_frame_time in enumerate(key_frame_times_ms[:5]):\n",
    "            print(f\"  Frame {idx + 1}: Time {key_frame_time} ms\")\n",
    "        if len(key_frame_times_ms) > 5:\n",
    "            print(f\"  ... and {len(key_frame_times_ms) - 5} more.\")\n",
    "    else:\n",
    "        print(\"\\nüñºÔ∏è No key frame data available.\")\n",
    "\n",
    "    # Markdown Preview\n",
    "    if markdown:\n",
    "        print(\"\\nüéµ Markdown Content Preview:\")\n",
    "        print(markdown[:200] + (\"...\" if len(markdown) > 200 else \"\"))\n",
    "    else:\n",
    "        print(\"\\nüéµ No Markdown content available.\")\n",
    "else:\n",
    "    print(\"\\nüé¨ Video-Visual Information: Not available for this content type.\")\n",
    "\n",
    "# Save the result\n",
    "saved_json_path = save_json_to_file(analysis_result, filename_prefix=\"content_analyzers_video\")\n",
    "print(f\"\\nüìã Full analysis result saved. Review the complete JSON at: {saved_json_path}\")\n",
    "\n",
    "# Keyframe Processing\n",
    "def extract_keyframe_ids(analysis_result: Dict[str, Any]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extract all keyframe IDs from the analysis result.\n",
    "\n",
    "    Args:\n",
    "        analysis_result: The analysis result from the analyzer\n",
    "    Returns:\n",
    "        List of keyframe IDs (e.g., 'keyframes/1000', 'keyframes/2000')\n",
    "    \"\"\"\n",
    "    print(\"Starting keyframe extraction from analysis result...\")\n",
    "    keyframe_ids = []\n",
    "    contents = analysis_result.get(\"result\", {}).get(\"contents\", [])\n",
    "    for idx, content in enumerate(contents):\n",
    "        if content.get(\"kind\") == \"audioVisual\":\n",
    "            print(f\"Found audioVisual content at index {idx}:\")\n",
    "            # Support both keyFrameTimesMs and KeyFrameTimesMs for forward compatibility\n",
    "            key_frame_times_ms = content.get(\"keyFrameTimesMs\") or content.get(\"KeyFrameTimesMs\", [])\n",
    "            if key_frame_times_ms:\n",
    "                print(f\"  Found {len(key_frame_times_ms)} keyframe timestamps\")\n",
    "                for time_ms in key_frame_times_ms:\n",
    "                    keyframe_id = f\"keyframes/{time_ms}\"\n",
    "                    keyframe_ids.append(keyframe_id)\n",
    "            else:\n",
    "                print(f\"  No keyframe timestamps found in this audioVisual content.\")\n",
    "    print(f\"Extracted {len(keyframe_ids)} total keyframe IDs: {keyframe_ids}\")\n",
    "    return keyframe_ids\n",
    "\n",
    "keyframe_ids = extract_keyframe_ids(analysis_result)\n",
    "if keyframe_ids:\n",
    "    print(f\"\\nüñºÔ∏è Downloading {len(keyframe_ids)} keyframe images...\")\n",
    "\n",
    "    files_to_download = keyframe_ids[:min(3, len(keyframe_ids))]\n",
    "    print(f\"Files to download (first {len(files_to_download)}): {files_to_download}\")\n",
    "\n",
    "    for keyframe_id in files_to_download:\n",
    "        print(f\"Getting result file: {keyframe_id}\")\n",
    "\n",
    "        # Get the result file (keyframe image)\n",
    "        image_content = client.get_result_file(\n",
    "            analyze_response=analysis_response,\n",
    "            file_id=keyframe_id,\n",
    "        )\n",
    "\n",
    "        if image_content:\n",
    "            print(f\"Retrieved image file for {keyframe_id} ({len(image_content)} bytes)\")\n",
    "\n",
    "            # Save the image file\n",
    "            saved_file_path = save_keyframe_image_to_file(\n",
    "                image_content=image_content,\n",
    "                keyframe_id=keyframe_id,\n",
    "                test_name=\"content_extraction_video\",\n",
    "                test_py_file_dir=os.getcwd(),\n",
    "                identifier=analyzer_id\n",
    "            )\n",
    "            print(f\"‚úÖ Saved keyframe image to: {saved_file_path}\")\n",
    "        else:\n",
    "            print(f\"‚ùå No image content retrieved for keyframe: {keyframe_id}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No keyframe IDs found in analysis result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully learned how to extract content from multimodal files using Azure Content Understanding! You explored:\n",
    "\n",
    "- **Document extraction** with the `prebuilt-documentSearch` analyzer\n",
    "- **Audio transcription** with speaker diarization using `prebuilt-audioSearch`\n",
    "- **Video analysis** with keyframe extraction using `prebuilt-videoSearch`\n",
    "\n",
    "### Learn More\n",
    "\n",
    "To dive deeper into Azure Content Understanding capabilities:\n",
    "\n",
    "- **[Content Understanding Overview](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/overview)** - Comprehensive introduction to the service\n",
    "- **[What's New](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/whats-new)** - Latest features and updates\n",
    "- **[Content Extraction Guide](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/overview#content-extraction)** - Detailed documentation on extraction capabilities\n",
    "\n",
    "Explore other notebooks in this repository to learn about custom analyzers, field extraction, and advanced scenarios!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
