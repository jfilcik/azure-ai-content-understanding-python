{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Analyzers in Your Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to create a simple analyzer and manage its lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Ensure your Azure AI service is configured following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. Install the required packages to run this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Azure AI Content Understanding Client\n",
    "\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class that provides functions to interact with the Content Understanding API. Before the official release of the Content Understanding SDK, this client serves as a lightweight SDK.\n",
    "\n",
    "> Fill the constants **AZURE_AI_ENDPOINT**, **AZURE_AI_API_VERSION**, and **AZURE_AI_API_KEY** with your Azure AI Service details.\n",
    "\n",
    "> ‚ö†Ô∏è Important:\n",
    "Update the code below to match your Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments and modify those sections accordingly.\n",
    "If you skip this step, the sample might not run correctly.\n",
    "\n",
    "> ‚ö†Ô∏è Note: Using a subscription key works, but using Azure Active Directory (AAD) token-based authentication is more secure and highly recommended for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Optional\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Add the parent directory to the Python path to import the sample_helper module\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'python'))\n",
    "from content_understanding_client import AzureContentUnderstandingClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key; only one is required\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set it in your \".env\" file if not using token authentication\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "API_VERSION = \"2025-11-01\"\n",
    "\n",
    "# Create token provider for Azure AD authentication\n",
    "def token_provider():\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    return token.token\n",
    "\n",
    "# Create the Content Understanding client\n",
    "try:\n",
    "    client = AzureContentUnderstandingClient(\n",
    "        endpoint=AZURE_AI_ENDPOINT,\n",
    "        api_version=API_VERSION,\n",
    "        subscription_key=AZURE_AI_API_KEY,\n",
    "        token_provider=token_provider if not AZURE_AI_API_KEY else None,\n",
    "        x_ms_useragent=\"azure-ai-content-understanding-python-sample-ga\"    # The user agent is used for tracking sample usage and does not provide identity information. You can change this if you want to opt out of tracking.\n",
    "    )\n",
    "    credential_type = \"Subscription Key\" if AZURE_AI_API_KEY else \"Azure AD Token\"\n",
    "    print(f\"‚úÖ Client created successfully\")\n",
    "    print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "    print(f\"   Credential: {credential_type}\")\n",
    "    print(f\"   API Version: {API_VERSION}\")\n",
    "except Exception as e:\n",
    "    credential_type = \"Subscription Key\" if AZURE_AI_API_KEY else \"Azure AD Token\"\n",
    "    print(f\"‚ùå Failed to create client\")\n",
    "    print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "    print(f\"   Credential: {credential_type}\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model Deployments for Prebuilt Analyzers\n",
    "\n",
    "> **üí° Note:** This step is only required **once per Azure Content Understanding resource**, unless the GPT deployment has been changed. You can skip this section if:\n",
    "> - This configuration has already been run once for your resource, or\n",
    "> - Your administrator has already configured the model deployments for you\n",
    "\n",
    "Before using prebuilt analyzers, you need to configure the default model deployment mappings. This tells Content Understanding which model deployments to use.\n",
    "\n",
    "**Model Requirements:**\n",
    "- **GPT-4.1** - Required for most prebuilt analyzers (e.g., `prebuilt-invoice`, `prebuilt-receipt`, `prebuilt-idDocument`)\n",
    "- **GPT-4.1-mini** - Required for RAG analyzers (e.g., `prebuilt-documentSearch`, `prebuilt-audioSearch`, `prebuilt-videoSearch`)\n",
    "- **text-embedding-3-large** - Required for all prebuilt analyzers that use embeddings\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Deploy **GPT-4.1**, **GPT-4.1-mini**, and **text-embedding-3-large** models in Azure AI Foundry\n",
    "2. Set `GPT_4_1_DEPLOYMENT`, `GPT_4_1_MINI_DEPLOYMENT`, and `TEXT_EMBEDDING_3_LARGE_DEPLOYMENT` in your `.env` file with the deployment names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model deployment names from environment variables\n",
    "GPT_4_1_DEPLOYMENT = os.getenv(\"GPT_4_1_DEPLOYMENT\")\n",
    "GPT_4_1_MINI_DEPLOYMENT = os.getenv(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "TEXT_EMBEDDING_3_LARGE_DEPLOYMENT = os.getenv(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "# Check if required deployments are configured\n",
    "missing_deployments = []\n",
    "if not GPT_4_1_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_DEPLOYMENT\")\n",
    "if not GPT_4_1_MINI_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "if not TEXT_EMBEDDING_3_LARGE_DEPLOYMENT:\n",
    "    missing_deployments.append(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "if missing_deployments:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing required model deployment configuration(s):\")\n",
    "    for deployment in missing_deployments:\n",
    "        print(f\"   - {deployment}\")\n",
    "    print(\"\\n   Prebuilt analyzers require GPT-4.1, GPT-4.1-mini, and text-embedding-3-large deployments.\")\n",
    "    print(\"   Please:\")\n",
    "    print(\"   1. Deploy all three models in Azure AI Foundry\")\n",
    "    print(\"   2. Add the following to notebooks/.env:\")\n",
    "    print(\"      GPT_4_1_DEPLOYMENT=<your-gpt-4.1-deployment-name>\")\n",
    "    print(\"      GPT_4_1_MINI_DEPLOYMENT=<your-gpt-4.1-mini-deployment-name>\")\n",
    "    print(\"      TEXT_EMBEDDING_3_LARGE_DEPLOYMENT=<your-text-embedding-3-large-deployment-name>\")\n",
    "    print(\"   3. Restart the kernel and run this cell again\")\n",
    "else:\n",
    "    print(f\"üìã Configuring default model deployments...\")\n",
    "    print(f\"   GPT-4.1 deployment: {GPT_4_1_DEPLOYMENT}\")\n",
    "    print(f\"   GPT-4.1-mini deployment: {GPT_4_1_MINI_DEPLOYMENT}\")\n",
    "    print(f\"   text-embedding-3-large deployment: {TEXT_EMBEDDING_3_LARGE_DEPLOYMENT}\")\n",
    "    \n",
    "    try:\n",
    "        # Update defaults to map model names to your deployments\n",
    "        result = client.update_defaults({\n",
    "            \"gpt-4.1\": GPT_4_1_DEPLOYMENT,\n",
    "            \"gpt-4.1-mini\": GPT_4_1_MINI_DEPLOYMENT,\n",
    "            \"text-embedding-3-large\": TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Default model deployments configured successfully\")\n",
    "        print(f\"   Model mappings:\")\n",
    "        for model, deployment in result.get(\"modelDeployments\", {}).items():\n",
    "            print(f\"     {model} ‚Üí {deployment}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to configure defaults: {e}\")\n",
    "        print(f\"   This may happen if:\")\n",
    "        print(f\"   - One or more deployment names don't exist in your Azure AI Foundry project\")\n",
    "        print(f\"   - You don't have permission to update defaults\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Default Settings\n",
    "\n",
    "You can retrieve the default model deployment mappings configured for your Content Understanding resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    defaults = client.get_defaults()\n",
    "    print(f\"‚úÖ Retrieved default settings\")\n",
    "    \n",
    "    model_deployments = defaults.get(\"modelDeployments\", {})\n",
    "    if model_deployments:\n",
    "        print(f\"\\nüìã Model Deployments:\")\n",
    "        for model_name, deployment_name in model_deployments.items():\n",
    "            print(f\"   {model_name}: {deployment_name}\")\n",
    "    else:\n",
    "        print(\"\\n   No model deployments configured\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error retrieving defaults: {e}\")\n",
    "    print(\"   This is expected if no defaults have been configured yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Simple Analyzer\n",
    "\n",
    "> **üí° Note:** This section demonstrates analyzer creation for learning purposes only. For actual invoice field extraction, we recommend using the **`prebuilt-invoice`** analyzer, which is optimized for invoice processing. See the `field_extraction.ipynb` notebook for examples of using prebuilt analyzers.\n",
    "\n",
    "First, we create an analyzer from a template to extract invoice fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "analyzer_id = f\"notebooks_sample_management_{int(time.time())}\"\n",
    "\n",
    "# Create a custom analyzer using dictionary format\n",
    "print(f\"üîß Creating custom analyzer '{analyzer_id}'...\")\n",
    "\n",
    "call_analyzer = {\n",
    "    \"baseAnalyzerId\": \"prebuilt-callCenter\",\n",
    "    \"description\": \"Sample call recording analytics\",\n",
    "    \"config\": {\n",
    "        \"returnDetails\": True,\n",
    "        \"locales\": [\"en-US\"]\n",
    "    },\n",
    "    \"fieldSchema\": {\n",
    "        \"fields\": {\n",
    "            \"Summary\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"A one-paragraph summary\"\n",
    "            },\n",
    "            \"Topics\": {\n",
    "                \"type\": \"array\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"Top 5 topics mentioned\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"Companies\": {\n",
    "                \"type\": \"array\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"List of companies mentioned\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"People\": {\n",
    "                \"type\": \"array\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"List of people mentioned\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"Name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Person's name\"\n",
    "                        },\n",
    "                        \"Role\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Person's title/role\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"Sentiment\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"classify\",\n",
    "                \"description\": \"Overall sentiment\",\n",
    "                \"enum\": [\n",
    "                    \"Positive\",\n",
    "                    \"Neutral\",\n",
    "                    \"Negative\"\n",
    "                ]\n",
    "            },\n",
    "            \"Categories\": {\n",
    "                \"type\": \"array\",\n",
    "                \"method\": \"classify\",\n",
    "                \"description\": \"List of relevant categories\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\n",
    "                        \"Agriculture\",\n",
    "                        \"Business\",\n",
    "                        \"Finance\",\n",
    "                        \"Health\",\n",
    "                        \"Insurance\",\n",
    "                        \"Mining\",\n",
    "                        \"Pharmaceutical\",\n",
    "                        \"Retail\",\n",
    "                        \"Technology\",\n",
    "                        \"Transportation\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"models\": {\"completion\": \"gpt-4.1\"}\n",
    "}\n",
    "\n",
    "# Start the analyzer creation operation\n",
    "response = client.begin_create_analyzer(\n",
    "    analyzer_id=analyzer_id,\n",
    "    analyzer_template=call_analyzer,\n",
    ")\n",
    "\n",
    "# Wait for the analyzer to be created\n",
    "print(f\"‚è≥ Waiting for analyzer creation to complete...\")\n",
    "client.poll_result(response)\n",
    "print(f\"‚úÖ Analyzer '{analyzer_id}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List All Analyzers in Your Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully creating an analyzer, you can use it to analyze our input files. You can also list all analyzers available in your resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.get_all_analyzers()\n",
    "analyzers = response.get(\"value\", [])\n",
    "\n",
    "print(f\"‚úÖ Found {len(analyzers)} analyzers\")\n",
    "\n",
    "# Display detailed information about each analyzer\n",
    "for i, analyzer in enumerate(analyzers, 1):\n",
    "    print(f\"üîç Analyzer {i}:\")\n",
    "    print(f\"   ID: {analyzer.get('analyzerId')}\")\n",
    "    print(f\"   Description: {analyzer.get('description')}\")\n",
    "    print(f\"   Status: {analyzer.get('status')}\")\n",
    "    print(f\"   Created at: {analyzer.get('createdAt')}\")\n",
    "\n",
    "    # Check if it's a prebuilt analyzer\n",
    "    if analyzer.get('analyzerId', '').startswith(\"prebuilt-\"):\n",
    "        print(f\"   Type: Prebuilt analyzer\")\n",
    "    else:\n",
    "        print(f\"   Type: Custom analyzer\")\n",
    "\n",
    "    # Show tags if available\n",
    "    tags = analyzer.get(\"tags\")\n",
    "    if tags:\n",
    "        print(f\"   Tags: {tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Analyzer Details by ID\n",
    "\n",
    "Keep track of the analyzer ID when you create it. Use the ID to retrieve detailed analyzer definitions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "retrieved_analyzer = client.get_analyzer_detail_by_id(analyzer_id=analyzer_id)\n",
    "print(f\"‚úÖ Analyzer '{analyzer_id}' retrieved successfully!\")\n",
    "print(f\"   Description: {retrieved_analyzer.get('description')}\")\n",
    "print(f\"   Status: {retrieved_analyzer.get('status')}\")\n",
    "print(f\"   Created at: {retrieved_analyzer.get('createdAt')}\")\n",
    "\n",
    "# Print the full analyzer response\n",
    "print(\"\\nüìÑ Full Analyzer Details:\")\n",
    "print(json.dumps(retrieved_analyzer, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete an Analyzer\n",
    "If you no longer need an analyzer, delete it using its ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up: delete the analyzer\n",
    "# Note: You can leave the analyzer for later use if desired\n",
    "print(f\"üóëÔ∏è  Deleting analyzer '{analyzer_id}'...\")\n",
    "client.delete_analyzer(analyzer_id=analyzer_id)\n",
    "print(f\"‚úÖ Analyzer '{analyzer_id}' deleted successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
