{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ae1183",
   "metadata": {},
   "source": [
    "# Legal Transcript Line Number Reflow\n",
    "\n",
    "This notebook demonstrates how to process legal documents (depositions, court transcripts, trial records) with Azure Content Understanding and reflow the output to include inline line numbers.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Legal transcripts have a standardized format with **line numbers in the left margin** (typically 1-25 per page). These line numbers are critical for:\n",
    "- Citing specific testimony in legal briefs\n",
    "- Cross-referencing during depositions and trials\n",
    "- Creating accurate legal summaries\n",
    "\n",
    "By default, Content Understanding's markdown output groups these margin line numbers separately from the main text content. This notebook shows how to **reflow the output** to include line numbers inline with each text line.\n",
    "\n",
    "## Workflow\n",
    "1. **Load PDF** - Read the local legal transcript file\n",
    "2. **Content Extraction** - Use Azure Content Understanding to extract text with position data\n",
    "3. **Reflow** - Match line numbers with text using bounding box coordinates\n",
    "4. **Output** - Generate markdown with inline line numbers (e.g., `1 | witness testimony...`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae8a24",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Ensure your Azure AI service is configured by following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. Install the required packages to run this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1756b078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from -r ../requirements.txt (line 1)) (3.13.2)\n",
      "Requirement already satisfied: azure-identity in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from -r ../requirements.txt (line 2)) (1.20.0)\n",
      "Requirement already satisfied: azure-storage-blob in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from -r ../requirements.txt (line 3)) (12.25.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from -r ../requirements.txt (line 4)) (1.2.1)\n",
      "Requirement already satisfied: requests in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from -r ../requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: Pillow in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from -r ../requirements.txt (line 6)) (12.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from aiohttp->-r ../requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from aiohttp->-r ../requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from aiohttp->-r ../requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from aiohttp->-r ../requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from aiohttp->-r ../requirements.txt (line 1)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from aiohttp->-r ../requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from aiohttp->-r ../requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->-r ../requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from azure-identity->-r ../requirements.txt (line 2)) (1.37.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from azure-identity->-r ../requirements.txt (line 2)) (46.0.3)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from azure-identity->-r ../requirements.txt (line 2)) (1.34.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from azure-identity->-r ../requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from azure-identity->-r ../requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from azure-storage-blob->-r ../requirements.txt (line 3)) (0.7.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from requests->-r ../requirements.txt (line 5)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from requests->-r ../requirements.txt (line 5)) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from requests->-r ../requirements.txt (line 5)) (2025.11.12)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from cryptography>=2.5->azure-identity->-r ../requirements.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=2.5->azure-identity->-r ../requirements.txt (line 2)) (2.23)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\src\\azure-ai-content-understanding-python\\.venv\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity->-r ../requirements.txt (line 2)) (2.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13e310",
   "metadata": {},
   "source": [
    "## Create Azure AI Content Understanding Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6480fbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client created successfully\n",
      "   Endpoint: https://mmi-usw3-eft-foundry.services.ai.azure.com/\n",
      "   Credential: Subscription Key\n",
      "   API Version: 2025-11-01\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Optional\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Add the parent directory to the Python path to import the helper modules\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'python'))\n",
    "from content_understanding_client import AzureContentUnderstandingClient\n",
    "from extension.sample_helper import save_json_to_file \n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "API_VERSION = \"2025-11-01\"\n",
    "\n",
    "# Create token provider for Azure AD authentication\n",
    "def token_provider():\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    return token.token\n",
    "\n",
    "# Create the Content Understanding client\n",
    "try:\n",
    "    client = AzureContentUnderstandingClient(\n",
    "        endpoint=AZURE_AI_ENDPOINT,\n",
    "        api_version=API_VERSION,\n",
    "        subscription_key=AZURE_AI_API_KEY,\n",
    "        token_provider=token_provider if not AZURE_AI_API_KEY else None,\n",
    "        x_ms_useragent=\"azure-ai-content-understanding-python-sample-legal-reflow\"\n",
    "    )\n",
    "    credential_type = \"Subscription Key\" if AZURE_AI_API_KEY else \"Azure AD Token\"\n",
    "    print(f\"‚úÖ Client created successfully\")\n",
    "    print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "    print(f\"   Credential: {credential_type}\")\n",
    "    print(f\"   API Version: {API_VERSION}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create client: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb5f26a",
   "metadata": {},
   "source": [
    "## Configure Model Deployments\n",
    "\n",
    "> **üí° Note:** This step is only required **once per Azure Content Understanding resource**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4941027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuring default model deployments...\n",
      "‚úÖ Default model deployments configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Get model deployment names from environment variables\n",
    "GPT_4_1_DEPLOYMENT = os.getenv(\"GPT_4_1_DEPLOYMENT\")\n",
    "GPT_4_1_MINI_DEPLOYMENT = os.getenv(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "TEXT_EMBEDDING_3_LARGE_DEPLOYMENT = os.getenv(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "# Check if required deployments are configured\n",
    "missing_deployments = []\n",
    "if not GPT_4_1_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_DEPLOYMENT\")\n",
    "if not GPT_4_1_MINI_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "if not TEXT_EMBEDDING_3_LARGE_DEPLOYMENT:\n",
    "    missing_deployments.append(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "if missing_deployments:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing model deployment configuration(s): {missing_deployments}\")\n",
    "    print(\"   Add these to your .env file and restart the kernel.\")\n",
    "else:\n",
    "    print(f\"üìã Configuring default model deployments...\")\n",
    "    try:\n",
    "        result = client.update_defaults({\n",
    "            \"gpt-4.1\": GPT_4_1_DEPLOYMENT,\n",
    "            \"gpt-4.1-mini\": GPT_4_1_MINI_DEPLOYMENT,\n",
    "            \"text-embedding-3-large\": TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\n",
    "        })\n",
    "        print(f\"‚úÖ Default model deployments configured successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to configure defaults: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8544696",
   "metadata": {},
   "source": [
    "## Analyze Legal Transcript\n",
    "\n",
    "We'll use a publicly available deposition transcript from the Internet Archive. This is a real legal document with the standard line-numbered format used in depositions.\n",
    "\n",
    "**Sample Document:** [Farr Deposition Transcript](https://archive.org/details/799436-farr-deposition-transcript) (15 pages, Public Domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18932e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing legal transcript from local file...\n",
      "   Document: c:\\src\\azure-ai-content-understanding-python\\data\\legal_examples\\Trascript Example.pdf\n",
      "   Analyzer: prebuilt-documentSearch\n",
      "   File size: 1,666,047 bytes\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AzureContentUnderstandingClient.begin_analyze_binary() got an unexpected keyword argument 'binary_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   File size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(document_bytes)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m bytes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Analyze the document using binary content\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin_analyze_binary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43manalyzer_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43manalyzer_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbinary_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocument_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mapplication/pdf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚è≥ Waiting for analysis to complete...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m result = client.poll_result(response)\n",
      "\u001b[31mTypeError\u001b[39m: AzureContentUnderstandingClient.begin_analyze_binary() got an unexpected keyword argument 'binary_data'"
     ]
    }
   ],
   "source": [
    "# Analyze legal transcript from local file\n",
    "# Using the transcript example from the data/legal_examples folder\n",
    "document_path = os.path.join(os.path.dirname(os.getcwd()), 'data', 'legal_examples', 'Trascript Example.pdf')\n",
    "analyzer_id = 'prebuilt-documentSearch'\n",
    "\n",
    "print(f\"üîç Analyzing legal transcript from local file...\")\n",
    "print(f\"   Document: {document_path}\")\n",
    "print(f\"   Analyzer: {analyzer_id}\")\n",
    "\n",
    "# Verify file exists\n",
    "if not os.path.exists(document_path):\n",
    "    raise FileNotFoundError(f\"Document not found: {document_path}\")\n",
    "\n",
    "file_size = os.path.getsize(document_path)\n",
    "print(f\"   File size: {file_size:,} bytes\")\n",
    "\n",
    "# Analyze the document using binary file path\n",
    "response = client.begin_analyze_binary(\n",
    "    analyzer_id=analyzer_id,\n",
    "    file_location=document_path\n",
    ")\n",
    "\n",
    "print(f\"‚è≥ Waiting for analysis to complete...\")\n",
    "result = client.poll_result(response)\n",
    "print(f\"‚úÖ Analysis completed!\")\n",
    "\n",
    "# Get document info\n",
    "contents = result.get(\"result\", {}).get(\"contents\", [])\n",
    "if contents:\n",
    "    content = contents[0]\n",
    "    if content.get(\"kind\") == \"document\":\n",
    "        print(f\"\\nüìÑ Document Information:\")\n",
    "        print(f\"   Pages: {content.get('startPageNumber')} - {content.get('endPageNumber')}\")\n",
    "        print(f\"   Total pages: {content.get('endPageNumber') - content.get('startPageNumber') + 1}\")\n",
    "\n",
    "# Save the full result for processing\n",
    "saved_json_path = save_json_to_file(result, filename_prefix=\"legal_transcript_analysis\")\n",
    "print(f\"\\nüíæ Full analysis saved to: {saved_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45dcbdb",
   "metadata": {},
   "source": [
    "## View Default Markdown Output\n",
    "\n",
    "Let's first look at Content Understanding's default markdown output. Notice how the **line numbers are grouped separately** at the bottom of each page's content rather than inline with the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b60915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the default markdown output (first 2000 characters)\n",
    "markdown = content.get(\"markdown\", \"\")\n",
    "\n",
    "print(\"üìÑ Default Markdown Output (first 2000 chars):\")\n",
    "print(\"=\" * 60)\n",
    "print(markdown[:2000])\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n... (Total length: {len(markdown)} characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb101887",
   "metadata": {},
   "source": [
    "## How Reflow Works\n",
    "\n",
    "The reflow algorithm uses **bounding box coordinates** from the JSON output to match line numbers with their corresponding text:\n",
    "\n",
    "### Step 1: Parse Coordinates\n",
    "Every element in CU's JSON has a `source` field with position data:\n",
    "```\n",
    "\"source\": \"D(1,1.0309,1.1277,1.131,1.1277,1.131,1.2711,1.0309,1.2711)\"\n",
    "           D(page, x1,y1, x2,y2, x3,y3, x4,y4)\n",
    "```\n",
    "\n",
    "### Step 2: Group by Vertical Position\n",
    "Elements with similar Y values (within ~0.15 inches) are on the same horizontal line.\n",
    "\n",
    "### Step 3: Sort Left-to-Right\n",
    "Within each group, sort by X coordinate. Line numbers (X ‚âà 1.0\") come before text content (X ‚âà 1.3\"+).\n",
    "\n",
    "### Step 4: Combine\n",
    "Pair line numbers with their corresponding text and output as `N | text content`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13abdb4f",
   "metadata": {},
   "source": [
    "## Reflow Functions\n",
    "\n",
    "Here are the core functions for reflowing Content Understanding output to include inline line numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a421c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LineElement:\n",
    "    \"\"\"Represents a line or element from the document with its position.\"\"\"\n",
    "    content: str\n",
    "    y_position: float  # Top Y coordinate\n",
    "    x_position: float  # Left X coordinate\n",
    "    page_number: int\n",
    "    is_line_number: bool = False\n",
    "\n",
    "\n",
    "def parse_source_coordinates(source: str) -> tuple[int, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Parse the source coordinate string from Content Understanding.\n",
    "    \n",
    "    The source format is: D(pageNumber,x1,y1,x2,y2,x3,y3,x4,y4)\n",
    "    where the points represent a quadrilateral (upper-left, upper-right, lower-right, lower-left)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (page_number, left_x, top_y, right_x, bottom_y)\n",
    "    \"\"\"\n",
    "    match = re.match(r'D\\((\\d+),([^)]+)\\)', source)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid source format: {source}\")\n",
    "    \n",
    "    page_number = int(match.group(1))\n",
    "    coords = [float(x) for x in match.group(2).split(',')]\n",
    "    \n",
    "    if len(coords) == 8:\n",
    "        # Bounding polygon: x1,y1,x2,y2,x3,y3,x4,y4\n",
    "        x1, y1, x2, y2, x3, y3, x4, y4 = coords\n",
    "        left_x = min(x1, x4)\n",
    "        top_y = min(y1, y2)\n",
    "    elif len(coords) == 4:\n",
    "        # Axis-aligned bounding box: left, top, width, height\n",
    "        left_x, top_y, width, height = coords\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected coordinate count: {source}\")\n",
    "    \n",
    "    return page_number, left_x, top_y, 0, 0  # We only need left_x and top_y\n",
    "\n",
    "\n",
    "def is_line_number(content: str) -> bool:\n",
    "    \"\"\"Check if content is a line number (1-99).\"\"\"\n",
    "    return content.strip().isdigit() and 1 <= int(content.strip()) <= 99\n",
    "\n",
    "\n",
    "def is_noise_element(content: str) -> bool:\n",
    "    \"\"\"Check if content is noise (bullets, single dots) that should be filtered.\"\"\"\n",
    "    content = content.strip()\n",
    "    return content in ['¬∑', '‚Ä¢', '‚àô'] or (len(content) == 1 and not content.isalnum())\n",
    "\n",
    "\n",
    "def extract_lines_from_page(page_data: dict) -> list[LineElement]:\n",
    "    \"\"\"Extract all lines from a page with position information.\"\"\"\n",
    "    elements = []\n",
    "    page_number = page_data.get('pageNumber', 1)\n",
    "    \n",
    "    for line in page_data.get('lines', []):\n",
    "        content = line.get('content', '').strip()\n",
    "        source = line.get('source', '')\n",
    "        \n",
    "        if not source or not content or is_noise_element(content):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            parsed_page, left_x, top_y, _, _ = parse_source_coordinates(source)\n",
    "            element = LineElement(\n",
    "                content=content,\n",
    "                y_position=top_y,\n",
    "                x_position=left_x,\n",
    "                page_number=parsed_page,\n",
    "                is_line_number=is_line_number(content)\n",
    "            )\n",
    "            elements.append(element)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    return elements\n",
    "\n",
    "\n",
    "def group_lines_by_vertical_position(elements: list[LineElement], \n",
    "                                      y_tolerance: float = 0.15) -> list[list[LineElement]]:\n",
    "    \"\"\"Group elements that appear on the same horizontal line.\"\"\"\n",
    "    if not elements:\n",
    "        return []\n",
    "    \n",
    "    sorted_elements = sorted(elements, key=lambda e: e.y_position)\n",
    "    groups = []\n",
    "    current_group = [sorted_elements[0]]\n",
    "    current_y = sorted_elements[0].y_position\n",
    "    \n",
    "    for element in sorted_elements[1:]:\n",
    "        if abs(element.y_position - current_y) <= y_tolerance:\n",
    "            current_group.append(element)\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [element]\n",
    "            current_y = element.y_position\n",
    "    \n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "\n",
    "def reflow_page_with_line_numbers(page_data: dict, separator: str = \" | \") -> str:\n",
    "    \"\"\"Reflow a single page's content to include line numbers inline.\"\"\"\n",
    "    elements = extract_lines_from_page(page_data)\n",
    "    if not elements:\n",
    "        return \"\"\n",
    "    \n",
    "    line_groups = group_lines_by_vertical_position(elements)\n",
    "    output_lines = []\n",
    "    \n",
    "    for group in line_groups:\n",
    "        # Sort by X position (left to right)\n",
    "        group.sort(key=lambda e: e.x_position)\n",
    "        \n",
    "        line_numbers = [e for e in group if e.is_line_number]\n",
    "        content_elements = [e for e in group if not e.is_line_number]\n",
    "        \n",
    "        if not content_elements:\n",
    "            continue\n",
    "        \n",
    "        combined_content = ' '.join(e.content for e in content_elements)\n",
    "        \n",
    "        if line_numbers:\n",
    "            line_num = line_numbers[0].content\n",
    "            output_lines.append(f\"{line_num}{separator}{combined_content}\")\n",
    "        else:\n",
    "            output_lines.append(combined_content)\n",
    "    \n",
    "    return '\\n'.join(output_lines)\n",
    "\n",
    "\n",
    "def reflow_document(json_data: dict, target_page: Optional[int] = None, \n",
    "                    separator: str = \" | \") -> str:\n",
    "    \"\"\"Reflow an entire document or specific page with line numbers inline.\"\"\"\n",
    "    contents = json_data.get('result', {}).get('contents', [])\n",
    "    if not contents:\n",
    "        raise ValueError(\"No contents found in JSON data\")\n",
    "    \n",
    "    content = contents[0]\n",
    "    pages = content.get('pages', [])\n",
    "    if not pages:\n",
    "        raise ValueError(\"No pages found in document content\")\n",
    "    \n",
    "    output_parts = []\n",
    "    \n",
    "    for page in pages:\n",
    "        page_number = page.get('pageNumber', 0)\n",
    "        if target_page is not None and page_number != target_page:\n",
    "            continue\n",
    "        \n",
    "        page_output = reflow_page_with_line_numbers(page, separator)\n",
    "        if page_output:\n",
    "            if target_page is None:\n",
    "                output_parts.append(f\"\\n<!-- Page {page_number} -->\\n\")\n",
    "            output_parts.append(page_output)\n",
    "    \n",
    "    return '\\n'.join(output_parts)\n",
    "\n",
    "print(\"‚úÖ Reflow functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d139d",
   "metadata": {},
   "source": [
    "## Reflow a Single Page\n",
    "\n",
    "Let's reflow page 3 of the transcript to see the line numbers inline with the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflow a single page (page 3)\n",
    "page_to_reflow = 3\n",
    "\n",
    "print(f\"üìÑ Reflowed Output for Page {page_to_reflow}:\")\n",
    "print(\"=\" * 60)\n",
    "reflowed_page = reflow_document(result, target_page=page_to_reflow)\n",
    "print(reflowed_page)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a635c8",
   "metadata": {},
   "source": [
    "## Reflow Entire Document\n",
    "\n",
    "Now let's reflow the entire document and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflow the entire document\n",
    "print(\"üìÑ Reflowing entire document...\")\n",
    "reflowed_document = reflow_document(result)\n",
    "\n",
    "# Save to file\n",
    "output_path = os.path.join(os.getcwd(), 'test_output', 'legal_transcript_reflowed.md')\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(reflowed_document)\n",
    "\n",
    "print(f\"‚úÖ Reflowed document saved to: {output_path}\")\n",
    "print(f\"   Total characters: {len(reflowed_document)}\")\n",
    "\n",
    "# Show first 3000 characters\n",
    "print(\"\\nüìÑ Preview (first 3000 characters):\")\n",
    "print(\"=\" * 60)\n",
    "print(reflowed_document[:3000])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae13e7",
   "metadata": {},
   "source": [
    "## Compare: Before vs After\n",
    "\n",
    "Let's compare the default output with the reflowed output for a specific page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare default vs reflowed for page 3\n",
    "page_number = 3\n",
    "\n",
    "# Get the original markdown (extract just page 3 content - approximation)\n",
    "original_lines = markdown.split('\\n')\n",
    "\n",
    "print(\"üìä COMPARISON: Default vs Reflowed Output\")\n",
    "print(\"\\n\" + \"=\" * 30 + \" DEFAULT OUTPUT \" + \"=\" * 30)\n",
    "print(\"(Line numbers grouped separately at page bottom)\")\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# Show a sample of the default output\n",
    "sample_start = 200\n",
    "sample_end = 800\n",
    "print(markdown[sample_start:sample_end])\n",
    "print(\"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 30 + \" REFLOWED OUTPUT \" + \"=\" * 29)\n",
    "print(\"(Line numbers inline with text)\")\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# Show the reflowed output for page 3\n",
    "reflowed_page = reflow_document(result, target_page=page_number)\n",
    "print(reflowed_page[:800])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc1e27",
   "metadata": {},
   "source": [
    "## Using the Standalone Script\n",
    "\n",
    "For batch processing or command-line usage, you can use the standalone script located at `python/reflow_markdown_with_line_numbers.py`:\n",
    "\n",
    "```bash\n",
    "# Process a specific page\n",
    "python python/reflow_markdown_with_line_numbers.py analysis.json --page 3\n",
    "\n",
    "# Process all pages and save to file\n",
    "python python/reflow_markdown_with_line_numbers.py analysis.json --output reflowed.md\n",
    "\n",
    "# Custom separator\n",
    "python python/reflow_markdown_with_line_numbers.py analysis.json --separator \" | \" --output reflowed.md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580894a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run the standalone script on our saved JSON\n",
    "import subprocess\n",
    "\n",
    "script_path = os.path.join(os.path.dirname(os.getcwd()), 'python', 'reflow_markdown_with_line_numbers.py')\n",
    "output_file = os.path.join(os.getcwd(), 'test_output', 'legal_transcript_reflowed_script.md')\n",
    "\n",
    "print(f\"üîß Running standalone reflow script...\")\n",
    "print(f\"   Input: {saved_json_path}\")\n",
    "print(f\"   Output: {output_file}\")\n",
    "\n",
    "result_code = subprocess.run(\n",
    "    ['python', script_path, saved_json_path, '--output', output_file],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result_code.returncode == 0:\n",
    "    print(f\"‚úÖ Script completed successfully!\")\n",
    "    print(result_code.stdout)\n",
    "else:\n",
    "    print(f\"‚ùå Script failed:\")\n",
    "    print(result_code.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0ae60",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Extract content** from legal transcripts using Azure Content Understanding's `prebuilt-documentSearch` analyzer\n",
    "2. **Understand the JSON structure** including the `source` field with bounding box coordinates\n",
    "3. **Reflow the output** to include line numbers inline with text by:\n",
    "   - Parsing bounding box coordinates to determine element positions\n",
    "   - Grouping elements by vertical position (Y coordinate)\n",
    "   - Matching line numbers with their corresponding text content\n",
    "4. **Use the standalone script** for batch processing\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "This technique is valuable for:\n",
    "- **Legal document processing** - Depositions, trial transcripts, court records\n",
    "- **Academic citations** - Line-numbered source materials\n",
    "- **Content indexing** - Building searchable databases with line-level citations\n",
    "- **AI-powered legal research** - RAG applications that need line-accurate references\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- [Content Understanding Document Elements](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/elements)\n",
    "- [Document Markdown Representation](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/markdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
