{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ae1183",
   "metadata": {},
   "source": [
    "# Legal Transcript Line Number Reflow\n",
    "\n",
    "This notebook demonstrates how to process legal documents (depositions, court transcripts, trial records) with Azure Content Understanding and reflow the output to include inline line numbers.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Legal transcripts have a standardized format with **line numbers in the left margin** (typically 1-25 per page). These line numbers are critical for:\n",
    "- Citing specific testimony in legal briefs\n",
    "- Cross-referencing during depositions and trials\n",
    "- Creating accurate legal summaries\n",
    "\n",
    "By default, Content Understanding's markdown output groups these margin line numbers separately from the main text content. This notebook shows how to **reflow the output** to include line numbers inline with each text line.\n",
    "\n",
    "## Workflow\n",
    "1. **Load PDF** - Read the local legal transcript file\n",
    "2. **Content Extraction** - Use Azure Content Understanding to extract text with position data\n",
    "3. **Reflow** - Match line numbers with text using bounding box coordinates\n",
    "4. **Output** - Generate markdown with inline line numbers (e.g., `1 | witness testimony...`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae8a24",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Ensure your Azure AI service is configured by following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. Install the required packages to run this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13e310",
   "metadata": {},
   "source": [
    "## Create Azure AI Content Understanding Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6480fbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client created successfully\n",
      "   Endpoint: https://mmi-usw3-eft-foundry.services.ai.azure.com/\n",
      "   Credential: Subscription Key\n",
      "   API Version: 2025-11-01\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Optional\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Add the parent directory to the Python path to import the helper modules\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'python'))\n",
    "from content_understanding_client import AzureContentUnderstandingClient\n",
    "from extension.sample_helper import save_json_to_file \n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "API_VERSION = \"2025-11-01\"\n",
    "\n",
    "# Create token provider for Azure AD authentication\n",
    "def token_provider():\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    return token.token\n",
    "\n",
    "# Create the Content Understanding client\n",
    "try:\n",
    "    client = AzureContentUnderstandingClient(\n",
    "        endpoint=AZURE_AI_ENDPOINT,\n",
    "        api_version=API_VERSION,\n",
    "        subscription_key=AZURE_AI_API_KEY,\n",
    "        token_provider=token_provider if not AZURE_AI_API_KEY else None,\n",
    "        x_ms_useragent=\"azure-ai-content-understanding-python-sample-legal-reflow\"\n",
    "    )\n",
    "    credential_type = \"Subscription Key\" if AZURE_AI_API_KEY else \"Azure AD Token\"\n",
    "    print(f\"‚úÖ Client created successfully\")\n",
    "    print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "    print(f\"   Credential: {credential_type}\")\n",
    "    print(f\"   API Version: {API_VERSION}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create client: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb5f26a",
   "metadata": {},
   "source": [
    "## Configure Model Deployments\n",
    "\n",
    "> **üí° Note:** This step is only required **once per Azure Content Understanding resource**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4941027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuring default model deployments...\n",
      "‚úÖ Default model deployments configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Get model deployment names from environment variables\n",
    "GPT_4_1_DEPLOYMENT = os.getenv(\"GPT_4_1_DEPLOYMENT\")\n",
    "GPT_4_1_MINI_DEPLOYMENT = os.getenv(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "TEXT_EMBEDDING_3_LARGE_DEPLOYMENT = os.getenv(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "# Check if required deployments are configured\n",
    "missing_deployments = []\n",
    "if not GPT_4_1_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_DEPLOYMENT\")\n",
    "if not GPT_4_1_MINI_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "if not TEXT_EMBEDDING_3_LARGE_DEPLOYMENT:\n",
    "    missing_deployments.append(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "if missing_deployments:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing model deployment configuration(s): {missing_deployments}\")\n",
    "    print(\"   Add these to your .env file and restart the kernel.\")\n",
    "else:\n",
    "    print(f\"üìã Configuring default model deployments...\")\n",
    "    try:\n",
    "        result = client.update_defaults({\n",
    "            \"gpt-4.1\": GPT_4_1_DEPLOYMENT,\n",
    "            \"gpt-4.1-mini\": GPT_4_1_MINI_DEPLOYMENT,\n",
    "            \"text-embedding-3-large\": TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\n",
    "        })\n",
    "        print(f\"‚úÖ Default model deployments configured successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to configure defaults: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8544696",
   "metadata": {},
   "source": [
    "## Analyze Legal Transcript\n",
    "\n",
    "We'll use a publicly available deposition transcript from the Internet Archive. This is a real legal document with the standard line-numbered format used in depositions.\n",
    "\n",
    "**Sample Document:** [Farr Deposition Transcript](https://archive.org/details/799436-farr-deposition-transcript) (15 pages, Public Domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18932e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing legal transcript from local file...\n",
      "   Document: c:\\src\\azure-ai-content-understanding-python\\data\\legal_examples\\Trascript Example.pdf\n",
      "   Analyzer: prebuilt-layout\n",
      "   File size: 1,666,047 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:content_understanding_client:Analyzing binary file c:\\src\\azure-ai-content-understanding-python\\data\\legal_examples\\Trascript Example.pdf with analyzer: prebuilt-layout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Waiting for analysis to complete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:content_understanding_client:Request 9e72763b-ecdf-4b8d-8f9b-a6733d30b6b1 in progress ...\n",
      "INFO:content_understanding_client:Request 9e72763b-ecdf-4b8d-8f9b-a6733d30b6b1 in progress ...\n",
      "INFO:content_understanding_client:Request 9e72763b-ecdf-4b8d-8f9b-a6733d30b6b1 in progress ...\n",
      "INFO:content_understanding_client:Request result is ready after 7.11 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analysis completed!\n",
      "\n",
      "üìÑ Document Information:\n",
      "   Pages: 1 - 52\n",
      "   Total pages: 52\n",
      "üíæ Analysis result saved to: test_output\\legal_transcript_analysis_20260122_162606.json\n",
      "\n",
      "üíæ Full analysis saved to: test_output\\legal_transcript_analysis_20260122_162606.json\n"
     ]
    }
   ],
   "source": [
    "# Analyze legal transcript from local file\n",
    "# Using the transcript example from the data/legal_examples folder\n",
    "document_path = os.path.join(os.path.dirname(os.getcwd()), 'data', 'legal_examples', 'Trascript Example.pdf')\n",
    "analyzer_id = 'prebuilt-layout'\n",
    "\n",
    "print(f\"üîç Analyzing legal transcript from local file...\")\n",
    "print(f\"   Document: {document_path}\")\n",
    "print(f\"   Analyzer: {analyzer_id}\")\n",
    "\n",
    "# Verify file exists\n",
    "if not os.path.exists(document_path):\n",
    "    raise FileNotFoundError(f\"Document not found: {document_path}\")\n",
    "\n",
    "file_size = os.path.getsize(document_path)\n",
    "print(f\"   File size: {file_size:,} bytes\")\n",
    "\n",
    "# Analyze the document using binary file path\n",
    "response = client.begin_analyze_binary(\n",
    "    analyzer_id=analyzer_id,\n",
    "    file_location=document_path\n",
    ")\n",
    "\n",
    "print(f\"‚è≥ Waiting for analysis to complete...\")\n",
    "result = client.poll_result(response)\n",
    "print(f\"‚úÖ Analysis completed!\")\n",
    "\n",
    "# Get document info\n",
    "contents = result.get(\"result\", {}).get(\"contents\", [])\n",
    "if contents:\n",
    "    content = contents[0]\n",
    "    if content.get(\"kind\") == \"document\":\n",
    "        print(f\"\\nüìÑ Document Information:\")\n",
    "        print(f\"   Pages: {content.get('startPageNumber')} - {content.get('endPageNumber')}\")\n",
    "        print(f\"   Total pages: {content.get('endPageNumber') - content.get('startPageNumber') + 1}\")\n",
    "\n",
    "# Save the full result for processing\n",
    "saved_json_path = save_json_to_file(result, filename_prefix=\"legal_transcript_analysis\")\n",
    "print(f\"\\nüíæ Full analysis saved to: {saved_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45dcbdb",
   "metadata": {},
   "source": [
    "## View Default Markdown Output\n",
    "\n",
    "Let's first look at Content Understanding's default markdown output. Notice how the **line numbers are grouped separately** at the bottom of each page's content rather than inline with the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b60915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Default Markdown Output (first 2000 chars):\n",
      "============================================================\n",
      "# (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "SUPERIOR COURT OF NEW JERSEY\n",
      "MERCER COUNTY-LAW DIVISION,\n",
      "DOCKET NO. L-90-2940\n",
      "\n",
      ":\n",
      "\n",
      ":\n",
      "\n",
      "IN RE:\n",
      "IN THE MATTER OF\n",
      "SUSAN MICHAUD\n",
      "\n",
      ":\n",
      "\n",
      "DEPOSITION OF:\n",
      "\n",
      ":\n",
      "\n",
      "Susan Michaud\n",
      "\n",
      ":\n",
      "\n",
      ":\n",
      "\n",
      "Transcript of proceedings taken on July 13, 1990,\n",
      "at 1 pm, at the office of Mason, Griffin & Pierson, 101 Poor\n",
      "Farm Road, Princeton, NJ 08540.\n",
      "\n",
      "682499390\n",
      "\n",
      "<!-- PageFooter: http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocuments.ucsf.edu/docs/khhl0001 -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "\n",
      "# (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "\n",
      "2\n",
      "\n",
      "APPEARANCES\n",
      "\n",
      "On behalf of\n",
      "Susan Michaud:\n",
      "\n",
      "MASON, GRIFFIN & PIERSON\n",
      "BY: Stephanie J. Briody, Esq.\n",
      "101 Poor Farm Road\n",
      "Princton, NJ 08540\n",
      "\n",
      "On behalf of Dr. Alfred\n",
      "Cook, Dr. Charles Howard &\n",
      "Princeton Radiology Assoc.\n",
      "\n",
      "JACKSON & VAURIO\n",
      "BY: John Zen Jackson, Esq.\n",
      "1000 Herrontown Road\n",
      "Princeton, NJ 08540\n",
      "\n",
      "682499391\n",
      "\n",
      "<!-- PageFooter: http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocumƒónts.ucsf.edu/docs/khhl0001 -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "\n",
      "## (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "3\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "\n",
      "INDEX\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>WITNESS:</th>\n",
      "<th>DIRECT</th>\n",
      "<th>CROSS</th>\n",
      "<th>REDIRECT</th>\n",
      "<th>RECROSS</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Susan Michaud</td>\n",
      "<td>4</td>\n",
      "<td>40</td>\n",
      "<td>44</td>\n",
      "<td>44</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      "EXHIBITS:\n",
      "Diagram (P-1)\n",
      "\n",
      "EVIDENCE\n",
      "\n",
      "IDENTIFICATION\n",
      "\n",
      "23\n",
      "\n",
      "682499392\n",
      "\n",
      "<!-- PageFooter: http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001 -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "\n",
      "### (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "4\n",
      "\n",
      "Susan Michaud, M-I-C-H-A-U-D, sworn by the Notary Public,\n",
      "testified as follows.\n",
      "\n",
      "DIRECT EXAMINATION BY\n",
      "\n",
      "MS. BRIODY:\n",
      "\n",
      "Q.\n",
      "Susan, how old are you at the present time?\n",
      "\n",
      "A.\n",
      "Just turned thirty-eight.\n",
      "\n",
      "Q.\n",
      "And are you married?\n",
      "\n",
      "A. Yes, I am.\n",
      "\n",
      "Q.\n",
      "And for how many years have you been married?\n",
      "\n",
      "============================================================\n",
      "\n",
      "... (Total length: 66709 characters)\n"
     ]
    }
   ],
   "source": [
    "# Show the default markdown output (first 2000 characters)\n",
    "markdown = content.get(\"markdown\", \"\")\n",
    "\n",
    "print(\"üìÑ Default Markdown Output (first 2000 chars):\")\n",
    "print(\"=\" * 60)\n",
    "print(markdown[:2000])\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n... (Total length: {len(markdown)} characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb101887",
   "metadata": {},
   "source": [
    "## How Reflow Works\n",
    "\n",
    "The reflow algorithm uses **bounding box coordinates** from the JSON output to match line numbers with their corresponding text:\n",
    "\n",
    "### Step 1: Parse Coordinates\n",
    "Every element in CU's JSON has a `source` field with position data:\n",
    "```\n",
    "\"source\": \"D(1,1.0309,1.1277,1.131,1.1277,1.131,1.2711,1.0309,1.2711)\"\n",
    "           D(page, x1,y1, x2,y2, x3,y3, x4,y4)\n",
    "```\n",
    "\n",
    "### Step 2: Group by Vertical Position\n",
    "Elements with similar Y values (within ~0.15 inches) are on the same horizontal line.\n",
    "\n",
    "### Step 3: Sort Left-to-Right\n",
    "Within each group, sort by X coordinate. Line numbers (X ‚âà 1.0\") come before text content (X ‚âà 1.3\"+).\n",
    "\n",
    "### Step 4: Combine\n",
    "Pair line numbers with their corresponding text and output as `N | text content`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13abdb4f",
   "metadata": {},
   "source": [
    "## Reflow Functions\n",
    "\n",
    "Here are the core functions for reflowing Content Understanding output to include inline line numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a421c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reflow functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LineElement:\n",
    "    \"\"\"Represents a line or element from the document with its position.\"\"\"\n",
    "    content: str\n",
    "    y_position: float  # Top Y coordinate\n",
    "    x_position: float  # Left X coordinate\n",
    "    page_number: int\n",
    "    is_line_number: bool = False\n",
    "\n",
    "\n",
    "def parse_source_coordinates(source: str) -> tuple[int, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Parse the source coordinate string from Content Understanding.\n",
    "    \n",
    "    The source format is: D(pageNumber,x1,y1,x2,y2,x3,y3,x4,y4)\n",
    "    where the points represent a quadrilateral (upper-left, upper-right, lower-right, lower-left)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (page_number, left_x, top_y, right_x, bottom_y)\n",
    "    \"\"\"\n",
    "    match = re.match(r'D\\((\\d+),([^)]+)\\)', source)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid source format: {source}\")\n",
    "    \n",
    "    page_number = int(match.group(1))\n",
    "    coords = [float(x) for x in match.group(2).split(',')]\n",
    "    \n",
    "    if len(coords) == 8:\n",
    "        # Bounding polygon: x1,y1,x2,y2,x3,y3,x4,y4\n",
    "        x1, y1, x2, y2, x3, y3, x4, y4 = coords\n",
    "        left_x = min(x1, x4)\n",
    "        top_y = min(y1, y2)\n",
    "    elif len(coords) == 4:\n",
    "        # Axis-aligned bounding box: left, top, width, height\n",
    "        left_x, top_y, width, height = coords\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected coordinate count: {source}\")\n",
    "    \n",
    "    return page_number, left_x, top_y, 0, 0  # We only need left_x and top_y\n",
    "\n",
    "\n",
    "def is_line_number(content: str) -> bool:\n",
    "    \"\"\"Check if content is a line number (1-99).\"\"\"\n",
    "    return content.strip().isdigit() and 1 <= int(content.strip()) <= 99\n",
    "\n",
    "\n",
    "def is_noise_element(content: str) -> bool:\n",
    "    \"\"\"Check if content is noise (bullets, single dots) that should be filtered.\"\"\"\n",
    "    content = content.strip()\n",
    "    return content in ['¬∑', '‚Ä¢', '‚àô'] or (len(content) == 1 and not content.isalnum())\n",
    "\n",
    "\n",
    "def extract_lines_from_page(page_data: dict) -> list[LineElement]:\n",
    "    \"\"\"Extract all lines from a page with position information.\"\"\"\n",
    "    elements = []\n",
    "    page_number = page_data.get('pageNumber', 1)\n",
    "    \n",
    "    for line in page_data.get('lines', []):\n",
    "        content = line.get('content', '').strip()\n",
    "        source = line.get('source', '')\n",
    "        \n",
    "        if not source or not content or is_noise_element(content):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            parsed_page, left_x, top_y, _, _ = parse_source_coordinates(source)\n",
    "            element = LineElement(\n",
    "                content=content,\n",
    "                y_position=top_y,\n",
    "                x_position=left_x,\n",
    "                page_number=parsed_page,\n",
    "                is_line_number=is_line_number(content)\n",
    "            )\n",
    "            elements.append(element)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    return elements\n",
    "\n",
    "\n",
    "def group_lines_by_vertical_position(elements: list[LineElement], \n",
    "                                      y_tolerance: float = 0.15) -> list[list[LineElement]]:\n",
    "    \"\"\"Group elements that appear on the same horizontal line.\"\"\"\n",
    "    if not elements:\n",
    "        return []\n",
    "    \n",
    "    sorted_elements = sorted(elements, key=lambda e: e.y_position)\n",
    "    groups = []\n",
    "    current_group = [sorted_elements[0]]\n",
    "    current_y = sorted_elements[0].y_position\n",
    "    \n",
    "    for element in sorted_elements[1:]:\n",
    "        if abs(element.y_position - current_y) <= y_tolerance:\n",
    "            current_group.append(element)\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [element]\n",
    "            current_y = element.y_position\n",
    "    \n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "\n",
    "def reflow_page_with_line_numbers(page_data: dict, separator: str = \" | \") -> str:\n",
    "    \"\"\"Reflow a single page's content to include line numbers inline.\"\"\"\n",
    "    elements = extract_lines_from_page(page_data)\n",
    "    if not elements:\n",
    "        return \"\"\n",
    "    \n",
    "    line_groups = group_lines_by_vertical_position(elements)\n",
    "    output_lines = []\n",
    "    \n",
    "    for group in line_groups:\n",
    "        # Sort by X position (left to right)\n",
    "        group.sort(key=lambda e: e.x_position)\n",
    "        \n",
    "        line_numbers = [e for e in group if e.is_line_number]\n",
    "        content_elements = [e for e in group if not e.is_line_number]\n",
    "        \n",
    "        if not content_elements:\n",
    "            continue\n",
    "        \n",
    "        combined_content = ' '.join(e.content for e in content_elements)\n",
    "        \n",
    "        if line_numbers:\n",
    "            line_num = line_numbers[0].content\n",
    "            output_lines.append(f\"{line_num}{separator}{combined_content}\")\n",
    "        else:\n",
    "            output_lines.append(combined_content)\n",
    "    \n",
    "    return '\\n'.join(output_lines)\n",
    "\n",
    "\n",
    "def reflow_document(json_data: dict, target_page: Optional[int] = None, \n",
    "                    separator: str = \" | \") -> str:\n",
    "    \"\"\"Reflow an entire document or specific page with line numbers inline.\"\"\"\n",
    "    contents = json_data.get('result', {}).get('contents', [])\n",
    "    if not contents:\n",
    "        raise ValueError(\"No contents found in JSON data\")\n",
    "    \n",
    "    content = contents[0]\n",
    "    pages = content.get('pages', [])\n",
    "    if not pages:\n",
    "        raise ValueError(\"No pages found in document content\")\n",
    "    \n",
    "    output_parts = []\n",
    "    \n",
    "    for page in pages:\n",
    "        page_number = page.get('pageNumber', 0)\n",
    "        if target_page is not None and page_number != target_page:\n",
    "            continue\n",
    "        \n",
    "        page_output = reflow_page_with_line_numbers(page, separator)\n",
    "        if page_output:\n",
    "            if target_page is None:\n",
    "                output_parts.append(f\"\\n<!-- Page {page_number} -->\\n\")\n",
    "            output_parts.append(page_output)\n",
    "    \n",
    "    return '\\n'.join(output_parts)\n",
    "\n",
    "print(\"‚úÖ Reflow functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d139d",
   "metadata": {},
   "source": [
    "## Reflow a Single Page\n",
    "\n",
    "Let's reflow page 3 of the transcript to see the line numbers inline with the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ed1b4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Reflowed Output for Page 3:\n",
      "============================================================\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | INDEX\n",
      "2 | WITNESS: DIRECT CROSS REDIRECT RECROSS\n",
      "3 | Susan Michaud\n",
      "6 | EXHIBITS: EVIDENCE IDENTIFICATION\n",
      "7 | Diagram (P-1)\n",
      "682499392\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Reflow a single page (page 3)\n",
    "page_to_reflow = 3\n",
    "\n",
    "print(f\"üìÑ Reflowed Output for Page {page_to_reflow}:\")\n",
    "print(\"=\" * 60)\n",
    "reflowed_page = reflow_document(result, target_page=page_to_reflow)\n",
    "print(reflowed_page)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a635c8",
   "metadata": {},
   "source": [
    "## Reflow Entire Document\n",
    "\n",
    "Now let's reflow the entire document and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc9a0906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Reflowing entire document...\n",
      "‚úÖ Reflowed document saved to: c:\\src\\azure-ai-content-understanding-python\\notebooks\\test_output\\legal_transcript_reflowed.md\n",
      "   Total characters: 65678\n",
      "\n",
      "üìÑ Preview (first 3000 characters):\n",
      "============================================================\n",
      "\n",
      "<!-- Page 1 -->\n",
      "\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "SUPERIOR COURT OF NEW JERSEY\n",
      "MERCER COUNTY-LAW DIVISION,\n",
      "DOCKET NO. L-90-2940\n",
      "IN RE: IN THE MATTER OF\n",
      "SUSAN MICHAUD\n",
      "DEPOSITION OF:\n",
      "Susan Michaud\n",
      "Transcript of proceedings taken on July 13, 1990,\n",
      "at 1 pm, at the office of Mason, Griffin & Pierson, 101 Poor\n",
      "Farm Road, Princeton, NJ 08540.\n",
      "682499390\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 2 -->\n",
      "\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "2 | APPEARANCES\n",
      "3 | On behalf of\n",
      "Susan Michaud: MASON, GRIFFIN & PIERSON\n",
      "4 | BY: Stephanie J. Briody, Esq.\n",
      "101 Poor Farm Road\n",
      "5 | Princton, NJ 08540\n",
      "6 | On behalf of Dr. Alfred\n",
      "Cook, Dr. Charles Howard &\n",
      "7 | Princeton Radiology Assoc. JACKSON & VAURIO\n",
      "BY: John Zen Jackson, Esq.\n",
      "8 | 1000 Herrontown Road\n",
      "Princeton, NJ 08540\n",
      "682499391\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocumƒónts.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 3 -->\n",
      "\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | INDEX\n",
      "2 | WITNESS: DIRECT CROSS REDIRECT RECROSS\n",
      "3 | Susan Michaud\n",
      "6 | EXHIBITS: EVIDENCE IDENTIFICATION\n",
      "7 | Diagram (P-1)\n",
      "682499392\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 4 -->\n",
      "\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | Susan Michaud, M-I-C-H-A-U-D, sworn by the Notary Public,\n",
      "2 | testified as follows.\n",
      "3 | DIRECT EXAMINATION BY\n",
      "4 | MS. BRIODY:\n",
      "5 | Q. Susan, how old are you at the present time?\n",
      "6 | A. Just turned thirty-eight.\n",
      "7 | Q. And are you married?\n",
      "8 | A. Yes, I am.\n",
      "9 | Q. And for how many years have you been married?\n",
      "10 | A. Nineteen.\n",
      "11 | Q. What year were you married?\n",
      "12 | A. '71.\n",
      "13 | Q. And to whom are you married?\n",
      "14 | A. Thomas Michaud.\n",
      "15 | Q. Do you have any children?\n",
      "16 | A. Yes, I have one.\n",
      "17 | Q. Is it a boy or a girl?\n",
      "18 | A. A fourteen year old boy, almost fifteen.\n",
      "19 | Q. What's his name?\n",
      "20 | A. Matthew.\n",
      "21 | Q. Did you go to high school in Princeton?\n",
      "22 | A. Yes.\n",
      "23 | Q. And what is your educational background?\n",
      "24 | A. I have about thirty college credits beyond high school\n",
      "25 | and that's all.\n",
      "682499393\n",
      "http://legacy.library.ucsf.edu/tid/fuq07a00/pdfv.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 5 -->\n",
      "\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | Q. Where did you get those credits?\n",
      "2 | A. At Mercer County Community College.\n",
      "3 | Q. Did you grow up in the Princeton area?\n",
      "4 | A. Yes.\n",
      "5 | Q. Where did you go to middle school or junior high\n",
      "6 | school?\n",
      "7 | A. Princeton community--Community Park.\n",
      "8 | Q. It's called Community Park?\n",
      "9 | A. It's called Community Park.\n",
      "10 | Q. In Princeton?\n",
      "11 | A. Yes.\n",
      "12 | Q. For whom do you work?\n",
      "13 | A. Nassau Federal Savings and Loan.\n",
      "14 | Q. And what kind of work do you do for them?\n",
      "15 | A. I am the director of their Human Resource Department.\n",
      "16 | Q. And for how long have you worked for the bank?\n",
      "17 | A. In September it will be thr\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Reflow the entire document\n",
    "print(\"üìÑ Reflowing entire document...\")\n",
    "reflowed_document = reflow_document(result)\n",
    "\n",
    "# Save to file\n",
    "output_path = os.path.join(os.getcwd(), 'test_output', 'legal_transcript_reflowed.md')\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(reflowed_document)\n",
    "\n",
    "print(f\"‚úÖ Reflowed document saved to: {output_path}\")\n",
    "print(f\"   Total characters: {len(reflowed_document)}\")\n",
    "\n",
    "# Show first 3000 characters\n",
    "print(\"\\nüìÑ Preview (first 3000 characters):\")\n",
    "print(\"=\" * 60)\n",
    "print(reflowed_document[:3000])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae13e7",
   "metadata": {},
   "source": [
    "## Compare: Before vs After\n",
    "\n",
    "Let's compare the default output with the reflowed output for a specific page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare default vs reflowed for page 3\n",
    "page_number = 3\n",
    "\n",
    "# Get the original markdown (extract just page 3 content - approximation)\n",
    "original_lines = markdown.split('\\n')\n",
    "\n",
    "print(\"üìä COMPARISON: Default vs Reflowed Output\")\n",
    "print(\"\\n\" + \"=\" * 30 + \" DEFAULT OUTPUT \" + \"=\" * 30)\n",
    "print(\"(Line numbers grouped separately at page bottom)\")\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# Show a sample of the default output\n",
    "sample_start = 200\n",
    "sample_end = 800\n",
    "print(markdown[sample_start:sample_end])\n",
    "print(\"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 30 + \" REFLOWED OUTPUT \" + \"=\" * 29)\n",
    "print(\"(Line numbers inline with text)\")\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# Show the reflowed output for page 3\n",
    "reflowed_page = reflow_document(result, target_page=page_number)\n",
    "print(reflowed_page[:800])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc1e27",
   "metadata": {},
   "source": [
    "## Using the Standalone Script\n",
    "\n",
    "For batch processing or command-line usage, you can use the standalone script located at `python/reflow_markdown_with_line_numbers.py`:\n",
    "\n",
    "```bash\n",
    "# Process a specific page\n",
    "python python/reflow_markdown_with_line_numbers.py analysis.json --page 3\n",
    "\n",
    "# Process all pages and save to file\n",
    "python python/reflow_markdown_with_line_numbers.py analysis.json --output reflowed.md\n",
    "\n",
    "# Custom separator\n",
    "python python/reflow_markdown_with_line_numbers.py analysis.json --separator \" | \" --output reflowed.md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580894a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run the standalone script on our saved JSON\n",
    "import subprocess\n",
    "\n",
    "script_path = os.path.join(os.path.dirname(os.getcwd()), 'python', 'reflow_markdown_with_line_numbers.py')\n",
    "output_file = os.path.join(os.getcwd(), 'test_output', 'legal_transcript_reflowed_script.md')\n",
    "\n",
    "print(f\"üîß Running standalone reflow script...\")\n",
    "print(f\"   Input: {saved_json_path}\")\n",
    "print(f\"   Output: {output_file}\")\n",
    "\n",
    "result_code = subprocess.run(\n",
    "    ['python', script_path, saved_json_path, '--output', output_file],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result_code.returncode == 0:\n",
    "    print(f\"‚úÖ Script completed successfully!\")\n",
    "    print(result_code.stdout)\n",
    "else:\n",
    "    print(f\"‚ùå Script failed:\")\n",
    "    print(result_code.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0ae60",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Extract content** from legal transcripts using Azure Content Understanding's `prebuilt-layout` analyzer\n",
    "2. **Understand the JSON structure** including the `source` field with bounding box coordinates\n",
    "3. **Reflow the output** to include line numbers inline with text by:\n",
    "   - Parsing bounding box coordinates to determine element positions\n",
    "   - Grouping elements by vertical position (Y coordinate)\n",
    "   - Matching line numbers with their corresponding text content\n",
    "4. **Use the standalone script** for batch processing\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "This technique is valuable for:\n",
    "- **Legal document processing** - Depositions, trial transcripts, court records\n",
    "- **Academic citations** - Line-numbered source materials\n",
    "- **Content indexing** - Building searchable databases with line-level citations\n",
    "- **AI-powered legal research** - RAG applications that need line-accurate references\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- [Content Understanding Document Elements](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/elements)\n",
    "- [Document Markdown Representation](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/markdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
