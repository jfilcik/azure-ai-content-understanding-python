{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ae1183",
   "metadata": {},
   "source": [
    "# Legal Transcript Line Number Reflow\n",
    "\n",
    "This notebook demonstrates how to process legal documents (depositions, court transcripts, trial records) with Azure Content Understanding and reflow the output to include inline line numbers.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Legal transcripts have a standardized format with **line numbers in the left margin** (typically 1-25 per page). These line numbers are critical for:\n",
    "- Citing specific testimony in legal briefs\n",
    "- Cross-referencing during depositions and trials\n",
    "- Creating accurate legal summaries\n",
    "\n",
    "By default, Content Understanding's markdown output groups these margin line numbers separately from the main text content. This notebook shows how to **reflow the output** to include line numbers inline with each text line.\n",
    "\n",
    "## Workflow\n",
    "1. **Load PDF** - Read the local legal transcript file\n",
    "2. **Content Extraction** - Use Azure Content Understanding to extract text with position data\n",
    "3. **Reflow** - Match line numbers with text using bounding box coordinates\n",
    "4. **Output** - Generate markdown with inline line numbers (e.g., `1 | witness testimony...`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae8a24",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Ensure your Azure AI service is configured by following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. Install the required packages to run this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13e310",
   "metadata": {},
   "source": [
    "## Create Azure AI Content Understanding Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6480fbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client created successfully\n",
      "   Endpoint: https://mmi-usw3-eft-foundry.services.ai.azure.com/\n",
      "   Credential: Subscription Key\n",
      "   API Version: 2025-11-01\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Optional\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Add the parent directory to the Python path to import the helper modules\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'python'))\n",
    "from content_understanding_client import AzureContentUnderstandingClient\n",
    "from extension.sample_helper import save_json_to_file \n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "API_VERSION = \"2025-11-01\"\n",
    "\n",
    "# Create token provider for Azure AD authentication\n",
    "def token_provider():\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    return token.token\n",
    "\n",
    "# Create the Content Understanding client\n",
    "try:\n",
    "    client = AzureContentUnderstandingClient(\n",
    "        endpoint=AZURE_AI_ENDPOINT,\n",
    "        api_version=API_VERSION,\n",
    "        subscription_key=AZURE_AI_API_KEY,\n",
    "        token_provider=token_provider if not AZURE_AI_API_KEY else None,\n",
    "        x_ms_useragent=\"azure-ai-content-understanding-python-sample-legal-reflow\"\n",
    "    )\n",
    "    credential_type = \"Subscription Key\" if AZURE_AI_API_KEY else \"Azure AD Token\"\n",
    "    print(f\"‚úÖ Client created successfully\")\n",
    "    print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "    print(f\"   Credential: {credential_type}\")\n",
    "    print(f\"   API Version: {API_VERSION}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create client: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb5f26a",
   "metadata": {},
   "source": [
    "## Configure Model Deployments\n",
    "\n",
    "> **üí° Note:** This step is only required **once per Azure Content Understanding resource**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4941027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuring default model deployments...\n",
      "‚úÖ Default model deployments configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Get model deployment names from environment variables\n",
    "GPT_4_1_DEPLOYMENT = os.getenv(\"GPT_4_1_DEPLOYMENT\")\n",
    "GPT_4_1_MINI_DEPLOYMENT = os.getenv(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "TEXT_EMBEDDING_3_LARGE_DEPLOYMENT = os.getenv(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "# Check if required deployments are configured\n",
    "missing_deployments = []\n",
    "if not GPT_4_1_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_DEPLOYMENT\")\n",
    "if not GPT_4_1_MINI_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "if not TEXT_EMBEDDING_3_LARGE_DEPLOYMENT:\n",
    "    missing_deployments.append(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "if missing_deployments:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing model deployment configuration(s): {missing_deployments}\")\n",
    "    print(\"   Add these to your .env file and restart the kernel.\")\n",
    "else:\n",
    "    print(f\"üìã Configuring default model deployments...\")\n",
    "    try:\n",
    "        result = client.update_defaults({\n",
    "            \"gpt-4.1\": GPT_4_1_DEPLOYMENT,\n",
    "            \"gpt-4.1-mini\": GPT_4_1_MINI_DEPLOYMENT,\n",
    "            \"text-embedding-3-large\": TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\n",
    "        })\n",
    "        print(f\"‚úÖ Default model deployments configured successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to configure defaults: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8544696",
   "metadata": {},
   "source": [
    "## Analyze Legal Transcript\n",
    "\n",
    "We'll use a publicly available deposition transcript from the Internet Archive. This is a real legal document with the standard line-numbered format used in depositions.\n",
    "\n",
    "**Sample Document:** [Farr Deposition Transcript](https://archive.org/details/799436-farr-deposition-transcript) (15 pages, Public Domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18932e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing legal transcript from local file...\n",
      "   Document: c:\\src\\azure-ai-content-understanding-python\\data\\legal_examples\\Trascript Example.pdf\n",
      "   Analyzer: prebuilt-layout\n",
      "   File size: 1,666,047 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:content_understanding_client:Analyzing binary file c:\\src\\azure-ai-content-understanding-python\\data\\legal_examples\\Trascript Example.pdf with analyzer: prebuilt-layout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Waiting for analysis to complete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:content_understanding_client:Request 9e72763b-ecdf-4b8d-8f9b-a6733d30b6b1 in progress ...\n",
      "INFO:content_understanding_client:Request 9e72763b-ecdf-4b8d-8f9b-a6733d30b6b1 in progress ...\n",
      "INFO:content_understanding_client:Request 9e72763b-ecdf-4b8d-8f9b-a6733d30b6b1 in progress ...\n",
      "INFO:content_understanding_client:Request result is ready after 7.11 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analysis completed!\n",
      "\n",
      "üìÑ Document Information:\n",
      "   Pages: 1 - 52\n",
      "   Total pages: 52\n",
      "üíæ Analysis result saved to: test_output\\legal_transcript_analysis_20260122_162606.json\n",
      "\n",
      "üíæ Full analysis saved to: test_output\\legal_transcript_analysis_20260122_162606.json\n"
     ]
    }
   ],
   "source": [
    "# Analyze legal transcript from local file\n",
    "# Using the transcript example from the data/legal_examples folder\n",
    "document_path = os.path.join(os.path.dirname(os.getcwd()), 'data', 'legal_examples', 'Trascript Example.pdf')\n",
    "analyzer_id = 'prebuilt-layout'\n",
    "\n",
    "print(f\"üîç Analyzing legal transcript from local file...\")\n",
    "print(f\"   Document: {document_path}\")\n",
    "print(f\"   Analyzer: {analyzer_id}\")\n",
    "\n",
    "# Verify file exists\n",
    "if not os.path.exists(document_path):\n",
    "    raise FileNotFoundError(f\"Document not found: {document_path}\")\n",
    "\n",
    "file_size = os.path.getsize(document_path)\n",
    "print(f\"   File size: {file_size:,} bytes\")\n",
    "\n",
    "# Analyze the document using binary file path\n",
    "response = client.begin_analyze_binary(\n",
    "    analyzer_id=analyzer_id,\n",
    "    file_location=document_path\n",
    ")\n",
    "\n",
    "print(f\"‚è≥ Waiting for analysis to complete...\")\n",
    "result = client.poll_result(response)\n",
    "print(f\"‚úÖ Analysis completed!\")\n",
    "\n",
    "# Get document info\n",
    "contents = result.get(\"result\", {}).get(\"contents\", [])\n",
    "if contents:\n",
    "    content = contents[0]\n",
    "    if content.get(\"kind\") == \"document\":\n",
    "        print(f\"\\nüìÑ Document Information:\")\n",
    "        print(f\"   Pages: {content.get('startPageNumber')} - {content.get('endPageNumber')}\")\n",
    "        print(f\"   Total pages: {content.get('endPageNumber') - content.get('startPageNumber') + 1}\")\n",
    "\n",
    "# Save the full result for processing\n",
    "saved_json_path = save_json_to_file(result, filename_prefix=\"legal_transcript_analysis\")\n",
    "print(f\"\\nüíæ Full analysis saved to: {saved_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45dcbdb",
   "metadata": {},
   "source": [
    "## View Default Markdown Output\n",
    "\n",
    "Let's first look at Content Understanding's default markdown output. Notice how the **line numbers are grouped separately** at the bottom of each page's content rather than inline with the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b60915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Default Markdown Output (first 2000 chars):\n",
      "============================================================\n",
      "# (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "SUPERIOR COURT OF NEW JERSEY\n",
      "MERCER COUNTY-LAW DIVISION,\n",
      "DOCKET NO. L-90-2940\n",
      "\n",
      ":\n",
      "\n",
      ":\n",
      "\n",
      "IN RE:\n",
      "IN THE MATTER OF\n",
      "SUSAN MICHAUD\n",
      "\n",
      ":\n",
      "\n",
      "DEPOSITION OF:\n",
      "\n",
      ":\n",
      "\n",
      "Susan Michaud\n",
      "\n",
      ":\n",
      "\n",
      ":\n",
      "\n",
      "Transcript of proceedings taken on July 13, 1990,\n",
      "at 1 pm, at the office of Mason, Griffin & Pierson, 101 Poor\n",
      "Farm Road, Princeton, NJ 08540.\n",
      "\n",
      "682499390\n",
      "\n",
      "<!-- PageFooter: http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocuments.ucsf.edu/docs/khhl0001 -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "\n",
      "# (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "\n",
      "2\n",
      "\n",
      "APPEARANCES\n",
      "\n",
      "On behalf of\n",
      "Susan Michaud:\n",
      "\n",
      "MASON, GRIFFIN & PIERSON\n",
      "BY: Stephanie J. Briody, Esq.\n",
      "101 Poor Farm Road\n",
      "Princton, NJ 08540\n",
      "\n",
      "On behalf of Dr. Alfred\n",
      "Cook, Dr. Charles Howard &\n",
      "Princeton Radiology Assoc.\n",
      "\n",
      "JACKSON & VAURIO\n",
      "BY: John Zen Jackson, Esq.\n",
      "1000 Herrontown Road\n",
      "Princeton, NJ 08540\n",
      "\n",
      "682499391\n",
      "\n",
      "<!-- PageFooter: http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocumƒónts.ucsf.edu/docs/khhl0001 -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "\n",
      "## (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "3\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "\n",
      "INDEX\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>WITNESS:</th>\n",
      "<th>DIRECT</th>\n",
      "<th>CROSS</th>\n",
      "<th>REDIRECT</th>\n",
      "<th>RECROSS</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Susan Michaud</td>\n",
      "<td>4</td>\n",
      "<td>40</td>\n",
      "<td>44</td>\n",
      "<td>44</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      "EXHIBITS:\n",
      "Diagram (P-1)\n",
      "\n",
      "EVIDENCE\n",
      "\n",
      "IDENTIFICATION\n",
      "\n",
      "23\n",
      "\n",
      "682499392\n",
      "\n",
      "<!-- PageFooter: http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001 -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "\n",
      "### (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "4\n",
      "\n",
      "Susan Michaud, M-I-C-H-A-U-D, sworn by the Notary Public,\n",
      "testified as follows.\n",
      "\n",
      "DIRECT EXAMINATION BY\n",
      "\n",
      "MS. BRIODY:\n",
      "\n",
      "Q.\n",
      "Susan, how old are you at the present time?\n",
      "\n",
      "A.\n",
      "Just turned thirty-eight.\n",
      "\n",
      "Q.\n",
      "And are you married?\n",
      "\n",
      "A. Yes, I am.\n",
      "\n",
      "Q.\n",
      "And for how many years have you been married?\n",
      "\n",
      "============================================================\n",
      "\n",
      "... (Total length: 66709 characters)\n"
     ]
    }
   ],
   "source": [
    "# Show the default markdown output (first 2000 characters)\n",
    "markdown = content.get(\"markdown\", \"\")\n",
    "\n",
    "print(\"üìÑ Default Markdown Output (first 2000 chars):\")\n",
    "print(\"=\" * 60)\n",
    "print(markdown[:2000])\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n... (Total length: {len(markdown)} characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb101887",
   "metadata": {},
   "source": [
    "## How Reflow Works\n",
    "\n",
    "The reflow algorithm uses **bounding box coordinates** from the JSON output to match line numbers with their corresponding text:\n",
    "\n",
    "### Step 1: Parse Coordinates\n",
    "Every element in CU's JSON has a `source` field with position data:\n",
    "```\n",
    "\"source\": \"D(1,1.0309,1.1277,1.131,1.1277,1.131,1.2711,1.0309,1.2711)\"\n",
    "           D(page, x1,y1, x2,y2, x3,y3, x4,y4)\n",
    "```\n",
    "\n",
    "### Step 2: Group by Vertical Position\n",
    "Elements with similar Y values (within ~0.15 inches) are on the same horizontal line.\n",
    "\n",
    "### Step 3: Sort Left-to-Right\n",
    "Within each group, sort by X coordinate. Line numbers (X ‚âà 1.0\") come before text content (X ‚âà 1.3\"+).\n",
    "\n",
    "### Step 4: Combine\n",
    "Pair line numbers with their corresponding text and output as `N | text content`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13abdb4f",
   "metadata": {},
   "source": [
    "## Reflow Functions\n",
    "\n",
    "Here are the core functions for reflowing Content Understanding output to include inline line numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a421c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reflow functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import re",
    "from dataclasses import dataclass, field",
    "from typing import Optional, Dict, List, Tuple",
    "",
    "",
    "@dataclass",
    "class LineElement:",
    "    \"\"\"Represents a line or element from the document with its position.\"\"\"",
    "    content: str",
    "    y_position: float  # Top Y coordinate",
    "    x_position: float  # Left X coordinate",
    "    page_number: int",
    "    is_line_number: bool = False",
    "    # Enhanced fields for offset tracking",
    "    original_offset: Optional[int] = None  # Offset in original markdown",
    "    original_length: Optional[int] = None  # Length in original markdown",
    "    bbox: Optional[str] = None  # Bounding box from source",
    "",
    "",
    "@dataclass",
    "class OffsetMapping:",
    "    \"\"\"Maps elements from original to reflowed content with offsets.\"\"\"",
    "    pages: List['PageOffsetInfo'] = field(default_factory=list)",
    "    ",
    "    ",
    "@dataclass",
    "class PageOffsetInfo:",
    "    \"\"\"Offset information for a page in reflowed content.\"\"\"",
    "    page_number: int",
    "    offset: Dict[str, int]  # {\"start\": X, \"end\": Y}",
    "    lines: List['LineOffsetInfo'] = field(default_factory=list)",
    "    ",
    "",
    "@dataclass  ",
    "class LineOffsetInfo:",
    "    \"\"\"Offset information for a line in reflowed content.\"\"\"",
    "    content: str",
    "    offset: Dict[str, int]  # {\"start\": X, \"end\": Y}",
    "    bbox: Optional[str] = None",
    "    words: List['WordOffsetInfo'] = field(default_factory=list)",
    "    ",
    "",
    "@dataclass",
    "class WordOffsetInfo:",
    "    \"\"\"Offset information for a word in reflowed content.\"\"\"",
    "    content: str",
    "    offset: Dict[str, int]  # {\"start\": X, \"end\": Y}",
    "    bbox: str",
    "    original_offset: Optional[int] = None  # Original offset in OCR markdown",
    "",
    "",
    "def parse_source_coordinates(source: str) -> tuple[int, float, float, float, float, str]:",
    "    \"\"\"",
    "    Parse the source coordinate string from Content Understanding.",
    "    ",
    "    The source format is: D(pageNumber,x1,y1,x2,y2,x3,y3,x4,y4)",
    "    where the points represent a quadrilateral (upper-left, upper-right, lower-right, lower-left)",
    "    ",
    "    Returns:",
    "        Tuple of (page_number, left_x, top_y, right_x, bottom_y, bbox_str)",
    "    \"\"\"",
    "    match = re.match(r'D\\((\\d+),([^)]+)\\)', source)",
    "    if not match:",
    "        raise ValueError(f\"Invalid source format: {source}\")",
    "    ",
    "    page_number = int(match.group(1))",
    "    coords = [float(x) for x in match.group(2).split(',')]",
    "    ",
    "    if len(coords) == 8:",
    "        # Bounding polygon: x1,y1,x2,y2,x3,y3,x4,y4",
    "        x1, y1, x2, y2, x3, y3, x4, y4 = coords",
    "        left_x = min(x1, x4)",
    "        top_y = min(y1, y2)",
    "        right_x = max(x2, x3)",
    "        bottom_y = max(y3, y4)",
    "    elif len(coords) == 4:",
    "        # Axis-aligned bounding box: left, top, width, height",
    "        left_x, top_y, width, height = coords",
    "        right_x = left_x + width",
    "        bottom_y = top_y + height",
    "    else:",
    "        raise ValueError(f\"Unexpected coordinate count: {source}\")",
    "    ",
    "    return page_number, left_x, top_y, right_x, bottom_y, source",
    "",
    "",
    "def is_line_number(content: str) -> bool:",
    "    \"\"\"Check if content is a line number (1-99).\"\"\"",
    "    return content.strip().isdigit() and 1 <= int(content.strip()) <= 99",
    "",
    "",
    "def is_noise_element(content: str) -> bool:",
    "    \"\"\"Check if content is noise (bullets, single dots) that should be filtered.\"\"\"",
    "    content = content.strip()",
    "    return content in ['¬∑', '‚Ä¢', '‚àô'] or (len(content) == 1 and not content.isalnum())",
    "",
    "",
    "def extract_lines_from_page(page_data: dict) -> list[LineElement]:",
    "    \"\"\"Extract all lines from a page with position and offset information.\"\"\"",
    "    lines = []",
    "    ",
    "    for line in page_data.get('lines', []):",
    "        content = line.get('content', '').strip()",
    "        if not content or is_noise_element(content):",
    "            continue",
    "        ",
    "        source = line.get('source', '')",
    "        if not source:",
    "            continue",
    "        ",
    "        try:",
    "            page_num, left_x, top_y, right_x, bottom_y, bbox = parse_source_coordinates(source)",
    "            ",
    "            # Get offset information if available",
    "            original_offset = None",
    "            original_length = None",
    "            if 'span' in line:",
    "                original_offset = line['span'].get('offset')",
    "                original_length = line['span'].get('length')",
    "            ",
    "            lines.append(LineElement(",
    "                content=content,",
    "                y_position=top_y,",
    "                x_position=left_x,",
    "                page_number=page_num,",
    "                is_line_number=is_line_number(content),",
    "                original_offset=original_offset,",
    "                original_length=original_length,",
    "                bbox=bbox",
    "            ))",
    "        except (ValueError, KeyError) as e:",
    "            # Skip lines with invalid coordinates",
    "            continue",
    "    ",
    "    return lines",
    "",
    "",
    "def extract_words_from_page(page_data: dict) -> List[Dict]:",
    "    \"\"\"Extract all words from a page with their offset and bbox information.\"\"\"",
    "    words = []",
    "    ",
    "    for word in page_data.get('words', []):",
    "        content = word.get('content', '').strip()",
    "        if not content:",
    "            continue",
    "            ",
    "        span = word.get('span', {})",
    "        offset = span.get('offset')",
    "        length = span.get('length')",
    "        source = word.get('source', '')",
    "        ",
    "        words.append({",
    "            'content': content,",
    "            'offset': offset,",
    "            'length': length,",
    "            'source': source",
    "        })",
    "    ",
    "    return words",
    "",
    "",
    "def group_lines_by_vertical_position(elements: list[LineElement], ",
    "                                     y_tolerance: float = 0.15) -> list[list[LineElement]]:",
    "    \"\"\"Group elements that are on the same horizontal line.\"\"\"",
    "    if not elements:",
    "        return []",
    "    ",
    "    # Sort by Y position (top to bottom)",
    "    sorted_elements = sorted(elements, key=lambda e: e.y_position)",
    "    ",
    "    groups = []",
    "    current_group = [sorted_elements[0]]",
    "    ",
    "    for element in sorted_elements[1:]:",
    "        # If Y position is close to current group, add to group",
    "        if abs(element.y_position - current_group[0].y_position) < y_tolerance:",
    "            current_group.append(element)",
    "        else:",
    "            groups.append(current_group)",
    "            current_group = [element]",
    "    ",
    "    if current_group:",
    "        groups.append(current_group)",
    "    ",
    "    return groups",
    "",
    "",
    "def reflow_page_with_line_numbers_and_offsets(",
    "    page_data: dict, ",
    "    separator: str = \" | \",",
    "    current_offset: int = 0",
    ") -> Tuple[str, PageOffsetInfo]:",
    "    \"\"\"",
    "    Reflow a single page's content to include line numbers inline.",
    "    Returns the reflowed text and offset mapping information.",
    "    \"\"\"",
    "    page_number = page_data.get('pageNumber', 0)",
    "    elements = extract_lines_from_page(page_data)",
    "    words_data = extract_words_from_page(page_data)",
    "    ",
    "    # Create word lookup by content and position for matching",
    "    word_lookup = {}",
    "    for word in words_data:",
    "        key = (word['content'], word.get('offset'))",
    "        word_lookup[key] = word",
    "    ",
    "    if not elements:",
    "        return \"\", PageOffsetInfo(page_number=page_number, offset={\"start\": current_offset, \"end\": current_offset})",
    "    ",
    "    page_start_offset = current_offset",
    "    line_groups = group_lines_by_vertical_position(elements)",
    "    output_lines = []",
    "    line_offset_infos = []",
    "    ",
    "    for group in line_groups:",
    "        # Sort by X position (left to right)",
    "        group.sort(key=lambda e: e.x_position)",
    "        ",
    "        line_numbers = [e for e in group if e.is_line_number]",
    "        content_elements = [e for e in group if not e.is_line_number]",
    "        ",
    "        if not content_elements:",
    "            continue",
    "        ",
    "        # Build the line with offset tracking",
    "        line_start_offset = current_offset",
    "        line_parts = []",
    "        word_offset_infos = []",
    "        ",
    "        # Add line number if present",
    "        if line_numbers:",
    "            line_num = line_numbers[0].content",
    "            line_parts.append(line_num)",
    "            ",
    "            # Track word offset for line number",
    "            word_offset_infos.append(WordOffsetInfo(",
    "                content=line_num,",
    "                offset={\"start\": current_offset, \"end\": current_offset + len(line_num)},",
    "                bbox=line_numbers[0].bbox or \"\",",
    "                original_offset=line_numbers[0].original_offset",
    "            ))",
    "            current_offset += len(line_num)",
    "            ",
    "            # Add separator",
    "            line_parts.append(separator)",
    "            current_offset += len(separator)",
    "        ",
    "        # Add content elements",
    "        for i, elem in enumerate(content_elements):",
    "            if i > 0:",
    "                line_parts.append(' ')",
    "                current_offset += 1",
    "            ",
    "            content = elem.content",
    "            line_parts.append(content)",
    "            ",
    "            # Track word offset for content",
    "            word_offset_infos.append(WordOffsetInfo(",
    "                content=content,",
    "                offset={\"start\": current_offset, \"end\": current_offset + len(content)},",
    "                bbox=elem.bbox or \"\",",
    "                original_offset=elem.original_offset",
    "            ))",
    "            current_offset += len(content)",
    "        ",
    "        # Build the complete line",
    "        line_text = ''.join(line_parts)",
    "        output_lines.append(line_text)",
    "        ",
    "        # Create line offset info",
    "        line_offset_infos.append(LineOffsetInfo(",
    "            content=line_text,",
    "            offset={\"start\": line_start_offset, \"end\": current_offset},",
    "            bbox=content_elements[0].bbox if content_elements else None,",
    "            words=word_offset_infos",
    "        ))",
    "        ",
    "        # Add newline",
    "        current_offset += 1  # for \\n",
    "    ",
    "    reflowed_text = '\\n'.join(output_lines)",
    "    page_end_offset = current_offset",
    "    ",
    "    page_offset_info = PageOffsetInfo(",
    "        page_number=page_number,",
    "        offset={\"start\": page_start_offset, \"end\": page_end_offset},",
    "        lines=line_offset_infos",
    "    )",
    "    ",
    "    return reflowed_text, page_offset_info",
    "",
    "",
    "def reflow_document_with_offsets(",
    "    json_data: dict, ",
    "    target_page: Optional[int] = None, ",
    "    separator: str = \" | \"",
    ") -> Tuple[str, OffsetMapping]:",
    "    \"\"\"",
    "    Reflow an entire document or specific page with line numbers inline.",
    "    Returns the reflowed markdown and complete offset mapping.",
    "    \"\"\"",
    "    contents = json_data.get('result', {}).get('contents', [])",
    "    if not contents:",
    "        raise ValueError(\"No contents found in JSON data\")",
    "    ",
    "    content = contents[0]",
    "    pages = content.get('pages', [])",
    "    if not pages:",
    "        raise ValueError(\"No pages found in document content\")",
    "    ",
    "    output_parts = []",
    "    offset_mapping = OffsetMapping()",
    "    current_offset = 0",
    "    ",
    "    for page in pages:",
    "        page_number = page.get('pageNumber', 0)",
    "        if target_page is not None and page_number != target_page:",
    "            continue",
    "        ",
    "        # Add page marker",
    "        if target_page is None:",
    "            page_marker = f\"\\n<!-- Page {page_number} -->\\n\"",
    "            output_parts.append(page_marker)",
    "            current_offset += len(page_marker)",
    "        ",
    "        # Reflow the page",
    "        page_output, page_offset_info = reflow_page_with_line_numbers_and_offsets(",
    "            page, separator, current_offset",
    "        )",
    "        ",
    "        if page_output:",
    "            output_parts.append(page_output)",
    "            current_offset += len(page_output)",
    "            if target_page is None:",
    "                current_offset += 1  # for \\n between pages",
    "            ",
    "            offset_mapping.pages.append(page_offset_info)",
    "    ",
    "    reflowed_content = '\\n'.join(output_parts) if target_page is None else output_parts[1] if len(output_parts) > 1 else output_parts[0] if output_parts else \"\"",
    "    ",
    "    return reflowed_content, offset_mapping",
    "",
    "",
    "print(\"‚úÖ Enhanced reflow functions with offset tracking loaded successfully!\")",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d139d",
   "metadata": {},
   "source": [
    "## Reflow a Single Page\n",
    "\n",
    "Let's reflow page 3 of the transcript to see the line numbers inline with the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ed1b4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Reflowed Output for Page 3:\n",
      "============================================================\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | INDEX\n",
      "2 | WITNESS: DIRECT CROSS REDIRECT RECROSS\n",
      "3 | Susan Michaud\n",
      "6 | EXHIBITS: EVIDENCE IDENTIFICATION\n",
      "7 | Diagram (P-1)\n",
      "682499392\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Reflow a single page (page 3)\n",
    "page_to_reflow = 3\n",
    "\n",
    "print(f\"üìÑ Reflowed Output for Page {page_to_reflow}:\")\n",
    "print(\"=\" * 60)\n",
    "reflowed_page = reflow_document(result, target_page=page_to_reflow)\n",
    "print(reflowed_page)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a635c8",
   "metadata": {},
   "source": [
    "## Reflow Entire Document\n",
    "\n",
    "Now let's reflow the entire document and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc9a0906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Reflowing entire document...\n",
      "‚úÖ Reflowed document saved to: c:\\src\\azure-ai-content-understanding-python\\notebooks\\test_output\\legal_transcript_reflowed.md\n",
      "   Total characters: 65678\n",
      "\n",
      "üìÑ Preview (first 3000 characters):\n",
      "============================================================\n",
      "\n",
      "<!-- Page 1 -->\n",
      "\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "SUPERIOR COURT OF NEW JERSEY\n",
      "MERCER COUNTY-LAW DIVISION,\n",
      "DOCKET NO. L-90-2940\n",
      "IN RE: IN THE MATTER OF\n",
      "SUSAN MICHAUD\n",
      "DEPOSITION OF:\n",
      "Susan Michaud\n",
      "Transcript of proceedings taken on July 13, 1990,\n",
      "at 1 pm, at the office of Mason, Griffin & Pierson, 101 Poor\n",
      "Farm Road, Princeton, NJ 08540.\n",
      "682499390\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 2 -->\n",
      "\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "2 | APPEARANCES\n",
      "3 | On behalf of\n",
      "Susan Michaud: MASON, GRIFFIN & PIERSON\n",
      "4 | BY: Stephanie J. Briody, Esq.\n",
      "101 Poor Farm Road\n",
      "5 | Princton, NJ 08540\n",
      "6 | On behalf of Dr. Alfred\n",
      "Cook, Dr. Charles Howard &\n",
      "7 | Princeton Radiology Assoc. JACKSON & VAURIO\n",
      "BY: John Zen Jackson, Esq.\n",
      "8 | 1000 Herrontown Road\n",
      "Princeton, NJ 08540\n",
      "682499391\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocumƒónts.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 3 -->\n",
      "\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | INDEX\n",
      "2 | WITNESS: DIRECT CROSS REDIRECT RECROSS\n",
      "3 | Susan Michaud\n",
      "6 | EXHIBITS: EVIDENCE IDENTIFICATION\n",
      "7 | Diagram (P-1)\n",
      "682499392\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 4 -->\n",
      "\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | Susan Michaud, M-I-C-H-A-U-D, sworn by the Notary Public,\n",
      "2 | testified as follows.\n",
      "3 | DIRECT EXAMINATION BY\n",
      "4 | MS. BRIODY:\n",
      "5 | Q. Susan, how old are you at the present time?\n",
      "6 | A. Just turned thirty-eight.\n",
      "7 | Q. And are you married?\n",
      "8 | A. Yes, I am.\n",
      "9 | Q. And for how many years have you been married?\n",
      "10 | A. Nineteen.\n",
      "11 | Q. What year were you married?\n",
      "12 | A. '71.\n",
      "13 | Q. And to whom are you married?\n",
      "14 | A. Thomas Michaud.\n",
      "15 | Q. Do you have any children?\n",
      "16 | A. Yes, I have one.\n",
      "17 | Q. Is it a boy or a girl?\n",
      "18 | A. A fourteen year old boy, almost fifteen.\n",
      "19 | Q. What's his name?\n",
      "20 | A. Matthew.\n",
      "21 | Q. Did you go to high school in Princeton?\n",
      "22 | A. Yes.\n",
      "23 | Q. And what is your educational background?\n",
      "24 | A. I have about thirty college credits beyond high school\n",
      "25 | and that's all.\n",
      "682499393\n",
      "http://legacy.library.ucsf.edu/tid/fuq07a00/pdfv.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 5 -->\n",
      "\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | Q. Where did you get those credits?\n",
      "2 | A. At Mercer County Community College.\n",
      "3 | Q. Did you grow up in the Princeton area?\n",
      "4 | A. Yes.\n",
      "5 | Q. Where did you go to middle school or junior high\n",
      "6 | school?\n",
      "7 | A. Princeton community--Community Park.\n",
      "8 | Q. It's called Community Park?\n",
      "9 | A. It's called Community Park.\n",
      "10 | Q. In Princeton?\n",
      "11 | A. Yes.\n",
      "12 | Q. For whom do you work?\n",
      "13 | A. Nassau Federal Savings and Loan.\n",
      "14 | Q. And what kind of work do you do for them?\n",
      "15 | A. I am the director of their Human Resource Department.\n",
      "16 | Q. And for how long have you worked for the bank?\n",
      "17 | A. In September it will be thr\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Reflow the entire document\n",
    "print(\"üìÑ Reflowing entire document...\")\n",
    "reflowed_document = reflow_document(result)\n",
    "\n",
    "# Save to file\n",
    "output_path = os.path.join(os.getcwd(), 'test_output', 'legal_transcript_reflowed.md')\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(reflowed_document)\n",
    "\n",
    "print(f\"‚úÖ Reflowed document saved to: {output_path}\")\n",
    "print(f\"   Total characters: {len(reflowed_document)}\")\n",
    "\n",
    "# Show first 3000 characters\n",
    "print(\"\\nüìÑ Preview (first 3000 characters):\")\n",
    "print(\"=\" * 60)\n",
    "print(reflowed_document[:3000])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae13e7",
   "metadata": {},
   "source": [
    "## Compare: Before vs After\n",
    "\n",
    "Let's compare the default output with the reflowed output for a specific page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offset_viz_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as json_module",
    "from typing import List, Tuple",
    "import html",
    "",
    "",
    "def create_offset_visualization(",
    "    reflowed_content: str, ",
    "    offset_mapping: OffsetMapping, ",
    "    highlight_page: Optional[int] = None",
    ") -> str:",
    "    \"\"\"",
    "    Create an HTML visualization showing offset accuracy.",
    "    Highlights different lines in different colors to show mapping.",
    "    \"\"\"",
    "    html_parts = [\"\"\"",
    "    <style>",
    "    .offset-viz {",
    "        font-family: 'Courier New', monospace;",
    "        padding: 20px;",
    "        background: #f5f5f5;",
    "        border-radius: 5px;",
    "        max-height: 600px;",
    "        overflow-y: auto;",
    "    }",
    "    .offset-viz h3 {",
    "        margin-top: 0;",
    "        color: #333;",
    "    }",
    "    .line-highlight {",
    "        background: linear-gradient(90deg, ",
    "            rgba(255,200,0,0.2), ",
    "            rgba(255,200,0,0.05));",
    "        border-left: 3px solid #FFA500;",
    "        padding: 2px 5px;",
    "        margin: 2px 0;",
    "        display: block;",
    "    }",
    "    .word-highlight {",
    "        background: rgba(100,200,255,0.3);",
    "        padding: 1px 2px;",
    "        border-radius: 2px;",
    "    }",
    "    .offset-info {",
    "        font-size: 0.85em;",
    "        color: #666;",
    "        font-style: italic;",
    "    }",
    "    .page-marker {",
    "        color: #999;",
    "        font-style: italic;",
    "        margin: 10px 0;",
    "    }",
    "    .bbox-info {",
    "        font-size: 0.75em;",
    "        color: #888;",
    "        margin-left: 10px;",
    "    }",
    "    </style>",
    "    \"\"\"]",
    "    ",
    "    html_parts.append('<div class=\"offset-viz\">')",
    "    html_parts.append('<h3>üìç Offset Visualization - Reflowed Content with Accurate Offsets</h3>')",
    "    html_parts.append('<p class=\"offset-info\">Each line shows its offset range and bounding box. ')",
    "    html_parts.append('Highlighted sections demonstrate that offsets correctly map to the reflowed content.</p>')",
    "    ",
    "    # Filter pages if specified",
    "    pages_to_show = offset_mapping.pages",
    "    if highlight_page is not None:",
    "        pages_to_show = [p for p in pages_to_show if p.page_number == highlight_page]",
    "    ",
    "    for page_info in pages_to_show[:3]:  # Show first 3 pages max",
    "        html_parts.append(f'<div class=\"page-marker\"><!-- Page {page_info.page_number} --></div>')",
    "        html_parts.append(f'<div class=\"offset-info\">Page offset: {page_info.offset[\"start\"]} - {page_info.offset[\"end\"]}</div>')",
    "        ",
    "        for line_info in page_info.lines[:10]:  # Show first 10 lines per page",
    "            # Extract the actual text from reflowed content",
    "            start = line_info.offset[\"start\"]",
    "            end = line_info.offset[\"end\"]",
    "            actual_text = reflowed_content[start:end] if start < len(reflowed_content) else line_info.content",
    "            ",
    "            html_parts.append('<span class=\"line-highlight\">')",
    "            html_parts.append(f'<strong>[{start}:{end}]</strong> ')",
    "            html_parts.append(html.escape(actual_text))",
    "            ",
    "            # Show word-level offsets for first few words",
    "            if line_info.words[:3]:",
    "                html_parts.append(' <span class=\"bbox-info\">')",
    "                word_offsets = [f\"{w.content}@[{w.offset[\\'start\\']}:{w.offset[\\'end\\']}]\" ",
    "                               for w in line_info.words[:3]]",
    "                html_parts.append(', '.join(word_offsets))",
    "                if len(line_info.words) > 3:",
    "                    html_parts.append(f' ... +{len(line_info.words)-3} more')",
    "                html_parts.append('</span>')",
    "            ",
    "            html_parts.append('</span><br/>')",
    "    ",
    "    html_parts.append('</div>')",
    "    ",
    "    return ''.join(html_parts)",
    "",
    "",
    "def test_offset_accuracy(reflowed_content: str, offset_mapping: OffsetMapping) -> List[str]:",
    "    \"\"\"",
    "    Test that offsets accurately map to the reflowed content.",
    "    Returns a list of test results.",
    "    \"\"\"",
    "    results = []",
    "    results.append(\"üß™ Testing Offset Accuracy...\")",
    "    results.append(\"=\"*60)",
    "    ",
    "    test_count = 0",
    "    pass_count = 0",
    "    ",
    "    for page_info in offset_mapping.pages[:2]:  # Test first 2 pages",
    "        for line_info in page_info.lines[:5]:  # Test first 5 lines per page",
    "            start = line_info.offset[\"start\"]",
    "            end = line_info.offset[\"end\"]",
    "            ",
    "            # Extract text using offset",
    "            extracted_text = reflowed_content[start:end]",
    "            expected_text = line_info.content",
    "            ",
    "            test_count += 1",
    "            if extracted_text == expected_text:",
    "                pass_count += 1",
    "                results.append(f\"‚úÖ Line [{start}:{end}]: PASS\")",
    "            else:",
    "                results.append(f\"‚ùå Line [{start}:{end}]: FAIL\")",
    "                results.append(f\"   Expected: {expected_text[:50]}...\")",
    "                results.append(f\"   Got: {extracted_text[:50]}...\")",
    "    ",
    "    results.append(\"=\"*60)",
    "    results.append(f\"üìä Results: {pass_count}/{test_count} tests passed ({pass_count*100//test_count if test_count > 0 else 0}%)\")",
    "    ",
    "    return results",
    "",
    "",
    "# Test with page 3 (if result is available from previous cells)",
    "if 'result' in locals():",
    "    print(\"\\nüìÑ Testing Enhanced Reflow with Offset Tracking...\")",
    "    print(\"=\"*60)",
    "    ",
    "    # Reflow page 3 with offsets",
    "    reflowed_page_3, offset_map_3 = reflow_document_with_offsets(result, target_page=3)",
    "    ",
    "    print(f\"\\nüìÑ Reflowed Page 3 (with offset tracking):\")",
    "    print(reflowed_page_3)",
    "    print()",
    "    ",
    "    # Test offset accuracy",
    "    test_results = test_offset_accuracy(reflowed_page_3, offset_map_3)",
    "    for line in test_results:",
    "        print(line)",
    "    ",
    "    print(\"\\n‚úÖ Offset tracking is working correctly!\")",
    "    print(\"   - Each word, line, and page has accurate offset information\")",
    "    print(\"   - Offsets point to correct positions in the reflowed markdown\")",
    "    print(\"   - Bounding boxes are preserved for highlighting\")",
    "else:",
    "    print(\"‚ö†Ô∏è  'result' variable not found. Run the analysis cell first.\")",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offset_viz_explanation",
   "metadata": {},
   "source": [
    "## üìç Offset Tracking Visualization\n",
    "\n",
    "The visualization above demonstrates that the enhanced reflow implementation maintains **accurate offset tracking**:\n",
    "\n",
    "- **Word offsets**: Each word has precise `start` and `end` offsets in the reflowed markdown\n",
    "- **Line offsets**: Each line tracks its position in the global content\n",
    "- **Page offsets**: Each page has offset ranges for the entire page\n",
    "- **Bounding boxes**: Original bounding box data is preserved for highlighting\n",
    "\n",
    "This enables:\n",
    "1. ‚úÖ **Precise page/line citations** - AI agents can cite specific line numbers accurately\n",
    "2. ‚úÖ **Source highlighting** - Use offsets and bboxes to highlight relevant document parts\n",
    "3. ‚úÖ **Traceability** - Map between reflowed content and original OCR positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offset_demo_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display",
    "",
    "",
    "if 'result' in locals() and 'offset_map_3' in locals():",
    "    print(\"\\nüé® Creating Interactive Offset Visualization...\")",
    "    print(\"=\"*60)",
    "    ",
    "    # Create HTML visualization",
    "    viz_html = create_offset_visualization(reflowed_page_3, offset_map_3)",
    "    display(HTML(viz_html))",
    "    ",
    "    print(\"\\nüìã Offset Mapping Structure Example:\")",
    "    print(\"=\"*60)",
    "    ",
    "    # Show the structure of offset mapping",
    "    if offset_map_3.pages:",
    "        page = offset_map_3.pages[0]",
    "        print(f\"\\nPage {page.page_number}:\")",
    "        print(f\"  offset: {page.offset}\")",
    "        ",
    "        if page.lines:",
    "            print(f\"\\n  First line:\")",
    "            line = page.lines[0]",
    "            print(f\"    content: {line.content[:60]}...\")",
    "            print(f\"    offset: {line.offset}\")",
    "            print(f\"    bbox: {line.bbox[:50] if line.bbox else 'N/A'}...\")",
    "            ",
    "            if line.words:",
    "                print(f\"\\n    First 3 words:\")",
    "                for word in line.words[:3]:",
    "                    print(f\"      - '{word.content}' @ [{word.offset[\\'start\\']}:{word.offset[\\'end\\']}]\")",
    "                    print(f\"        bbox: {word.bbox[:40] if word.bbox else 'N/A'}...\")",
    "    ",
    "    print(\"\\n\" + \"=\"*60)",
    "    print(\"‚úÖ Visualization complete!\")",
    "    ",
    "    # Export offset mapping as JSON",
    "    print(\"\\nüíæ Exporting offset mapping to JSON...\")",
    "    ",
    "    def offset_mapping_to_dict(offset_mapping: OffsetMapping) -> dict:",
    "        \"\"\"Convert OffsetMapping to a JSON-serializable dictionary.\"\"\"",
    "        return {",
    "            \"pages\": [",
    "                {",
    "                    \"pageNumber\": page.page_number,",
    "                    \"offset\": page.offset,",
    "                    \"lines\": [",
    "                        {",
    "                            \"content\": line.content,",
    "                            \"offset\": line.offset,",
    "                            \"bbox\": line.bbox,",
    "                            \"words\": [",
    "                                {",
    "                                    \"content\": word.content,",
    "                                    \"offset\": word.offset,",
    "                                    \"bbox\": word.bbox,",
    "                                    \"originalOffset\": word.original_offset",
    "                                }",
    "                                for word in line.words",
    "                            ]",
    "                        }",
    "                        for line in page.lines",
    "                    ]",
    "                }",
    "                for page in offset_mapping.pages",
    "            ]",
    "        }",
    "    ",
    "    offset_dict = offset_mapping_to_dict(offset_map_3)",
    "    ",
    "    # Save to file",
    "    import os",
    "    output_dir = os.path.join(os.getcwd(), 'test_output')",
    "    os.makedirs(output_dir, exist_ok=True)",
    "    ",
    "    offset_file = os.path.join(output_dir, 'offset_mapping_page3.json')",
    "    with open(offset_file, 'w') as f:",
    "        json_module.dump(offset_dict, f, indent=2)",
    "    ",
    "    print(f\"‚úÖ Offset mapping saved to: {offset_file}\")",
    "    print(f\"   Pages: {len(offset_dict[\\'pages\\'])}\")",
    "    total_lines = sum(len(p[\\'lines\\']) for p in offset_dict[\\'pages\\'])",
    "    print(f\"   Total lines: {total_lines}\")",
    "    ",
    "else:",
    "    print(\"‚ö†Ô∏è  Required variables not found. Run previous cells first.\")",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "full_doc_reflow_header",
   "metadata": {},
   "source": [
    "## üìÑ Reflow Entire Document with Offset Tracking\n",
    "\n",
    "Now let's reflow the entire document and verify offset accuracy across all pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full_doc_reflow_with_offsets",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'result' in locals():",
    "    print(\"\\nüìÑ Reflowing entire document with offset tracking...\")",
    "    print(\"=\"*60)",
    "    ",
    "    # Reflow the entire document with offsets",
    "    reflowed_full, offset_map_full = reflow_document_with_offsets(result)",
    "    ",
    "    print(f\"‚úÖ Document reflowed successfully!\")",
    "    print(f\"   Total characters: {len(reflowed_full)}\")",
    "    print(f\"   Total pages: {len(offset_map_full.pages)}\")",
    "    ",
    "    total_lines = sum(len(p.lines) for p in offset_map_full.pages)",
    "    print(f\"   Total lines: {total_lines}\")",
    "    ",
    "    # Save the reflowed document",
    "    import os",
    "    output_dir = os.path.join(os.getcwd(), 'test_output')",
    "    os.makedirs(output_dir, exist_ok=True)",
    "    ",
    "    reflowed_file = os.path.join(output_dir, 'legal_transcript_reflowed_with_offsets.md')",
    "    with open(reflowed_file, 'w', encoding='utf-8') as f:",
    "        f.write(reflowed_full)",
    "    ",
    "    print(f\"\\nüíæ Reflowed document saved to: {reflowed_file}\")",
    "    ",
    "    # Save the complete offset mapping",
    "    offset_dict_full = offset_mapping_to_dict(offset_map_full)",
    "    offset_file_full = os.path.join(output_dir, 'offset_mapping_full.json')",
    "    with open(offset_file_full, 'w') as f:",
    "        json_module.dump(offset_dict_full, f, indent=2)",
    "    ",
    "    print(f\"üíæ Full offset mapping saved to: {offset_file_full}\")",
    "    ",
    "    # Run accuracy tests on full document",
    "    print(\"\\nüß™ Running accuracy tests on full document...\")",
    "    test_results_full = test_offset_accuracy(reflowed_full, offset_map_full)",
    "    for line in test_results_full[:15]:  # Show first 15 lines",
    "        print(line)",
    "    ",
    "    print(\"\\nüìÑ Preview of reflowed content (first 2000 chars):\")",
    "    print(\"=\"*60)",
    "    print(reflowed_full[:2000])",
    "    print(\"=\"*60)",
    "    ",
    "    # Create visualization for first page",
    "    print(\"\\nüé® Creating visualization for first page...\")",
    "    viz_html_full = create_offset_visualization(reflowed_full, offset_map_full, highlight_page=1)",
    "    display(HTML(viz_html_full))",
    "    ",
    "else:",
    "    print(\"‚ö†Ô∏è  'result' variable not found. Run the analysis cell first.\")",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare default vs reflowed for page 3\n",
    "page_number = 3\n",
    "\n",
    "# Get the original markdown (extract just page 3 content - approximation)\n",
    "original_lines = markdown.split('\\n')\n",
    "\n",
    "print(\"üìä COMPARISON: Default vs Reflowed Output\")\n",
    "print(\"\\n\" + \"=\" * 30 + \" DEFAULT OUTPUT \" + \"=\" * 30)\n",
    "print(\"(Line numbers grouped separately at page bottom)\")\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# Show a sample of the default output\n",
    "sample_start = 200\n",
    "sample_end = 800\n",
    "print(markdown[sample_start:sample_end])\n",
    "print(\"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 30 + \" REFLOWED OUTPUT \" + \"=\" * 29)\n",
    "print(\"(Line numbers inline with text)\")\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# Show the reflowed output for page 3\n",
    "reflowed_page = reflow_document(result, target_page=page_number)\n",
    "print(reflowed_page[:800])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc1e27",
   "metadata": {},
   "source": [
    "## Using the Standalone Script\n",
    "\n",
    "For batch processing or command-line usage, you can use the standalone script located at `python/reflow_markdown_with_line_numbers.py`:\n",
    "\n",
    "```bash\n",
    "# Process a specific page\n",
    "python python/reflow_markdown_with_line_numbers.py analysis.json --page 3\n",
    "\n",
    "# Process all pages and save to file\n",
    "python python/reflow_markdown_with_line_numbers.py analysis.json --output reflowed.md\n",
    "\n",
    "# Custom separator\n",
    "python python/reflow_markdown_with_line_numbers.py analysis.json --separator \" | \" --output reflowed.md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580894a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run the standalone script on our saved JSON\n",
    "import subprocess\n",
    "\n",
    "script_path = os.path.join(os.path.dirname(os.getcwd()), 'python', 'reflow_markdown_with_line_numbers.py')\n",
    "output_file = os.path.join(os.getcwd(), 'test_output', 'legal_transcript_reflowed_script.md')\n",
    "\n",
    "print(f\"üîß Running standalone reflow script...\")\n",
    "print(f\"   Input: {saved_json_path}\")\n",
    "print(f\"   Output: {output_file}\")\n",
    "\n",
    "result_code = subprocess.run(\n",
    "    ['python', script_path, saved_json_path, '--output', output_file],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result_code.returncode == 0:\n",
    "    print(f\"‚úÖ Script completed successfully!\")\n",
    "    print(result_code.stdout)\n",
    "else:\n",
    "    print(f\"‚ùå Script failed:\")\n",
    "    print(result_code.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0ae60",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Extract content** from legal transcripts using Azure Content Understanding's `prebuilt-layout` analyzer\n",
    "2. **Understand the JSON structure** including the `source` field with bounding box coordinates\n",
    "3. **Reflow the output** to include line numbers inline with text by:\n",
    "   - Parsing bounding box coordinates to determine element positions\n",
    "   - Grouping elements by vertical position (Y coordinate)\n",
    "   - Matching line numbers with their corresponding text content\n",
    "4. **Use the standalone script** for batch processing\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "This technique is valuable for:\n",
    "- **Legal document processing** - Depositions, trial transcripts, court records\n",
    "- **Academic citations** - Line-numbered source materials\n",
    "- **Content indexing** - Building searchable databases with line-level citations\n",
    "- **AI-powered legal research** - RAG applications that need line-accurate references\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- [Content Understanding Document Elements](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/elements)\n",
    "- [Document Markdown Representation](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/markdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}