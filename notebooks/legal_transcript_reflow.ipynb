{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ae1183",
   "metadata": {},
   "source": [
    "# Legal Transcript Line Number Reflow\n",
    "\n",
    "This notebook demonstrates how to process legal documents (depositions, court transcripts, trial records) with Azure Content Understanding and reflow the output to include inline line numbers.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Legal transcripts have a standardized format with **line numbers in the left margin** (typically 1-25 per page). These line numbers are critical for:\n",
    "- Citing specific testimony in legal briefs\n",
    "- Cross-referencing during depositions and trials\n",
    "- Creating accurate legal summaries\n",
    "\n",
    "By default, Content Understanding's markdown output groups these margin line numbers separately from the main text content. This notebook shows how to **reflow the output** to include line numbers inline with each text line.\n",
    "\n",
    "## Workflow\n",
    "1. **Load PDF** - Read the local legal transcript file\n",
    "2. **Content Extraction** - Use Azure Content Understanding to extract text with position data\n",
    "3. **Reflow** - Match line numbers with text using bounding box coordinates\n",
    "4. **Output** - Generate markdown with inline line numbers (e.g., `1 | witness testimony...`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae8a24",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Ensure your Azure AI service is configured by following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. Install the required packages to run this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1756b078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: aiohttp in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 1)) (3.13.3)\n",
      "Requirement already satisfied: azure-identity in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 2)) (1.25.1)\n",
      "Requirement already satisfied: azure-storage-blob in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 3)) (12.28.0)\n",
      "Requirement already satisfied: python-dotenv in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 4)) (1.2.1)\n",
      "Requirement already satisfied: requests in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 5)) (2.32.5)\n",
      "Requirement already satisfied: Pillow in /home/vscode/.local/lib/python3.11/site-packages (from -r ../requirements.txt (line 6)) (12.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp->-r ../requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp->-r ../requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp->-r ../requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp->-r ../requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp->-r ../requirements.txt (line 1)) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp->-r ../requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vscode/.local/lib/python3.11/site-packages (from aiohttp->-r ../requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in /home/vscode/.local/lib/python3.11/site-packages (from azure-identity->-r ../requirements.txt (line 2)) (1.38.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /home/vscode/.local/lib/python3.11/site-packages (from azure-identity->-r ../requirements.txt (line 2)) (46.0.3)\n",
      "Requirement already satisfied: msal>=1.30.0 in /home/vscode/.local/lib/python3.11/site-packages (from azure-identity->-r ../requirements.txt (line 2)) (1.34.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from azure-identity->-r ../requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from azure-identity->-r ../requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /home/vscode/.local/lib/python3.11/site-packages (from azure-storage-blob->-r ../requirements.txt (line 3)) (0.7.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vscode/.local/lib/python3.11/site-packages (from requests->-r ../requirements.txt (line 5)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.11/site-packages (from requests->-r ../requirements.txt (line 5)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.11/site-packages (from requests->-r ../requirements.txt (line 5)) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.11/site-packages (from requests->-r ../requirements.txt (line 5)) (2026.1.4)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from cryptography>=2.5->azure-identity->-r ../requirements.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity->-r ../requirements.txt (line 2)) (2.10.1)\n",
      "Requirement already satisfied: pycparser in /home/vscode/.local/lib/python3.11/site-packages (from cffi>=2.0.0->cryptography>=2.5->azure-identity->-r ../requirements.txt (line 2)) (3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13e310",
   "metadata": {},
   "source": [
    "## Create Azure AI Content Understanding Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6480fbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:dotenv.main:python-dotenv could not parse statement starting at line 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client created successfully\n",
      "   Endpoint: https://mmi-usw3-eft-foundry.services.ai.azure.com/\n",
      "   Credential: Subscription Key\n",
      "   API Version: 2025-11-01\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Optional\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Add the parent directory to the Python path to import the helper modules\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'python'))\n",
    "from content_understanding_client import AzureContentUnderstandingClient\n",
    "from extension.sample_helper import save_json_to_file \n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "API_VERSION = \"2025-11-01\"\n",
    "\n",
    "# Create token provider for Azure AD authentication\n",
    "def token_provider():\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    return token.token\n",
    "\n",
    "# Create the Content Understanding client\n",
    "try:\n",
    "    client = AzureContentUnderstandingClient(\n",
    "        endpoint=AZURE_AI_ENDPOINT,\n",
    "        api_version=API_VERSION,\n",
    "        subscription_key=AZURE_AI_API_KEY,\n",
    "        token_provider=token_provider if not AZURE_AI_API_KEY else None,\n",
    "        x_ms_useragent=\"azure-ai-content-understanding-python-sample-legal-reflow\"\n",
    "    )\n",
    "    credential_type = \"Subscription Key\" if AZURE_AI_API_KEY else \"Azure AD Token\"\n",
    "    print(f\"‚úÖ Client created successfully\")\n",
    "    print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "    print(f\"   Credential: {credential_type}\")\n",
    "    print(f\"   API Version: {API_VERSION}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create client: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb5f26a",
   "metadata": {},
   "source": [
    "## Configure Model Deployments\n",
    "\n",
    "> **üí° Note:** This step is only required **once per Azure Content Understanding resource**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4941027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuring default model deployments...\n",
      "‚úÖ Default model deployments configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Get model deployment names from environment variables\n",
    "GPT_4_1_DEPLOYMENT = os.getenv(\"GPT_4_1_DEPLOYMENT\")\n",
    "GPT_4_1_MINI_DEPLOYMENT = os.getenv(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "TEXT_EMBEDDING_3_LARGE_DEPLOYMENT = os.getenv(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "# Check if required deployments are configured\n",
    "missing_deployments = []\n",
    "if not GPT_4_1_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_DEPLOYMENT\")\n",
    "if not GPT_4_1_MINI_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "if not TEXT_EMBEDDING_3_LARGE_DEPLOYMENT:\n",
    "    missing_deployments.append(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "if missing_deployments:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing model deployment configuration(s): {missing_deployments}\")\n",
    "    print(\"   Add these to your .env file and restart the kernel.\")\n",
    "else:\n",
    "    print(f\"üìã Configuring default model deployments...\")\n",
    "    try:\n",
    "        result = client.update_defaults({\n",
    "            \"gpt-4.1\": GPT_4_1_DEPLOYMENT,\n",
    "            \"gpt-4.1-mini\": GPT_4_1_MINI_DEPLOYMENT,\n",
    "            \"text-embedding-3-large\": TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\n",
    "        })\n",
    "        print(f\"‚úÖ Default model deployments configured successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to configure defaults: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8544696",
   "metadata": {},
   "source": [
    "## Analyze Legal Transcript\n",
    "\n",
    "We'll use a publicly available deposition transcript from the Internet Archive. This is a real legal document with the standard line-numbered format used in depositions.\n",
    "\n",
    "**Sample Document:** [Farr Deposition Transcript](https://archive.org/details/799436-farr-deposition-transcript) (15 pages, Public Domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18932e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing legal transcript from local file...\n",
      "   Document: /workspaces/azure-ai-content-understanding-python/data/legal_examples/Trascript Example.pdf\n",
      "   Analyzer: prebuilt-layout\n",
      "   File size: 1,666,047 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:content_understanding_client:Analyzing binary file /workspaces/azure-ai-content-understanding-python/data/legal_examples/Trascript Example.pdf with analyzer: prebuilt-layout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Waiting for analysis to complete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:content_understanding_client:Request 1aa29c3a-eb1b-4742-ba2c-9c6910ddb120 in progress ...\n",
      "INFO:content_understanding_client:Request 1aa29c3a-eb1b-4742-ba2c-9c6910ddb120 in progress ...\n",
      "INFO:content_understanding_client:Request 1aa29c3a-eb1b-4742-ba2c-9c6910ddb120 in progress ...\n",
      "INFO:content_understanding_client:Request result is ready after 6.93 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analysis completed!\n",
      "\n",
      "üìÑ Document Information:\n",
      "   Pages: 1 - 52\n",
      "   Total pages: 52\n",
      "üíæ Analysis result saved to: test_output/legal_transcript_analysis_20260127_155636.json\n",
      "\n",
      "üíæ Full analysis saved to: test_output/legal_transcript_analysis_20260127_155636.json\n"
     ]
    }
   ],
   "source": [
    "# Analyze legal transcript from local file\n",
    "# Using the transcript example from the data/legal_examples folder\n",
    "document_path = os.path.join(os.path.dirname(os.getcwd()), 'data', 'legal_examples', 'Trascript Example.pdf')\n",
    "analyzer_id = 'prebuilt-layout'\n",
    "\n",
    "print(f\"üîç Analyzing legal transcript from local file...\")\n",
    "print(f\"   Document: {document_path}\")\n",
    "print(f\"   Analyzer: {analyzer_id}\")\n",
    "\n",
    "# Verify file exists\n",
    "if not os.path.exists(document_path):\n",
    "    raise FileNotFoundError(f\"Document not found: {document_path}\")\n",
    "\n",
    "file_size = os.path.getsize(document_path)\n",
    "print(f\"   File size: {file_size:,} bytes\")\n",
    "\n",
    "# Analyze the document using binary file path\n",
    "response = client.begin_analyze_binary(\n",
    "    analyzer_id=analyzer_id,\n",
    "    file_location=document_path\n",
    ")\n",
    "\n",
    "print(f\"‚è≥ Waiting for analysis to complete...\")\n",
    "result = client.poll_result(response)\n",
    "print(f\"‚úÖ Analysis completed!\")\n",
    "\n",
    "# Get document info\n",
    "contents = result.get(\"result\", {}).get(\"contents\", [])\n",
    "if contents:\n",
    "    content = contents[0]\n",
    "    if content.get(\"kind\") == \"document\":\n",
    "        print(f\"\\nüìÑ Document Information:\")\n",
    "        print(f\"   Pages: {content.get('startPageNumber')} - {content.get('endPageNumber')}\")\n",
    "        print(f\"   Total pages: {content.get('endPageNumber') - content.get('startPageNumber') + 1}\")\n",
    "\n",
    "# Save the full result for processing\n",
    "saved_json_path = save_json_to_file(result, filename_prefix=\"legal_transcript_analysis\")\n",
    "print(f\"\\nüíæ Full analysis saved to: {saved_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45dcbdb",
   "metadata": {},
   "source": [
    "## View Default Markdown Output\n",
    "\n",
    "Let's first look at Content Understanding's default markdown output. Notice how the **line numbers are grouped separately** at the bottom of each page's content rather than inline with the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5b60915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Default Markdown Output (first 2000 chars):\n",
      "============================================================\n",
      "# (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "SUPERIOR COURT OF NEW JERSEY\n",
      "MERCER COUNTY-LAW DIVISION,\n",
      "DOCKET NO. L-90-2940\n",
      "\n",
      ":\n",
      "\n",
      ":\n",
      "\n",
      "IN RE:\n",
      "IN THE MATTER OF\n",
      "SUSAN MICHAUD\n",
      "\n",
      ":\n",
      "\n",
      "DEPOSITION OF:\n",
      "\n",
      ":\n",
      "\n",
      "Susan Michaud\n",
      "\n",
      ":\n",
      "\n",
      ":\n",
      "\n",
      "Transcript of proceedings taken on July 13, 1990,\n",
      "at 1 pm, at the office of Mason, Griffin & Pierson, 101 Poor\n",
      "Farm Road, Princeton, NJ 08540.\n",
      "\n",
      "682499390\n",
      "\n",
      "<!-- PageFooter: http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocuments.ucsf.edu/docs/khhl0001 -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "\n",
      "# (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "\n",
      "2\n",
      "\n",
      "APPEARANCES\n",
      "\n",
      "On behalf of\n",
      "Susan Michaud:\n",
      "\n",
      "MASON, GRIFFIN & PIERSON\n",
      "BY: Stephanie J. Briody, Esq.\n",
      "101 Poor Farm Road\n",
      "Princton, NJ 08540\n",
      "\n",
      "On behalf of Dr. Alfred\n",
      "Cook, Dr. Charles Howard &\n",
      "Princeton Radiology Assoc.\n",
      "\n",
      "JACKSON & VAURIO\n",
      "BY: John Zen Jackson, Esq.\n",
      "1000 Herrontown Road\n",
      "Princeton, NJ 08540\n",
      "\n",
      "682499391\n",
      "\n",
      "<!-- PageFooter: http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocumƒónts.ucsf.edu/docs/khhl0001 -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "\n",
      "## (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "3\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "\n",
      "INDEX\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>WITNESS:</th>\n",
      "<th>DIRECT</th>\n",
      "<th>CROSS</th>\n",
      "<th>REDIRECT</th>\n",
      "<th>RECROSS</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Susan Michaud</td>\n",
      "<td>4</td>\n",
      "<td>40</td>\n",
      "<td>44</td>\n",
      "<td>44</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      "EXHIBITS:\n",
      "Diagram (P-1)\n",
      "\n",
      "EVIDENCE\n",
      "\n",
      "IDENTIFICATION\n",
      "\n",
      "23\n",
      "\n",
      "682499392\n",
      "\n",
      "<!-- PageFooter: http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001 -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "\n",
      "### (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "4\n",
      "\n",
      "Susan Michaud, M-I-C-H-A-U-D, sworn by the Notary Public,\n",
      "testified as follows.\n",
      "\n",
      "DIRECT EXAMINATION BY\n",
      "\n",
      "MS. BRIODY:\n",
      "\n",
      "Q.\n",
      "Susan, how old are you at the present time?\n",
      "\n",
      "A.\n",
      "Just turned thirty-eight.\n",
      "\n",
      "Q.\n",
      "And are you married?\n",
      "\n",
      "A. Yes, I am.\n",
      "\n",
      "Q.\n",
      "And for how many years have you been married?\n",
      "\n",
      "============================================================\n",
      "\n",
      "... (Total length: 66709 characters)\n"
     ]
    }
   ],
   "source": [
    "# Show the default markdown output (first 2000 characters)\n",
    "markdown = content.get(\"markdown\", \"\")\n",
    "\n",
    "print(\"üìÑ Default Markdown Output (first 2000 chars):\")\n",
    "print(\"=\" * 60)\n",
    "print(markdown[:2000])\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n... (Total length: {len(markdown)} characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb101887",
   "metadata": {},
   "source": [
    "## How Reflow Works\n",
    "\n",
    "The reflow algorithm uses **bounding box coordinates** from the JSON output to match line numbers with their corresponding text:\n",
    "\n",
    "### Step 1: Parse Coordinates\n",
    "Every element in CU's JSON has a `source` field with position data:\n",
    "```\n",
    "\"source\": \"D(1,1.0309,1.1277,1.131,1.1277,1.131,1.2711,1.0309,1.2711)\"\n",
    "           D(page, x1,y1, x2,y2, x3,y3, x4,y4)\n",
    "```\n",
    "\n",
    "### Step 2: Group by Vertical Position\n",
    "Elements with similar Y values (within ~0.15 inches) are on the same horizontal line.\n",
    "\n",
    "### Step 3: Sort Left-to-Right\n",
    "Within each group, sort by X coordinate. Line numbers (X ‚âà 1.0\") come before text content (X ‚âà 1.3\"+).\n",
    "\n",
    "### Step 4: Combine\n",
    "Pair line numbers with their corresponding text and output as `N | text content`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13abdb4f",
   "metadata": {},
   "source": [
    "## Reflow Functions & Data Structures\n",
    "\n",
    "The reflow process produces two outputs:\n",
    "1. **Reflowed Markdown** - Text with inline line numbers (e.g., `1 | witness testimony...`)\n",
    "2. **Offset Mapping** - A data structure tracking exact positions for citations and highlighting\n",
    "\n",
    "### Offset Data Structure\n",
    "\n",
    "```\n",
    "OffsetMapping\n",
    "‚îî‚îÄ‚îÄ pages: List[PageOffsetInfo]\n",
    "    ‚îú‚îÄ‚îÄ page_number: int\n",
    "    ‚îú‚îÄ‚îÄ offset: {\"start\": X, \"end\": Y}      # Character range in reflowed markdown\n",
    "    ‚îî‚îÄ‚îÄ lines: List[LineOffsetInfo]\n",
    "        ‚îú‚îÄ‚îÄ content: str                     # The line text\n",
    "        ‚îú‚îÄ‚îÄ offset: {\"start\": X, \"end\": Y}  # Character range for this line\n",
    "        ‚îú‚îÄ‚îÄ bbox: str                        # Bounding box from original PDF\n",
    "        ‚îî‚îÄ‚îÄ words: List[WordOffsetInfo]\n",
    "            ‚îú‚îÄ‚îÄ content: str\n",
    "            ‚îú‚îÄ‚îÄ offset: {\"start\": X, \"end\": Y}\n",
    "            ‚îú‚îÄ‚îÄ bbox: str\n",
    "            ‚îî‚îÄ‚îÄ original_offset: int         # Position in original OCR output\n",
    "```\n",
    "\n",
    "### Key Function\n",
    "\n",
    "```python\n",
    "reflowed_text, offset_mapping = reflow_document_with_offsets(result, target_page=None)\n",
    "```\n",
    "\n",
    "- `reflowed_text` (str): The reformatted markdown with inline line numbers\n",
    "- `offset_mapping` (OffsetMapping): Hierarchical offset data for citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a421c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reflow functions loaded successfully!\n",
      "\n",
      "üì¶ Available functions:\n",
      "   ‚Ä¢ reflow_document_with_offsets(json_data, target_page=None) -> (str, OffsetMapping)\n",
      "   ‚Ä¢ offset_mapping_to_dict(offset_mapping) -> dict  (for JSON export)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json as json_module\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LineElement:\n",
    "    \"\"\"Represents a line or element from the document with its position.\"\"\"\n",
    "    content: str\n",
    "    y_position: float  # Top Y coordinate\n",
    "    x_position: float  # Left X coordinate\n",
    "    page_number: int\n",
    "    is_line_number: bool = False\n",
    "    # Enhanced fields for offset tracking\n",
    "    original_offset: Optional[int] = None  # Offset in original markdown\n",
    "    original_length: Optional[int] = None  # Length in original markdown\n",
    "    bbox: Optional[str] = None  # Bounding box from source\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OffsetMapping:\n",
    "    \"\"\"Maps elements from original to reflowed content with offsets.\"\"\"\n",
    "    pages: List['PageOffsetInfo'] = field(default_factory=list)\n",
    "    \n",
    "    \n",
    "@dataclass\n",
    "class PageOffsetInfo:\n",
    "    \"\"\"Offset information for a page in reflowed content.\"\"\"\n",
    "    page_number: int\n",
    "    offset: Dict[str, int]  # {\"start\": X, \"end\": Y}\n",
    "    lines: List['LineOffsetInfo'] = field(default_factory=list)\n",
    "    \n",
    "\n",
    "@dataclass  \n",
    "class LineOffsetInfo:\n",
    "    \"\"\"Offset information for a line in reflowed content.\"\"\"\n",
    "    content: str\n",
    "    offset: Dict[str, int]  # {\"start\": X, \"end\": Y}\n",
    "    bbox: Optional[str] = None\n",
    "    words: List['WordOffsetInfo'] = field(default_factory=list)\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class WordOffsetInfo:\n",
    "    \"\"\"Offset information for a word in reflowed content.\"\"\"\n",
    "    content: str\n",
    "    offset: Dict[str, int]  # {\"start\": X, \"end\": Y}\n",
    "    bbox: str\n",
    "    original_offset: Optional[int] = None  # Original offset in OCR markdown\n",
    "\n",
    "\n",
    "def parse_source_coordinates(source: str) -> tuple[int, float, float, float, float, str]:\n",
    "    \"\"\"\n",
    "    Parse the source coordinate string from Content Understanding.\n",
    "    \n",
    "    The source format is: D(pageNumber,x1,y1,x2,y2,x3,y3,x4,y4)\n",
    "    where the points represent a quadrilateral (upper-left, upper-right, lower-right, lower-left)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (page_number, left_x, top_y, right_x, bottom_y, bbox_str)\n",
    "    \"\"\"\n",
    "    match = re.match(r'D\\((\\d+),([^)]+)\\)', source)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid source format: {source}\")\n",
    "    \n",
    "    page_number = int(match.group(1))\n",
    "    coords = [float(x) for x in match.group(2).split(',')]\n",
    "    \n",
    "    if len(coords) == 8:\n",
    "        # Bounding polygon: x1,y1,x2,y2,x3,y3,x4,y4\n",
    "        x1, y1, x2, y2, x3, y3, x4, y4 = coords\n",
    "        left_x = min(x1, x4)\n",
    "        top_y = min(y1, y2)\n",
    "        right_x = max(x2, x3)\n",
    "        bottom_y = max(y3, y4)\n",
    "    elif len(coords) == 4:\n",
    "        # Axis-aligned bounding box: left, top, width, height\n",
    "        left_x, top_y, width, height = coords\n",
    "        right_x = left_x + width\n",
    "        bottom_y = top_y + height\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected coordinate count: {source}\")\n",
    "    \n",
    "    return page_number, left_x, top_y, right_x, bottom_y, source\n",
    "\n",
    "\n",
    "def is_line_number(content: str) -> bool:\n",
    "    \"\"\"Check if content is a line number (1-99).\"\"\"\n",
    "    return content.strip().isdigit() and 1 <= int(content.strip()) <= 99\n",
    "\n",
    "\n",
    "def is_noise_element(content: str) -> bool:\n",
    "    \"\"\"Check if content is noise (bullets, single dots) that should be filtered.\"\"\"\n",
    "    content = content.strip()\n",
    "    return content in ['¬∑', '‚Ä¢', '‚àô'] or (len(content) == 1 and not content.isalnum())\n",
    "\n",
    "\n",
    "def extract_lines_from_page(page_data: dict) -> list[LineElement]:\n",
    "    \"\"\"Extract all lines from a page with position and offset information.\"\"\"\n",
    "    lines = []\n",
    "    \n",
    "    for line in page_data.get('lines', []):\n",
    "        content = line.get('content', '').strip()\n",
    "        if not content or is_noise_element(content):\n",
    "            continue\n",
    "        \n",
    "        source = line.get('source', '')\n",
    "        if not source:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            page_num, left_x, top_y, right_x, bottom_y, bbox = parse_source_coordinates(source)\n",
    "            \n",
    "            # Get offset information if available\n",
    "            original_offset = None\n",
    "            original_length = None\n",
    "            if 'span' in line:\n",
    "                original_offset = line['span'].get('offset')\n",
    "                original_length = line['span'].get('length')\n",
    "            \n",
    "            lines.append(LineElement(\n",
    "                content=content,\n",
    "                y_position=top_y,\n",
    "                x_position=left_x,\n",
    "                page_number=page_num,\n",
    "                is_line_number=is_line_number(content),\n",
    "                original_offset=original_offset,\n",
    "                original_length=original_length,\n",
    "                bbox=bbox\n",
    "            ))\n",
    "        except (ValueError, KeyError) as e:\n",
    "            # Skip lines with invalid coordinates\n",
    "            continue\n",
    "    \n",
    "    return lines\n",
    "\n",
    "\n",
    "def extract_words_from_page(page_data: dict) -> List[Dict]:\n",
    "    \"\"\"Extract all words from a page with their offset and bbox information.\"\"\"\n",
    "    words = []\n",
    "    \n",
    "    for word in page_data.get('words', []):\n",
    "        content = word.get('content', '').strip()\n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        span = word.get('span', {})\n",
    "        offset = span.get('offset')\n",
    "        length = span.get('length')\n",
    "        source = word.get('source', '')\n",
    "        \n",
    "        words.append({\n",
    "            'content': content,\n",
    "            'offset': offset,\n",
    "            'length': length,\n",
    "            'source': source\n",
    "        })\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "def group_lines_by_vertical_position(elements: list[LineElement], \n",
    "                                     y_tolerance: float = 0.15) -> list[list[LineElement]]:\n",
    "    \"\"\"Group elements that are on the same horizontal line.\"\"\"\n",
    "    if not elements:\n",
    "        return []\n",
    "    \n",
    "    # Sort by Y position (top to bottom)\n",
    "    sorted_elements = sorted(elements, key=lambda e: e.y_position)\n",
    "    \n",
    "    groups = []\n",
    "    current_group = [sorted_elements[0]]\n",
    "    \n",
    "    for element in sorted_elements[1:]:\n",
    "        # If Y position is close to current group, add to group\n",
    "        if abs(element.y_position - current_group[0].y_position) < y_tolerance:\n",
    "            current_group.append(element)\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [element]\n",
    "    \n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "\n",
    "def reflow_page_with_line_numbers_and_offsets(\n",
    "    page_data: dict, \n",
    "    separator: str = \" | \",\n",
    "    current_offset: int = 0\n",
    ") -> Tuple[str, PageOffsetInfo]:\n",
    "    \"\"\"\n",
    "    Reflow a single page's content to include line numbers inline.\n",
    "    Returns the reflowed text and offset mapping information.\n",
    "    \"\"\"\n",
    "    page_number = page_data.get('pageNumber', 0)\n",
    "    elements = extract_lines_from_page(page_data)\n",
    "    words_data = extract_words_from_page(page_data)\n",
    "    \n",
    "    # Create word lookup by content and position for matching\n",
    "    word_lookup = {}\n",
    "    for word in words_data:\n",
    "        key = (word['content'], word.get('offset'))\n",
    "        word_lookup[key] = word\n",
    "    \n",
    "    if not elements:\n",
    "        return \"\", PageOffsetInfo(page_number=page_number, offset={\"start\": current_offset, \"end\": current_offset})\n",
    "    \n",
    "    page_start_offset = current_offset\n",
    "    line_groups = group_lines_by_vertical_position(elements)\n",
    "    output_lines = []\n",
    "    line_offset_infos = []\n",
    "    \n",
    "    for group in line_groups:\n",
    "        # Sort by X position (left to right)\n",
    "        group.sort(key=lambda e: e.x_position)\n",
    "        \n",
    "        line_numbers = [e for e in group if e.is_line_number]\n",
    "        content_elements = [e for e in group if not e.is_line_number]\n",
    "        \n",
    "        if not content_elements:\n",
    "            continue\n",
    "        \n",
    "        # Build the line with offset tracking\n",
    "        line_start_offset = current_offset\n",
    "        line_parts = []\n",
    "        word_offset_infos = []\n",
    "        \n",
    "        # Add line number if present\n",
    "        if line_numbers:\n",
    "            line_num = line_numbers[0].content\n",
    "            line_parts.append(line_num)\n",
    "            \n",
    "            # Track word offset for line number\n",
    "            word_offset_infos.append(WordOffsetInfo(\n",
    "                content=line_num,\n",
    "                offset={\"start\": current_offset, \"end\": current_offset + len(line_num)},\n",
    "                bbox=line_numbers[0].bbox or \"\",\n",
    "                original_offset=line_numbers[0].original_offset\n",
    "            ))\n",
    "            current_offset += len(line_num)\n",
    "            \n",
    "            # Add separator\n",
    "            line_parts.append(separator)\n",
    "            current_offset += len(separator)\n",
    "        \n",
    "        # Add content elements\n",
    "        for i, elem in enumerate(content_elements):\n",
    "            if i > 0:\n",
    "                line_parts.append(' ')\n",
    "                current_offset += 1\n",
    "            \n",
    "            content = elem.content\n",
    "            line_parts.append(content)\n",
    "            \n",
    "            # Track word offset for content\n",
    "            word_offset_infos.append(WordOffsetInfo(\n",
    "                content=content,\n",
    "                offset={\"start\": current_offset, \"end\": current_offset + len(content)},\n",
    "                bbox=elem.bbox or \"\",\n",
    "                original_offset=elem.original_offset\n",
    "            ))\n",
    "            current_offset += len(content)\n",
    "        \n",
    "        # Build the complete line\n",
    "        line_text = ''.join(line_parts)\n",
    "        output_lines.append(line_text)\n",
    "        \n",
    "        # Create line offset info\n",
    "        line_offset_infos.append(LineOffsetInfo(\n",
    "            content=line_text,\n",
    "            offset={\"start\": line_start_offset, \"end\": current_offset},\n",
    "            bbox=content_elements[0].bbox if content_elements else None,\n",
    "            words=word_offset_infos\n",
    "        ))\n",
    "        \n",
    "        # Add newline\n",
    "        current_offset += 1  # for \\n\n",
    "    \n",
    "    reflowed_text = '\\n'.join(output_lines)\n",
    "    page_end_offset = current_offset\n",
    "    \n",
    "    page_offset_info = PageOffsetInfo(\n",
    "        page_number=page_number,\n",
    "        offset={\"start\": page_start_offset, \"end\": page_end_offset},\n",
    "        lines=line_offset_infos\n",
    "    )\n",
    "    \n",
    "    return reflowed_text, page_offset_info\n",
    "\n",
    "\n",
    "def reflow_document_with_offsets(\n",
    "    json_data: dict, \n",
    "    target_page: Optional[int] = None, \n",
    "    separator: str = \" | \"\n",
    ") -> Tuple[str, OffsetMapping]:\n",
    "    \"\"\"\n",
    "    Reflow an entire document or specific page with line numbers inline.\n",
    "    Returns the reflowed markdown and complete offset mapping.\n",
    "    \"\"\"\n",
    "    contents = json_data.get('result', {}).get('contents', [])\n",
    "    if not contents:\n",
    "        raise ValueError(\"No contents found in JSON data\")\n",
    "    \n",
    "    content = contents[0]\n",
    "    pages = content.get('pages', [])\n",
    "    if not pages:\n",
    "        raise ValueError(\"No pages found in document content\")\n",
    "    \n",
    "    offset_mapping = OffsetMapping()\n",
    "    current_offset = 0\n",
    "    \n",
    "    # Build the output string incrementally to track offsets accurately\n",
    "    output_buffer = []\n",
    "    \n",
    "    for page in pages:\n",
    "        page_number = page.get('pageNumber', 0)\n",
    "        if target_page is not None and page_number != target_page:\n",
    "            continue\n",
    "        \n",
    "        # Add page marker (only for multi-page output)\n",
    "        if target_page is None:\n",
    "            page_marker = f\"\\n<!-- Page {page_number} -->\\n\"\n",
    "            output_buffer.append(page_marker)\n",
    "            current_offset += len(page_marker)\n",
    "        \n",
    "        # Reflow the page\n",
    "        page_output, page_offset_info = reflow_page_with_line_numbers_and_offsets(\n",
    "            page, separator, current_offset\n",
    "        )\n",
    "        \n",
    "        if page_output:\n",
    "            output_buffer.append(page_output)\n",
    "            current_offset += len(page_output)\n",
    "            \n",
    "            # Add newline between pages (but track it properly)\n",
    "            if target_page is None:\n",
    "                output_buffer.append(\"\\n\")\n",
    "                current_offset += 1\n",
    "            \n",
    "            offset_mapping.pages.append(page_offset_info)\n",
    "    \n",
    "    # Join without adding extra characters - we've already tracked everything\n",
    "    reflowed_content = ''.join(output_buffer)\n",
    "    \n",
    "    return reflowed_content, offset_mapping\n",
    "\n",
    "\n",
    "def offset_mapping_to_dict(offset_mapping: OffsetMapping) -> dict:\n",
    "    \"\"\"Convert OffsetMapping to a JSON-serializable dictionary for file export.\"\"\"\n",
    "    return {\n",
    "        \"pages\": [\n",
    "            {\n",
    "                \"pageNumber\": page.page_number,\n",
    "                \"offset\": page.offset,\n",
    "                \"lines\": [\n",
    "                    {\n",
    "                        \"content\": line.content,\n",
    "                        \"offset\": line.offset,\n",
    "                        \"bbox\": line.bbox,\n",
    "                        \"words\": [\n",
    "                            {\n",
    "                                \"content\": word.content,\n",
    "                                \"offset\": word.offset,\n",
    "                                \"bbox\": word.bbox,\n",
    "                                \"originalOffset\": word.original_offset\n",
    "                            }\n",
    "                            for word in line.words\n",
    "                        ]\n",
    "                    }\n",
    "                    for line in page.lines\n",
    "                ]\n",
    "            }\n",
    "            for page in offset_mapping.pages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Reflow functions loaded successfully!\")\n",
    "print()\n",
    "print(\"üì¶ Available functions:\")\n",
    "print(\"   ‚Ä¢ reflow_document_with_offsets(json_data, target_page=None) -> (str, OffsetMapping)\")\n",
    "print(\"   ‚Ä¢ offset_mapping_to_dict(offset_mapping) -> dict  (for JSON export)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d139d",
   "metadata": {},
   "source": [
    "## Reflow a Single Page\n",
    "\n",
    "Let's reflow page 3 of the transcript to see the line numbers inline with the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ed1b4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Reflowed Output for Page 3:\n",
      "============================================================\n",
      "('(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\\n1 | INDEX\\n2 | WITNESS: DIRECT CROSS REDIRECT RECROSS\\n3 | Susan Michaud\\n6 | EXHIBITS: EVIDENCE IDENTIFICATION\\n7 | Diagram (P-1)\\n682499392\\nhttp://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001', OffsetMapping(pages=[PageOffsetInfo(page_number=3, offset={'start': 0, 'end': 291}, lines=[LineOffsetInfo(content='(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER', offset={'start': 0, 'end': 64}, bbox='D(3,0.9331,0.3227,7.6077,0.3156,7.6079,0.4938,0.9333,0.5024)', words=[WordOffsetInfo(content='(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER', offset={'start': 0, 'end': 64}, bbox='D(3,0.9331,0.3227,7.6077,0.3156,7.6079,0.4938,0.9333,0.5024)', original_offset=1107)]), LineOffsetInfo(content='1 | INDEX', offset={'start': 65, 'end': 74}, bbox='D(3,4.4368,1.7027,5.2381,1.7041,5.2378,1.8759,4.4365,1.8745)', words=[WordOffsetInfo(content='1', offset={'start': 65, 'end': 66}, bbox='D(3,2.0223,1.7203,2.1084,1.7203,2.1084,1.8342,2.0223,1.8342)', original_offset=1176), WordOffsetInfo(content='INDEX', offset={'start': 69, 'end': 74}, bbox='D(3,4.4368,1.7027,5.2381,1.7041,5.2378,1.8759,4.4365,1.8745)', original_offset=1243)]), LineOffsetInfo(content='2 | WITNESS: DIRECT CROSS REDIRECT RECROSS', offset={'start': 75, 'end': 117}, bbox='D(3,2.4404,2.0388,3.1377,2.0388,3.1377,2.1651,2.4404,2.1651)', words=[WordOffsetInfo(content='2', offset={'start': 75, 'end': 76}, bbox='D(3,2.0181,2.0468,2.1063,2.0468,2.1063,2.1606,2.0181,2.1606)', original_offset=1178), WordOffsetInfo(content='WITNESS:', offset={'start': 79, 'end': 87}, bbox='D(3,2.4404,2.0388,3.1377,2.0388,3.1377,2.1651,2.4404,2.1651)', original_offset=1268), WordOffsetInfo(content='DIRECT', offset={'start': 88, 'end': 94}, bbox='D(3,4.7107,2.0326,5.2295,2.0326,5.2295,2.159,4.7107,2.159)', original_offset=1286), WordOffsetInfo(content='CROSS', offset={'start': 95, 'end': 100}, bbox='D(3,5.4651,2.0347,5.9101,2.0317,5.9102,2.1595,5.4661,2.1628)', original_offset=1302), WordOffsetInfo(content='REDIRECT', offset={'start': 101, 'end': 109}, bbox='D(3,6.1592,2.0304,6.8772,2.0304,6.8772,2.1604,6.1592,2.1604)', original_offset=1317), WordOffsetInfo(content='RECROSS', offset={'start': 110, 'end': 117}, bbox='D(3,7.1179,2.0334,7.7405,2.0334,7.7405,2.1563,7.1179,2.1563)', original_offset=1335)]), LineOffsetInfo(content='3 | Susan Michaud', offset={'start': 118, 'end': 135}, bbox='D(3,2.4441,2.3724,3.5984,2.3681,3.5989,2.4965,2.4446,2.5007)', words=[WordOffsetInfo(content='3', offset={'start': 118, 'end': 119}, bbox='D(3,2.0181,2.3865,2.1001,2.3865,2.1001,2.4888,2.0181,2.4888)', original_offset=1180), WordOffsetInfo(content='Susan Michaud', offset={'start': 122, 'end': 135}, bbox='D(3,2.4441,2.3724,3.5984,2.3681,3.5989,2.4965,2.4446,2.5007)', original_offset=1363)]), LineOffsetInfo(content='6 | EXHIBITS: EVIDENCE IDENTIFICATION', offset={'start': 136, 'end': 173}, bbox='D(3,2.4529,3.3445,3.2315,3.347,3.2311,3.4707,2.4525,3.4682)', words=[WordOffsetInfo(content='6', offset={'start': 136, 'end': 137}, bbox='D(3,2.0295,3.3529,2.1084,3.3529,2.1084,3.4619,2.0295,3.4619)', original_offset=1186), WordOffsetInfo(content='EXHIBITS:', offset={'start': 140, 'end': 149}, bbox='D(3,2.4529,3.3445,3.2315,3.347,3.2311,3.4707,2.4525,3.4682)', original_offset=1446), WordOffsetInfo(content='EVIDENCE', offset={'start': 150, 'end': 158}, bbox='D(3,4.7148,3.3338,5.4038,3.3327,5.404,3.4568,4.7148,3.458)', original_offset=1471), WordOffsetInfo(content='IDENTIFICATION', offset={'start': 159, 'end': 173}, bbox='D(3,6.3418,3.3272,7.5869,3.3301,7.5869,3.4632,6.3418,3.4603)', original_offset=1481)]), LineOffsetInfo(content='7 | Diagram (P-1)', offset={'start': 174, 'end': 191}, bbox='D(3,2.4529,3.6557,3.5714,3.6557,3.5714,3.8004,2.4529,3.8004)', words=[WordOffsetInfo(content='7', offset={'start': 174, 'end': 175}, bbox='D(3,2.0223,3.672,2.1084,3.672,2.1084,3.7788,2.0223,3.7788)', original_offset=1188), WordOffsetInfo(content='Diagram (P-1)', offset={'start': 178, 'end': 191}, bbox='D(3,2.4529,3.6557,3.5714,3.6557,3.5714,3.8004,2.4529,3.8004)', original_offset=1456)]), LineOffsetInfo(content='682499392', offset={'start': 192, 'end': 201}, bbox='D(3,6.5853,9.8443,7.7571,9.8323,7.7585,10.0164,6.5867,10.0285)', words=[WordOffsetInfo(content='682499392', offset={'start': 192, 'end': 201}, bbox='D(3,6.5853,9.8443,7.7571,9.8323,7.7585,10.0164,6.5867,10.0285)', original_offset=1501)]), LineOffsetInfo(content='http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001', offset={'start': 202, 'end': 290}, bbox='D(3,0.0117,10.7881,6.6074,10.782,6.6074,10.967,0.0119,10.9732)', words=[WordOffsetInfo(content='http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001', offset={'start': 202, 'end': 290}, bbox='D(3,0.0117,10.7881,6.6074,10.782,6.6074,10.967,0.0119,10.9732)', original_offset=1529)])])]))\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Reflow a single page (page 3)\n",
    "page_to_reflow = 3\n",
    "\n",
    "print(f\"üìÑ Reflowed Output for Page {page_to_reflow}:\")\n",
    "print(\"=\" * 60)\n",
    "reflowed_page = reflow_document_with_offsets(result, target_page=page_to_reflow)\n",
    "print(reflowed_page)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a635c8",
   "metadata": {},
   "source": [
    "## Reflow Entire Document\n",
    "\n",
    "Now let's reflow the entire document and save it to a file.\n",
    "\n",
    "### üìÅ Output File\n",
    "This cell saves the reflowed markdown to:\n",
    "```\n",
    "notebooks/test_output/legal_transcript_reflowed.md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc9a0906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Reflowing entire document...\n",
      "‚úÖ Reflowed document saved to: /workspaces/azure-ai-content-understanding-python/notebooks/test_output/legal_transcript_reflowed.md\n",
      "   Total characters: 65627\n",
      "\n",
      "üìÑ Preview (first 3000 characters):\n",
      "============================================================\n",
      "\n",
      "<!-- Page 1 -->\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "SUPERIOR COURT OF NEW JERSEY\n",
      "MERCER COUNTY-LAW DIVISION,\n",
      "DOCKET NO. L-90-2940\n",
      "IN RE: IN THE MATTER OF\n",
      "SUSAN MICHAUD\n",
      "DEPOSITION OF:\n",
      "Susan Michaud\n",
      "Transcript of proceedings taken on July 13, 1990,\n",
      "at 1 pm, at the office of Mason, Griffin & Pierson, 101 Poor\n",
      "Farm Road, Princeton, NJ 08540.\n",
      "682499390\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 2 -->\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "2 | APPEARANCES\n",
      "3 | On behalf of\n",
      "Susan Michaud: MASON, GRIFFIN & PIERSON\n",
      "4 | BY: Stephanie J. Briody, Esq.\n",
      "101 Poor Farm Road\n",
      "5 | Princton, NJ 08540\n",
      "6 | On behalf of Dr. Alfred\n",
      "Cook, Dr. Charles Howard &\n",
      "7 | Princeton Radiology Assoc. JACKSON & VAURIO\n",
      "BY: John Zen Jackson, Esq.\n",
      "8 | 1000 Herrontown Road\n",
      "Princeton, NJ 08540\n",
      "682499391\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocumƒónts.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 3 -->\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | INDEX\n",
      "2 | WITNESS: DIRECT CROSS REDIRECT RECROSS\n",
      "3 | Susan Michaud\n",
      "6 | EXHIBITS: EVIDENCE IDENTIFICATION\n",
      "7 | Diagram (P-1)\n",
      "682499392\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 4 -->\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | Susan Michaud, M-I-C-H-A-U-D, sworn by the Notary Public,\n",
      "2 | testified as follows.\n",
      "3 | DIRECT EXAMINATION BY\n",
      "4 | MS. BRIODY:\n",
      "5 | Q. Susan, how old are you at the present time?\n",
      "6 | A. Just turned thirty-eight.\n",
      "7 | Q. And are you married?\n",
      "8 | A. Yes, I am.\n",
      "9 | Q. And for how many years have you been married?\n",
      "10 | A. Nineteen.\n",
      "11 | Q. What year were you married?\n",
      "12 | A. '71.\n",
      "13 | Q. And to whom are you married?\n",
      "14 | A. Thomas Michaud.\n",
      "15 | Q. Do you have any children?\n",
      "16 | A. Yes, I have one.\n",
      "17 | Q. Is it a boy or a girl?\n",
      "18 | A. A fourteen year old boy, almost fifteen.\n",
      "19 | Q. What's his name?\n",
      "20 | A. Matthew.\n",
      "21 | Q. Did you go to high school in Princeton?\n",
      "22 | A. Yes.\n",
      "23 | Q. And what is your educational background?\n",
      "24 | A. I have about thirty college credits beyond high school\n",
      "25 | and that's all.\n",
      "682499393\n",
      "http://legacy.library.ucsf.edu/tid/fuq07a00/pdfv.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 5 -->\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | Q. Where did you get those credits?\n",
      "2 | A. At Mercer County Community College.\n",
      "3 | Q. Did you grow up in the Princeton area?\n",
      "4 | A. Yes.\n",
      "5 | Q. Where did you go to middle school or junior high\n",
      "6 | school?\n",
      "7 | A. Princeton community--Community Park.\n",
      "8 | Q. It's called Community Park?\n",
      "9 | A. It's called Community Park.\n",
      "10 | Q. In Princeton?\n",
      "11 | A. Yes.\n",
      "12 | Q. For whom do you work?\n",
      "13 | A. Nassau Federal Savings and Loan.\n",
      "14 | Q. And what kind of work do you do for them?\n",
      "15 | A. I am the director of their Human Resource Department.\n",
      "16 | Q. And for how long have you worked for the bank?\n",
      "17 | A. In September it will be three ye\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Reflow the entire document\n",
    "print(\"üìÑ Reflowing entire document...\")\n",
    "reflowed_document, _ = reflow_document_with_offsets(result)\n",
    "\n",
    "# Save to file\n",
    "output_path = os.path.join(os.getcwd(), 'test_output', 'legal_transcript_reflowed.md')\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(reflowed_document)\n",
    "\n",
    "print(f\"‚úÖ Reflowed document saved to: {output_path}\")\n",
    "print(f\"   Total characters: {len(reflowed_document)}\")\n",
    "\n",
    "# Show first 3000 characters\n",
    "print(\"\\nüìÑ Preview (first 3000 characters):\")\n",
    "print(\"=\" * 60)\n",
    "print(reflowed_document[:3000])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae13e7",
   "metadata": {},
   "source": [
    "## Test Offset Accuracy\n",
    "\n",
    "This cell demonstrates the offset tracking feature by:\n",
    "1. Reflowing page 3 with offset tracking\n",
    "2. Testing that offsets correctly map to text positions in the reflowed content\n",
    "\n",
    "The offset mapping is stored in memory as `offset_map_3` (an `OffsetMapping` object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "offset_viz_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Testing Enhanced Reflow with Offset Tracking...\n",
      "============================================================\n",
      "\n",
      "üìÑ Reflowed Page 3 (with offset tracking):\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | INDEX\n",
      "2 | WITNESS: DIRECT CROSS REDIRECT RECROSS\n",
      "3 | Susan Michaud\n",
      "6 | EXHIBITS: EVIDENCE IDENTIFICATION\n",
      "7 | Diagram (P-1)\n",
      "682499392\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "üß™ Testing Offset Accuracy...\n",
      "============================================================\n",
      "‚úÖ Line [0:64]: PASS\n",
      "‚úÖ Line [65:74]: PASS\n",
      "‚úÖ Line [75:117]: PASS\n",
      "‚úÖ Line [118:135]: PASS\n",
      "‚úÖ Line [136:173]: PASS\n",
      "============================================================\n",
      "üìä Results: 5/5 tests passed (100%)\n",
      "\n",
      "‚úÖ Offset tracking is working correctly!\n",
      "   - Each word, line, and page has accurate offset information\n",
      "   - Offsets point to correct positions in the reflowed markdown\n",
      "   - Bounding boxes are preserved for highlighting\n"
     ]
    }
   ],
   "source": [
    "import json as json_module\n",
    "from typing import List, Tuple\n",
    "import html\n",
    "\n",
    "\n",
    "def create_offset_visualization(\n",
    "    reflowed_content: str, \n",
    "    offset_mapping: OffsetMapping, \n",
    "    highlight_page: Optional[int] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create an HTML visualization showing offset accuracy.\n",
    "    Highlights different lines in different colors to show mapping.\n",
    "    \"\"\"\n",
    "    html_parts = [\"\"\"\n",
    "    <style>\n",
    "    .offset-viz {\n",
    "        font-family: 'Courier New', monospace;\n",
    "        padding: 20px;\n",
    "        background: #f5f5f5;\n",
    "        border-radius: 5px;\n",
    "        max-height: 600px;\n",
    "        overflow-y: auto;\n",
    "    }\n",
    "    .offset-viz h3 {\n",
    "        margin-top: 0;\n",
    "        color: #333;\n",
    "    }\n",
    "    .line-highlight {\n",
    "        background: linear-gradient(90deg, \n",
    "            rgba(255,200,0,0.2), \n",
    "            rgba(255,200,0,0.05));\n",
    "        border-left: 3px solid #FFA500;\n",
    "        padding: 2px 5px;\n",
    "        margin: 2px 0;\n",
    "        display: block;\n",
    "    }\n",
    "    .word-highlight {\n",
    "        background: rgba(100,200,255,0.3);\n",
    "        padding: 1px 2px;\n",
    "        border-radius: 2px;\n",
    "    }\n",
    "    .offset-info {\n",
    "        font-size: 0.85em;\n",
    "        color: #666;\n",
    "        font-style: italic;\n",
    "    }\n",
    "    .page-marker {\n",
    "        color: #999;\n",
    "        font-style: italic;\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "    .bbox-info {\n",
    "        font-size: 0.75em;\n",
    "        color: #888;\n",
    "        margin-left: 10px;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\"]\n",
    "    \n",
    "    html_parts.append('<div class=\"offset-viz\">')\n",
    "    html_parts.append('<h3>üìç Offset Visualization - Reflowed Content with Accurate Offsets</h3>')\n",
    "    html_parts.append('<p class=\"offset-info\">Each line shows its offset range and bounding box. ')\n",
    "    html_parts.append('Highlighted sections demonstrate that offsets correctly map to the reflowed content.</p>')\n",
    "    \n",
    "    # Filter pages if specified\n",
    "    pages_to_show = offset_mapping.pages\n",
    "    if highlight_page is not None:\n",
    "        pages_to_show = [p for p in pages_to_show if p.page_number == highlight_page]\n",
    "    \n",
    "    for page_info in pages_to_show[:3]:  # Show first 3 pages max\n",
    "        html_parts.append(f'<div class=\"page-marker\"><!-- Page {page_info.page_number} --></div>')\n",
    "        html_parts.append(f'<div class=\"offset-info\">Page offset: {page_info.offset[\"start\"]} - {page_info.offset[\"end\"]}</div>')\n",
    "        \n",
    "        for line_info in page_info.lines[:10]:  # Show first 10 lines per page\n",
    "            # Extract the actual text from reflowed content\n",
    "            start = line_info.offset[\"start\"]\n",
    "            end = line_info.offset[\"end\"]\n",
    "            actual_text = reflowed_content[start:end] if start < len(reflowed_content) else line_info.content\n",
    "            \n",
    "            html_parts.append('<span class=\"line-highlight\">')\n",
    "            html_parts.append(f'<strong>[{start}:{end}]</strong> ')\n",
    "            html_parts.append(html.escape(actual_text))\n",
    "            \n",
    "            # Show word-level offsets for first few words\n",
    "            if line_info.words[:3]:\n",
    "                html_parts.append(' <span class=\"bbox-info\">')\n",
    "                word_offsets = [\n",
    "                    f\"{w.content}@[{w.offset['start']}:{w.offset['end']}]\" \n",
    "                    for w in line_info.words[:3]\n",
    "                ]\n",
    "                html_parts.append(', '.join(word_offsets))\n",
    "                if len(line_info.words) > 3:\n",
    "                    html_parts.append(f' ... +{len(line_info.words)-3} more')\n",
    "                html_parts.append('</span>')\n",
    "            \n",
    "            html_parts.append('</span><br/>')\n",
    "    \n",
    "    html_parts.append('</div>')\n",
    "    \n",
    "    return ''.join(html_parts)\n",
    "\n",
    "\n",
    "def test_offset_accuracy(reflowed_content: str, offset_mapping: OffsetMapping) -> List[str]:\n",
    "    \"\"\"\n",
    "    Test that offsets accurately map to the reflowed content.\n",
    "    Returns a list of test results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    results.append(\"üß™ Testing Offset Accuracy...\")\n",
    "    results.append(\"=\"*60)\n",
    "    \n",
    "    test_count = 0\n",
    "    pass_count = 0\n",
    "    \n",
    "    for page_info in offset_mapping.pages[:2]:  # Test first 2 pages\n",
    "        for line_info in page_info.lines[:5]:  # Test first 5 lines per page\n",
    "            start = line_info.offset[\"start\"]\n",
    "            end = line_info.offset[\"end\"]\n",
    "            \n",
    "            # Extract text using offset\n",
    "            extracted_text = reflowed_content[start:end]\n",
    "            expected_text = line_info.content\n",
    "            \n",
    "            test_count += 1\n",
    "            if extracted_text == expected_text:\n",
    "                pass_count += 1\n",
    "                results.append(f\"‚úÖ Line [{start}:{end}]: PASS\")\n",
    "            else:\n",
    "                results.append(f\"‚ùå Line [{start}:{end}]: FAIL\")\n",
    "                results.append(f\"   Expected: {expected_text[:50]}...\")\n",
    "                results.append(f\"   Got: {extracted_text[:50]}...\")\n",
    "    \n",
    "    results.append(\"=\"*60)\n",
    "    results.append(f\"üìä Results: {pass_count}/{test_count} tests passed ({pass_count*100//test_count if test_count > 0 else 0}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test with page 3 (if result is available from previous cells)\n",
    "if 'result' in locals():\n",
    "    print(\"\\nüìÑ Testing Enhanced Reflow with Offset Tracking...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Reflow page 3 with offsets\n",
    "    reflowed_page_3, offset_map_3 = reflow_document_with_offsets(result, target_page=3)\n",
    "    \n",
    "    print(f\"\\nüìÑ Reflowed Page 3 (with offset tracking):\")\n",
    "    print(reflowed_page_3)\n",
    "    print()\n",
    "    \n",
    "    # Test offset accuracy\n",
    "    test_results = test_offset_accuracy(reflowed_page_3, offset_map_3)\n",
    "    for line in test_results:\n",
    "        print(line)\n",
    "    \n",
    "    print(\"\\n‚úÖ Offset tracking is working correctly!\")\n",
    "    print(\"   - Each word, line, and page has accurate offset information\")\n",
    "    print(\"   - Offsets point to correct positions in the reflowed markdown\")\n",
    "    print(\"   - Bounding boxes are preserved for highlighting\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  'result' variable not found. Run the analysis cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offset_viz_explanation",
   "metadata": {},
   "source": [
    "## üìç Export Offset Mapping to JSON\n",
    "\n",
    "The cell below exports the offset mapping to a JSON file for use by other applications.\n",
    "\n",
    "### üìÅ Output File\n",
    "```\n",
    "notebooks/test_output/offset_mapping_page3.json\n",
    "```\n",
    "\n",
    "### JSON Structure\n",
    "```json\n",
    "{\n",
    "  \"pages\": [{\n",
    "    \"pageNumber\": 3,\n",
    "    \"offset\": {\"start\": 0, \"end\": 1234},\n",
    "    \"lines\": [{\n",
    "      \"content\": \"1 | Q. Please state your name.\",\n",
    "      \"offset\": {\"start\": 0, \"end\": 30},\n",
    "      \"bbox\": \"D(3,1.03,1.12,...)\",\n",
    "      \"words\": [...]\n",
    "    }]\n",
    "  }]\n",
    "}\n",
    "```\n",
    "\n",
    "This enables:\n",
    "- ‚úÖ **Precise page/line citations** - AI agents can cite specific line numbers accurately\n",
    "- ‚úÖ **Source highlighting** - Use offsets and bboxes to highlight relevant document parts\n",
    "- ‚úÖ **Traceability** - Map between reflowed content and original OCR positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "offset_demo_viz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® Creating Interactive Offset Visualization...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .offset-viz {\n",
       "        font-family: 'Courier New', monospace;\n",
       "        padding: 20px;\n",
       "        background: #f5f5f5;\n",
       "        border-radius: 5px;\n",
       "        max-height: 600px;\n",
       "        overflow-y: auto;\n",
       "    }\n",
       "    .offset-viz h3 {\n",
       "        margin-top: 0;\n",
       "        color: #333;\n",
       "    }\n",
       "    .line-highlight {\n",
       "        background: linear-gradient(90deg, \n",
       "            rgba(255,200,0,0.2), \n",
       "            rgba(255,200,0,0.05));\n",
       "        border-left: 3px solid #FFA500;\n",
       "        padding: 2px 5px;\n",
       "        margin: 2px 0;\n",
       "        display: block;\n",
       "    }\n",
       "    .word-highlight {\n",
       "        background: rgba(100,200,255,0.3);\n",
       "        padding: 1px 2px;\n",
       "        border-radius: 2px;\n",
       "    }\n",
       "    .offset-info {\n",
       "        font-size: 0.85em;\n",
       "        color: #666;\n",
       "        font-style: italic;\n",
       "    }\n",
       "    .page-marker {\n",
       "        color: #999;\n",
       "        font-style: italic;\n",
       "        margin: 10px 0;\n",
       "    }\n",
       "    .bbox-info {\n",
       "        font-size: 0.75em;\n",
       "        color: #888;\n",
       "        margin-left: 10px;\n",
       "    }\n",
       "    </style>\n",
       "    <div class=\"offset-viz\"><h3>üìç Offset Visualization - Reflowed Content with Accurate Offsets</h3><p class=\"offset-info\">Each line shows its offset range and bounding box. Highlighted sections demonstrate that offsets correctly map to the reflowed content.</p><div class=\"page-marker\"><!-- Page 3 --></div><div class=\"offset-info\">Page offset: 0 - 291</div><span class=\"line-highlight\"><strong>[0:64]</strong> (B&amp;W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER <span class=\"bbox-info\">(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER@[0:64]</span></span><br/><span class=\"line-highlight\"><strong>[65:74]</strong> 1 | INDEX <span class=\"bbox-info\">1@[65:66], INDEX@[69:74]</span></span><br/><span class=\"line-highlight\"><strong>[75:117]</strong> 2 | WITNESS: DIRECT CROSS REDIRECT RECROSS <span class=\"bbox-info\">2@[75:76], WITNESS:@[79:87], DIRECT@[88:94] ... +3 more</span></span><br/><span class=\"line-highlight\"><strong>[118:135]</strong> 3 | Susan Michaud <span class=\"bbox-info\">3@[118:119], Susan Michaud@[122:135]</span></span><br/><span class=\"line-highlight\"><strong>[136:173]</strong> 6 | EXHIBITS: EVIDENCE IDENTIFICATION <span class=\"bbox-info\">6@[136:137], EXHIBITS:@[140:149], EVIDENCE@[150:158] ... +1 more</span></span><br/><span class=\"line-highlight\"><strong>[174:191]</strong> 7 | Diagram (P-1) <span class=\"bbox-info\">7@[174:175], Diagram (P-1)@[178:191]</span></span><br/><span class=\"line-highlight\"><strong>[192:201]</strong> 682499392 <span class=\"bbox-info\">682499392@[192:201]</span></span><br/><span class=\"line-highlight\"><strong>[202:290]</strong> http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001 <span class=\"bbox-info\">http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001@[202:290]</span></span><br/></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Offset Mapping Structure Example:\n",
      "============================================================\n",
      "\n",
      "Page 3:\n",
      "  offset: {'start': 0, 'end': 291}\n",
      "\n",
      "  First line:\n",
      "    content: (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE O...\n",
      "    offset: {'start': 0, 'end': 64}\n",
      "    bbox: D(3,0.9331,0.3227,7.6077,0.3156,7.6079,0.4938,0.93...\n",
      "\n",
      "    First 3 words:\n",
      "      - '(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER' @ [0:64]\n",
      "        bbox: D(3,0.9331,0.3227,7.6077,0.3156,7.6079,0...\n",
      "\n",
      "============================================================\n",
      "\n",
      "üíæ Saving offset mapping to JSON file...\n",
      "‚úÖ Offset mapping saved to: /workspaces/azure-ai-content-understanding-python/notebooks/test_output/offset_mapping_page3.json\n",
      "   Pages: 1\n",
      "   Total lines: 8\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "if 'result' in locals() and 'offset_map_3' in locals():\n",
    "    print(\"\\nüé® Creating Interactive Offset Visualization...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create HTML visualization (renders inline in notebook output)\n",
    "    viz_html = create_offset_visualization(reflowed_page_3, offset_map_3)\n",
    "    display(HTML(viz_html))\n",
    "    \n",
    "    print(\"\\nüìã Offset Mapping Structure Example:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show the structure of offset mapping\n",
    "    if offset_map_3.pages:\n",
    "        page = offset_map_3.pages[0]\n",
    "        print(f\"\\nPage {page.page_number}:\")\n",
    "        print(f\"  offset: {page.offset}\")\n",
    "        \n",
    "        if page.lines:\n",
    "            print(f\"\\n  First line:\")\n",
    "            line = page.lines[0]\n",
    "            print(f\"    content: {line.content[:60]}...\")\n",
    "            print(f\"    offset: {line.offset}\")\n",
    "            print(f\"    bbox: {line.bbox[:50] if line.bbox else 'N/A'}...\")\n",
    "            \n",
    "            if line.words:\n",
    "                print(f\"\\n    First 3 words:\")\n",
    "                for word in line.words[:3]:\n",
    "                    start = word.offset['start']\n",
    "                    end = word.offset['end']\n",
    "                    print(f\"      - '{word.content}' @ [{start}:{end}]\")\n",
    "                    print(f\"        bbox: {word.bbox[:40] if word.bbox else 'N/A'}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Export offset mapping as JSON\n",
    "    print(\"\\nüíæ Saving offset mapping to JSON file...\")\n",
    "    \n",
    "    offset_dict = offset_mapping_to_dict(offset_map_3)\n",
    "    \n",
    "    # Save to file\n",
    "    output_dir = os.path.join(os.getcwd(), 'test_output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    offset_file = os.path.join(output_dir, 'offset_mapping_page3.json')\n",
    "    with open(offset_file, 'w') as f:\n",
    "        json_module.dump(offset_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Offset mapping saved to: {offset_file}\")\n",
    "    num_pages = len(offset_dict['pages'])\n",
    "    print(f\"   Pages: {num_pages}\")\n",
    "    total_lines = sum(len(p['lines']) for p in offset_dict['pages'])\n",
    "    print(f\"   Total lines: {total_lines}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Required variables not found. Run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "full_doc_reflow_header",
   "metadata": {},
   "source": [
    "## üìÑ Reflow Entire Document with Offset Tracking\n",
    "\n",
    "This cell processes the complete document and saves both the reflowed markdown and the full offset mapping.\n",
    "\n",
    "### üìÅ Output Files\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `notebooks/test_output/legal_transcript_reflowed_with_offsets.md` | Full reflowed document with inline line numbers |\n",
    "| `notebooks/test_output/offset_mapping_full.json` | Complete offset mapping for all pages |\n",
    "\n",
    "### In-Memory Variables\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `reflowed_full` | `str` | The complete reflowed markdown text |\n",
    "| `offset_map_full` | `OffsetMapping` | Hierarchical offset data for all pages |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "full_doc_reflow_with_offsets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Reflowing entire document with offset tracking...\n",
      "============================================================\n",
      "‚úÖ Document reflowed successfully!\n",
      "   Total characters: 65627\n",
      "   Total pages: 52\n",
      "   Total lines: 1390\n",
      "\n",
      "üíæ Reflowed document saved to: /workspaces/azure-ai-content-understanding-python/notebooks/test_output/legal_transcript_reflowed_with_offsets.md\n",
      "üíæ Full offset mapping saved to: /workspaces/azure-ai-content-understanding-python/notebooks/test_output/offset_mapping_full.json\n",
      "\n",
      "üß™ Running accuracy tests on full document...\n",
      "üß™ Testing Offset Accuracy...\n",
      "============================================================\n",
      "‚úÖ Line [17:81]: PASS\n",
      "‚úÖ Line [82:110]: PASS\n",
      "‚úÖ Line [111:138]: PASS\n",
      "‚úÖ Line [139:159]: PASS\n",
      "‚úÖ Line [160:183]: PASS\n",
      "‚úÖ Line [487:551]: PASS\n",
      "‚úÖ Line [552:567]: PASS\n",
      "‚úÖ Line [568:584]: PASS\n",
      "‚úÖ Line [585:624]: PASS\n",
      "‚úÖ Line [625:658]: PASS\n",
      "============================================================\n",
      "üìä Results: 10/10 tests passed (100%)\n",
      "\n",
      "üìÑ Preview of reflowed content (first 2000 chars):\n",
      "============================================================\n",
      "\n",
      "<!-- Page 1 -->\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "SUPERIOR COURT OF NEW JERSEY\n",
      "MERCER COUNTY-LAW DIVISION,\n",
      "DOCKET NO. L-90-2940\n",
      "IN RE: IN THE MATTER OF\n",
      "SUSAN MICHAUD\n",
      "DEPOSITION OF:\n",
      "Susan Michaud\n",
      "Transcript of proceedings taken on July 13, 1990,\n",
      "at 1 pm, at the office of Mason, Griffin & Pierson, 101 Poor\n",
      "Farm Road, Princeton, NJ 08540.\n",
      "682499390\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 2 -->\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "2 | APPEARANCES\n",
      "3 | On behalf of\n",
      "Susan Michaud: MASON, GRIFFIN & PIERSON\n",
      "4 | BY: Stephanie J. Briody, Esq.\n",
      "101 Poor Farm Road\n",
      "5 | Princton, NJ 08540\n",
      "6 | On behalf of Dr. Alfred\n",
      "Cook, Dr. Charles Howard &\n",
      "7 | Princeton Radiology Assoc. JACKSON & VAURIO\n",
      "BY: John Zen Jackson, Esq.\n",
      "8 | 1000 Herrontown Road\n",
      "Princeton, NJ 08540\n",
      "682499391\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocumƒónts.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 3 -->\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | INDEX\n",
      "2 | WITNESS: DIRECT CROSS REDIRECT RECROSS\n",
      "3 | Susan Michaud\n",
      "6 | EXHIBITS: EVIDENCE IDENTIFICATION\n",
      "7 | Diagram (P-1)\n",
      "682499392\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "\n",
      "<!-- Page 4 -->\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | Susan Michaud, M-I-C-H-A-U-D, sworn by the Notary Public,\n",
      "2 | testified as follows.\n",
      "3 | DIRECT EXAMINATION BY\n",
      "4 | MS. BRIODY:\n",
      "5 | Q. Susan, how old are you at the present time?\n",
      "6 | A. Just turned thirty-eight.\n",
      "7 | Q. And are you married?\n",
      "8 | A. Yes, I am.\n",
      "9 | Q. And for how many years have you been married?\n",
      "10 | A. Nineteen.\n",
      "11 | Q. What year were you married?\n",
      "12 | A. '71.\n",
      "13 | Q. And to whom are you married?\n",
      "14 | A. Thomas Michaud.\n",
      "15 | Q. Do you have any children?\n",
      "16 | A. Yes, I have one.\n",
      "17 | Q. Is it a boy or a girl?\n",
      "18 | A. A fourteen year old boy, almost fifteen.\n",
      "19 | Q. What's his name?\n",
      "20 | A. Matthew.\n",
      "21 | Q. Did \n",
      "============================================================\n",
      "\n",
      "üé® Creating visualization for first page...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .offset-viz {\n",
       "        font-family: 'Courier New', monospace;\n",
       "        padding: 20px;\n",
       "        background: #f5f5f5;\n",
       "        border-radius: 5px;\n",
       "        max-height: 600px;\n",
       "        overflow-y: auto;\n",
       "    }\n",
       "    .offset-viz h3 {\n",
       "        margin-top: 0;\n",
       "        color: #333;\n",
       "    }\n",
       "    .line-highlight {\n",
       "        background: linear-gradient(90deg, \n",
       "            rgba(255,200,0,0.2), \n",
       "            rgba(255,200,0,0.05));\n",
       "        border-left: 3px solid #FFA500;\n",
       "        padding: 2px 5px;\n",
       "        margin: 2px 0;\n",
       "        display: block;\n",
       "    }\n",
       "    .word-highlight {\n",
       "        background: rgba(100,200,255,0.3);\n",
       "        padding: 1px 2px;\n",
       "        border-radius: 2px;\n",
       "    }\n",
       "    .offset-info {\n",
       "        font-size: 0.85em;\n",
       "        color: #666;\n",
       "        font-style: italic;\n",
       "    }\n",
       "    .page-marker {\n",
       "        color: #999;\n",
       "        font-style: italic;\n",
       "        margin: 10px 0;\n",
       "    }\n",
       "    .bbox-info {\n",
       "        font-size: 0.75em;\n",
       "        color: #888;\n",
       "        margin-left: 10px;\n",
       "    }\n",
       "    </style>\n",
       "    <div class=\"offset-viz\"><h3>üìç Offset Visualization - Reflowed Content with Accurate Offsets</h3><p class=\"offset-info\">Each line shows its offset range and bounding box. Highlighted sections demonstrate that offsets correctly map to the reflowed content.</p><div class=\"page-marker\"><!-- Page 1 --></div><div class=\"offset-info\">Page offset: 17 - 470</div><span class=\"line-highlight\"><strong>[17:81]</strong> (B&amp;W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER <span class=\"bbox-info\">(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER@[17:81]</span></span><br/><span class=\"line-highlight\"><strong>[82:110]</strong> SUPERIOR COURT OF NEW JERSEY <span class=\"bbox-info\">SUPERIOR COURT OF NEW JERSEY@[82:110]</span></span><br/><span class=\"line-highlight\"><strong>[111:138]</strong> MERCER COUNTY-LAW DIVISION, <span class=\"bbox-info\">MERCER COUNTY-LAW DIVISION,@[111:138]</span></span><br/><span class=\"line-highlight\"><strong>[139:159]</strong> DOCKET NO. L-90-2940 <span class=\"bbox-info\">DOCKET NO. L-90-2940@[139:159]</span></span><br/><span class=\"line-highlight\"><strong>[160:183]</strong> IN RE: IN THE MATTER OF <span class=\"bbox-info\">IN RE:@[160:166], IN THE MATTER OF@[167:183]</span></span><br/><span class=\"line-highlight\"><strong>[184:197]</strong> SUSAN MICHAUD <span class=\"bbox-info\">SUSAN MICHAUD@[184:197]</span></span><br/><span class=\"line-highlight\"><strong>[198:212]</strong> DEPOSITION OF: <span class=\"bbox-info\">DEPOSITION OF:@[198:212]</span></span><br/><span class=\"line-highlight\"><strong>[213:226]</strong> Susan Michaud <span class=\"bbox-info\">Susan Michaud@[213:226]</span></span><br/><span class=\"line-highlight\"><strong>[227:276]</strong> Transcript of proceedings taken on July 13, 1990, <span class=\"bbox-info\">Transcript of proceedings taken on July 13, 1990,@[227:276]</span></span><br/><span class=\"line-highlight\"><strong>[277:337]</strong> at 1 pm, at the office of Mason, Griffin &amp; Pierson, 101 Poor <span class=\"bbox-info\">at 1 pm, at the office of Mason, Griffin & Pierson, 101 Poor@[277:337]</span></span><br/></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'result' in locals():\n",
    "    print(\"\\nüìÑ Reflowing entire document with offset tracking...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Reflow the entire document with offsets\n",
    "    reflowed_full, offset_map_full = reflow_document_with_offsets(result)\n",
    "    \n",
    "    print(f\"‚úÖ Document reflowed successfully!\")\n",
    "    print(f\"   Total characters: {len(reflowed_full)}\")\n",
    "    print(f\"   Total pages: {len(offset_map_full.pages)}\")\n",
    "    \n",
    "    total_lines = sum(len(p.lines) for p in offset_map_full.pages)\n",
    "    print(f\"   Total lines: {total_lines}\")\n",
    "    \n",
    "    # Save the reflowed document\n",
    "    import os\n",
    "    output_dir = os.path.join(os.getcwd(), 'test_output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    reflowed_file = os.path.join(output_dir, 'legal_transcript_reflowed_with_offsets.md')\n",
    "    with open(reflowed_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(reflowed_full)\n",
    "    \n",
    "    print(f\"\\nüíæ Reflowed document saved to: {reflowed_file}\")\n",
    "    \n",
    "    # Save the complete offset mapping\n",
    "    offset_dict_full = offset_mapping_to_dict(offset_map_full)\n",
    "    offset_file_full = os.path.join(output_dir, 'offset_mapping_full.json')\n",
    "    with open(offset_file_full, 'w') as f:\n",
    "        json_module.dump(offset_dict_full, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Full offset mapping saved to: {offset_file_full}\")\n",
    "    \n",
    "    # Run accuracy tests on full document\n",
    "    print(\"\\nüß™ Running accuracy tests on full document...\")\n",
    "    test_results_full = test_offset_accuracy(reflowed_full, offset_map_full)\n",
    "    for line in test_results_full[:15]:  # Show first 15 lines\n",
    "        print(line)\n",
    "    \n",
    "    print(\"\\nüìÑ Preview of reflowed content (first 2000 chars):\")\n",
    "    print(\"=\"*60)\n",
    "    print(reflowed_full[:2000])\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create visualization for first page\n",
    "    print(\"\\nüé® Creating visualization for first page...\")\n",
    "    viz_html_full = create_offset_visualization(reflowed_full, offset_map_full, highlight_page=1)\n",
    "    display(HTML(viz_html_full))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  'result' variable not found. Run the analysis cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d5dd7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä COMPARISON: Default vs Reflowed Output\n",
      "\n",
      "============================== DEFAULT OUTPUT ==============================\n",
      "(Line numbers grouped separately at page bottom)\n",
      "----------------------------------------------------------------------------\n",
      "ITION OF:\n",
      "\n",
      ":\n",
      "\n",
      "Susan Michaud\n",
      "\n",
      ":\n",
      "\n",
      ":\n",
      "\n",
      "Transcript of proceedings taken on July 13, 1990,\n",
      "at 1 pm, at the office of Mason, Griffin & Pierson, 101 Poor\n",
      "Farm Road, Princeton, NJ 08540.\n",
      "\n",
      "682499390\n",
      "\n",
      "<!-- PageFooter: http://legacy.library.ucsf.e6u/tid/fuq07a00/pdfv.industrydocuments.ucsf.edu/docs/khhl0001 -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "\n",
      "# (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "\n",
      "2\n",
      "\n",
      "APPEARANCES\n",
      "\n",
      "On behalf of\n",
      "Susan Michaud:\n",
      "\n",
      "MASON, GRIFFIN & PIERSON\n",
      "BY: Stephanie J. Briody, Esq.\n",
      "101 Poor Farm Road\n",
      "Princton, NJ 08540\n",
      "\n",
      "On b\n",
      "...\n",
      "\n",
      "============================== REFLOWED OUTPUT =============================\n",
      "(Line numbers inline with text)\n",
      "----------------------------------------------------------------------------\n",
      "(B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATION PROTECTIVE ORDER\n",
      "1 | INDEX\n",
      "2 | WITNESS: DIRECT CROSS REDIRECT RECROSS\n",
      "3 | Susan Michaud\n",
      "6 | EXHIBITS: EVIDENCE IDENTIFICATION\n",
      "7 | Diagram (P-1)\n",
      "682499392\n",
      "http://legacy.library.ucsf.e6u/tid/fuq07a00/pdf.industrydocuments.ucsf.edu/docs/khhl0001\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Compare default vs reflowed for page 3\n",
    "page_number = 3\n",
    "\n",
    "# Get the original markdown (extract just page 3 content - approximation)\n",
    "original_lines = markdown.split('\\n')\n",
    "\n",
    "print(\"üìä COMPARISON: Default vs Reflowed Output\")\n",
    "print(\"\\n\" + \"=\" * 30 + \" DEFAULT OUTPUT \" + \"=\" * 30)\n",
    "print(\"(Line numbers grouped separately at page bottom)\")\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# Show a sample of the default output\n",
    "sample_start = 200\n",
    "sample_end = 800\n",
    "print(markdown[sample_start:sample_end])\n",
    "print(\"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 30 + \" REFLOWED OUTPUT \" + \"=\" * 29)\n",
    "print(\"(Line numbers inline with text)\")\n",
    "print(\"-\" * 76)\n",
    "\n",
    "# Show the reflowed output for page 3\n",
    "reflowed_page, _ = reflow_document_with_offsets(result, target_page=page_number)\n",
    "print(reflowed_page[:800])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc1e27",
   "metadata": {},
   "source": [
    "## Using the Standalone Script\n",
    "\n",
    "For batch processing or command-line usage, you can use the standalone script located at `Customers/Eve Legal/reflow_markdown_with_line_numbers.py`:\n",
    "\n",
    "```bash\n",
    "# Process a specific page\n",
    "python \"Customers/Eve Legal/reflow_markdown_with_line_numbers.py\" analysis.json --page 3\n",
    "\n",
    "# Process all pages and save to file\n",
    "python \"Customers/Eve Legal/reflow_markdown_with_line_numbers.py\" analysis.json --output reflowed.md\n",
    "\n",
    "# Custom separator\n",
    "python \"Customers/Eve Legal/reflow_markdown_with_line_numbers.py\" analysis.json --separator \" | \" --output reflowed.md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "580894a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Running standalone reflow script...\n",
      "   Input: test_output/legal_transcript_analysis_20260127_155636.json\n",
      "   Output: /workspaces/azure-ai-content-understanding-python/notebooks/test_output/legal_transcript_reflowed_script.md\n",
      "‚úÖ Script completed successfully!\n",
      "Output written to: /workspaces/azure-ai-content-understanding-python/notebooks/test_output/legal_transcript_reflowed_script.md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Run the standalone script on our saved JSON\n",
    "import subprocess\n",
    "\n",
    "script_path = os.path.join(os.path.dirname(os.getcwd()), 'Customers', 'Eve Legal', 'reflow_markdown_with_line_numbers.py')\n",
    "output_file = os.path.join(os.getcwd(), 'test_output', 'legal_transcript_reflowed_script.md')\n",
    "\n",
    "# Check if the script exists\n",
    "if not os.path.exists(script_path):\n",
    "    print(f\"‚ö†Ô∏è  Standalone script not found at: {script_path}\")\n",
    "    print(\"   Skipping standalone script test.\")\n",
    "else:\n",
    "    print(f\"üîß Running standalone reflow script...\")\n",
    "    print(f\"   Input: {saved_json_path}\")\n",
    "    print(f\"   Output: {output_file}\")\n",
    "\n",
    "    result_code = subprocess.run(\n",
    "        ['python', script_path, saved_json_path, '--output', output_file],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    if result_code.returncode == 0:\n",
    "        print(f\"‚úÖ Script completed successfully!\")\n",
    "        print(result_code.stdout)\n",
    "    else:\n",
    "        print(f\"‚ùå Script failed:\")\n",
    "        print(result_code.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0ae60",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Extract content** from legal transcripts using Azure Content Understanding's `prebuilt-layout` analyzer\n",
    "2. **Understand the JSON structure** including the `source` field with bounding box coordinates\n",
    "3. **Reflow the output** to include line numbers inline with text by:\n",
    "   - Parsing bounding box coordinates to determine element positions\n",
    "   - Grouping elements by vertical position (Y coordinate)\n",
    "   - Matching line numbers with their corresponding text content\n",
    "4. **Track offsets** for precise citations and source highlighting\n",
    "5. **Export offset mappings** to JSON for use by other applications\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ Output Files Generated\n",
    "\n",
    "After running all cells, you'll find these files in `notebooks/test_output/`:\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `legal_transcript_reflowed.md` | Reflowed document (markdown only, no offset data) |\n",
    "| `legal_transcript_reflowed_with_offsets.md` | Full reflowed document with inline line numbers |\n",
    "| `offset_mapping_page3.json` | Offset mapping for page 3 only |\n",
    "| `offset_mapping_full.json` | **Complete offset mapping for all pages** |\n",
    "\n",
    "### üìä In-Memory Variables\n",
    "\n",
    "| Variable | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `reflowed_full` | `str` | Complete reflowed markdown |\n",
    "| `offset_map_full` | `OffsetMapping` | Full offset data (pages ‚Üí lines ‚Üí words) |\n",
    "| `reflowed_page_3` | `str` | Reflowed markdown for page 3 |\n",
    "| `offset_map_3` | `OffsetMapping` | Offset data for page 3 |\n",
    "\n",
    "---\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "This technique is valuable for:\n",
    "- **Legal document processing** - Depositions, trial transcripts, court records\n",
    "- **Academic citations** - Line-numbered source materials\n",
    "- **Content indexing** - Building searchable databases with line-level citations\n",
    "- **AI-powered legal research** - RAG applications that need line-accurate references\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- [Content Understanding Document Elements](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/elements)\n",
    "- [Document Markdown Representation](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/markdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
