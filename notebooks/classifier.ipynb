{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Content Understanding - Classifier and Analyzer Demo\n",
    "\n",
    "This notebook demonstrates how to use the Azure AI Content Understanding service to:\n",
    "1. Create a classifier for document categorization\n",
    "2. Create a custom analyzer to extract specific fields\n",
    "3. Combine the classifier and analyzers to classify, optionally split, and analyze documents within a flexible processing pipeline\n",
    "\n",
    "For more detailed information before getting started, please refer to the official documentation:\n",
    "[Understanding Classifiers in Azure AI Services](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/concepts/classifier)\n",
    "\n",
    "## Prerequisites\n",
    "1. Ensure the Azure AI service is configured by following the [setup steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. Install the required packages to run this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure AI Content Understanding Client\n",
    "\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class that provides functions to interact with the Content Understanding API. Prior to the official release of the Content Understanding SDK, it serves as a lightweight SDK.\n",
    ">\n",
    "> Fill in the constants **AZURE_AI_ENDPOINT**, **AZURE_AI_API_VERSION**, and **AZURE_AI_API_KEY** with the details from your Azure AI Service.\n",
    "\n",
    "> ‚ö†Ô∏è Important:\n",
    "You must update the code below to use your preferred Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments in the code and modify those sections accordingly.\n",
    "Skipping this step may cause the sample to not run correctly.\n",
    "\n",
    "> ‚ö†Ô∏è Note: While using a subscription key is supported, it is strongly recommended to use a token provider with Azure Active Directory (AAD) for enhanced security in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# Add the parent directory to the Python path to import the sample_helper module\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'python'))\n",
    "from content_understanding_client import AzureContentUnderstandingClient\n",
    "from extension.sample_helper import save_json_to_file \n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# API Configuration\n",
    "API_VERSION = \"2025-11-01\"  # GA version\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key; only one is required\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set it in your \".env\" file if not using token authentication\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "\n",
    "# IMPORTANT: Choose your authentication method\n",
    "# Option 1: Using Subscription Key (simpler but less secure)\n",
    "if AZURE_AI_API_KEY:\n",
    "    client = AzureContentUnderstandingClient(\n",
    "        endpoint=AZURE_AI_ENDPOINT,\n",
    "        api_version=API_VERSION,\n",
    "        subscription_key=AZURE_AI_API_KEY\n",
    "    )\n",
    "    print(\"‚úÖ AzureContentUnderstandingClient created with subscription key\")\n",
    "else:\n",
    "    # Option 2: Using Azure AD Token Provider (recommended for production)\n",
    "    credential = DefaultAzureCredential()\n",
    "    \n",
    "    # Create a token provider function that returns the access token\n",
    "    def get_token():\n",
    "        token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "        return token.token\n",
    "    \n",
    "    client = AzureContentUnderstandingClient(\n",
    "        endpoint=AZURE_AI_ENDPOINT,\n",
    "        api_version=API_VERSION,\n",
    "        token_provider=get_token\n",
    "    )\n",
    "    print(\"‚úÖ AzureContentUnderstandingClient created with token provider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model Deployments for Prebuilt Analyzers\n",
    "\n",
    "> **üí° Note:** This step is only required **once per Azure Content Understanding resource**, unless the GPT deployment has been changed. You can skip this section if:\n",
    "> - This configuration has already been run once for your resource, or\n",
    "> - Your administrator has already configured the model deployments for you\n",
    "\n",
    "Before using prebuilt analyzers, you need to configure the default model deployment mappings. This tells Content Understanding which model deployments to use.\n",
    "\n",
    "**Model Requirements:**\n",
    "- **GPT-4.1** - Required for most prebuilt analyzers (e.g., `prebuilt-invoice`, `prebuilt-receipt`, `prebuilt-idDocument`)\n",
    "- **GPT-4.1-mini** - Required for RAG analyzers (e.g., `prebuilt-documentSearch`, `prebuilt-audioSearch`, `prebuilt-videoSearch`)\n",
    "- **text-embedding-3-large** - Required for all prebuilt analyzers that use embeddings\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Deploy **GPT-4.1**, **GPT-4.1-mini**, and **text-embedding-3-large** models in Azure AI Foundry\n",
    "2. Set `GPT_4_1_DEPLOYMENT`, `GPT_4_1_MINI_DEPLOYMENT`, and `TEXT_EMBEDDING_3_LARGE_DEPLOYMENT` in your `.env` file with the deployment names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model deployment names from environment variables\n",
    "GPT_4_1_DEPLOYMENT = os.getenv(\"GPT_4_1_DEPLOYMENT\")\n",
    "GPT_4_1_MINI_DEPLOYMENT = os.getenv(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "TEXT_EMBEDDING_3_LARGE_DEPLOYMENT = os.getenv(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "# Check if required deployments are configured\n",
    "missing_deployments = []\n",
    "if not GPT_4_1_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_DEPLOYMENT\")\n",
    "if not GPT_4_1_MINI_DEPLOYMENT:\n",
    "    missing_deployments.append(\"GPT_4_1_MINI_DEPLOYMENT\")\n",
    "if not TEXT_EMBEDDING_3_LARGE_DEPLOYMENT:\n",
    "    missing_deployments.append(\"TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\")\n",
    "\n",
    "if missing_deployments:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing required model deployment configuration(s):\")\n",
    "    for deployment in missing_deployments:\n",
    "        print(f\"   - {deployment}\")\n",
    "    print(\"\\n   Prebuilt analyzers require GPT-4.1, GPT-4.1-mini, and text-embedding-3-large deployments.\")\n",
    "    print(\"   Please:\")\n",
    "    print(\"   1. Deploy all three models in Azure AI Foundry\")\n",
    "    print(\"   2. Add the following to notebooks/.env:\")\n",
    "    print(\"      GPT_4_1_DEPLOYMENT=<your-gpt-4.1-deployment-name>\")\n",
    "    print(\"      GPT_4_1_MINI_DEPLOYMENT=<your-gpt-4.1-mini-deployment-name>\")\n",
    "    print(\"      TEXT_EMBEDDING_3_LARGE_DEPLOYMENT=<your-text-embedding-3-large-deployment-name>\")\n",
    "    print(\"   3. Restart the kernel and run this cell again\")\n",
    "else:\n",
    "    print(f\"üìã Configuring default model deployments...\")\n",
    "    print(f\"   GPT-4.1 deployment: {GPT_4_1_DEPLOYMENT}\")\n",
    "    print(f\"   GPT-4.1-mini deployment: {GPT_4_1_MINI_DEPLOYMENT}\")\n",
    "    print(f\"   text-embedding-3-large deployment: {TEXT_EMBEDDING_3_LARGE_DEPLOYMENT}\")\n",
    "    \n",
    "    try:\n",
    "        # Update defaults to map model names to your deployments\n",
    "        result = client.update_defaults({\n",
    "            \"gpt-4.1\": GPT_4_1_DEPLOYMENT,\n",
    "            \"gpt-4.1-mini\": GPT_4_1_MINI_DEPLOYMENT,\n",
    "            \"text-embedding-3-large\": TEXT_EMBEDDING_3_LARGE_DEPLOYMENT\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Default model deployments configured successfully\")\n",
    "        print(f\"   Model mappings:\")\n",
    "        for model, deployment in result.get(\"modelDeployments\", {}).items():\n",
    "            print(f\"     {model} ‚Üí {deployment}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to configure defaults: {e}\")\n",
    "        print(f\"   This may happen if:\")\n",
    "        print(f\"   - One or more deployment names don't exist in your Azure AI Foundry project\")\n",
    "        print(f\"   - You don't have permission to update defaults\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Basic Classifier\n",
    "Classify document from URL using begin_classify API.\n",
    "\n",
    "High-level steps:\n",
    "1. Create a custom classifier\n",
    "2. Classify a document from a remote URL\n",
    "3. Save the classification result to a file\n",
    "4. Clean up the created classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique classifier ID\n",
    "analyzer_id = f\"notebooks_sample_classifier_{int(time.time())}\"\n",
    "\n",
    "# Define the classifier as a dictionary\n",
    "content_analyzer = {\n",
    "    \"baseAnalyzerId\": \"prebuilt-document\",\n",
    "    \"description\": f\"Custom classifier for URL classification demo: {analyzer_id}\",\n",
    "    \"config\": {\n",
    "        \"returnDetails\": True,\n",
    "        \"enableSegment\": True,\n",
    "        \"contentCategories\": {\n",
    "            \"Loan application\": {\n",
    "                \"description\": \"Documents submitted by individuals or businesses to request funding, typically including personal or business details, financial history, loan amount, purpose, and supporting documentation.\"\n",
    "            },\n",
    "            \"Invoice\": {\n",
    "                \"description\": \"Billing documents issued by sellers or service providers to request payment for goods or services, detailing items, prices, taxes, totals, and payment terms.\"\n",
    "            },\n",
    "            \"Bank_Statement\": {\n",
    "                \"description\": \"Official statements issued by banks that summarize account activity over a period, including deposits, withdrawals, fees, and balances.\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"models\": {\"completion\": \"gpt-4.1\"},\n",
    "    \"tags\": {\"demo_type\": \"url_classification\"}\n",
    "}\n",
    "\n",
    "# Create a custom classifier\n",
    "print(f\"üîß Creating custom classifier '{analyzer_id}'...\")\n",
    "\n",
    "# Start the classifier creation operation\n",
    "response = client.begin_create_analyzer(\n",
    "    analyzer_id=analyzer_id,\n",
    "    analyzer_template=content_analyzer,\n",
    ")\n",
    "\n",
    "# Wait for the classifier to be created\n",
    "print(f\"‚è≥ Waiting for classifier creation to complete...\")\n",
    "client.poll_result(response)\n",
    "print(f\"‚úÖ Classifier '{analyzer_id}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Your Document\n",
    "\n",
    "Now, use the classifier to categorize your document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the mixed financial docs PDF file\n",
    "pdf_path = \"../data/mixed_financial_docs.pdf\"\n",
    "print(f\"üìÑ Reading document file: {pdf_path}\")\n",
    "\n",
    "# Begin binary classification operation\n",
    "print(f\"üîç Starting binary classification with classifier '{analyzer_id}'...\")\n",
    "analysis_response = client.begin_analyze_binary(\n",
    "    analyzer_id=analyzer_id,\n",
    "    file_location=pdf_path,\n",
    ")\n",
    "\n",
    "# Wait for analysis completion\n",
    "print(f\"‚è≥ Waiting for document analysis to complete...\")\n",
    "analysis_result = client.poll_result(analysis_response)\n",
    "print(f\"‚úÖ Document analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Classification Results\n",
    "\n",
    "Review the classification results generated for your document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "if analysis_result and \"result\" in analysis_result:\n",
    "    result = analysis_result[\"result\"]\n",
    "    contents = result.get(\"contents\", [])\n",
    "    \n",
    "    if contents:\n",
    "        first_content = contents[0]\n",
    "        \n",
    "        # Display classification results from segments\n",
    "        segments = first_content.get(\"segments\", [])\n",
    "        if segments:\n",
    "            print(\"\\nüìä Classification Results:\")\n",
    "            print(\"=\" * 50)\n",
    "            for idx, segment in enumerate(segments, 1):\n",
    "                print(f\"\\nSegment {idx}:\")\n",
    "                print(f\"   Category: {segment.get('category', 'N/A')}\")\n",
    "                print(f\"   Start Page: {segment.get('startPageNumber', 'N/A')}\")\n",
    "                print(f\"   End Page: {segment.get('endPageNumber', 'N/A')}\")\n",
    "                print(f\"   Segment ID: {segment.get('segmentId', 'N/A')}\")\n",
    "            print(\"=\" * 50)\n",
    "    else:\n",
    "        print(\"No contents available in analysis result\")\n",
    "else:\n",
    "    print(\"No analysis result available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Classification Results\n",
    "The classification result is saved to a JSON file for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the analysis result to a file\n",
    "saved_file_path = save_json_to_file(analysis_result, filename_prefix=\"classification_get_result\")\n",
    "# Print the full analysis result as a JSON string\n",
    "print(json.dumps(analysis_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the created analyzer \n",
    "After the demo completes, the classifier is automatically deleted to prevent resource accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the created classifier\n",
    "print(f\"üóëÔ∏è  Deleting classifier '{analyzer_id}'...\")\n",
    "client.delete_analyzer(analyzer_id=analyzer_id)\n",
    "print(f\"‚úÖ Classifier '{analyzer_id}' deleted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Analyzer (Advanced)\n",
    "\n",
    "Create a custom analyzer to extract specific fields from documents.\n",
    "This example extracts common fields from loan application documents and generates document excerpts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique analyzer ID for loan applications\n",
    "loan_analyzer_id = f\"notebooks_sample_loan_analyzer_{int(time.time())}\"\n",
    "\n",
    "# Define custom analyzer as a dictionary\n",
    "custom_analyzer = {\n",
    "    \"baseAnalyzerId\": \"prebuilt-document\",\n",
    "    \"description\": \"Loan application analyzer - extracts key information from loan applications\",\n",
    "    \"config\": {\n",
    "        \"returnDetails\": True,\n",
    "        \"enableLayout\": True,\n",
    "        \"enableFormula\": False,\n",
    "        \"estimateFieldSourceAndConfidence\": True\n",
    "    },\n",
    "    \"fieldSchema\": {\n",
    "        \"fields\": {\n",
    "            \"ApplicationDate\": {\n",
    "                \"type\": \"date\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The date when the loan application was submitted.\"\n",
    "            },\n",
    "            \"ApplicantName\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"Full name of the loan applicant or company.\"\n",
    "            },\n",
    "            \"LoanAmountRequested\": {\n",
    "                \"type\": \"number\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The total loan amount requested by the applicant.\"\n",
    "            },\n",
    "            \"LoanPurpose\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The stated purpose or reason for the loan.\"\n",
    "            },\n",
    "            \"CreditScore\": {\n",
    "                \"type\": \"number\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"Credit score of the applicant, if available.\"\n",
    "            },\n",
    "            \"Summary\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"A brief summary overview of the loan application details.\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"models\": {\"completion\": \"gpt-4.1\"},\n",
    "    \"tags\": {\"demo\": \"loan-application\"}\n",
    "}\n",
    "\n",
    "# Create the custom analyzer\n",
    "print(f\"üîß Creating custom analyzer '{loan_analyzer_id}'...\")\n",
    "response = client.begin_create_analyzer(\n",
    "    analyzer_id=loan_analyzer_id,\n",
    "    analyzer_template=custom_analyzer,\n",
    ")\n",
    "client.poll_result(response)\n",
    "print(f\"‚úÖ Analyzer '{loan_analyzer_id}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Enhanced Classifier with Custom Analyzer\n",
    "\n",
    "Now create a new classifier that uses the prebuilt invoice analyzer for invoices and the custom analyzer for loan application documents.\n",
    "This combines document classification with field extraction in one operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique enhanced classifier ID\n",
    "enhanced_classifier_id = f\"notebooks_sample_enhanced_classifier_{int(time.time())}\"\n",
    "\n",
    "# Define enhanced classifier with custom analyzer for loan applications\n",
    "enhanced_analyzer = {\n",
    "    \"baseAnalyzerId\": \"prebuilt-document\",\n",
    "    \"description\": f\"Enhanced classifier with custom loan analyzer: {enhanced_classifier_id}\",\n",
    "    \"config\": {\n",
    "        \"returnDetails\": True,\n",
    "        \"enableSegment\": True,\n",
    "        \"contentCategories\": {\n",
    "            \"Loan application\": {\n",
    "                \"description\": \"Documents submitted by individuals or businesses to request funding, typically including personal or business details, financial history, loan amount, purpose, and supporting documentation.\",\n",
    "                \"analyzerId\": loan_analyzer_id  # Use the custom loan analyzer\n",
    "            },\n",
    "            \"Invoice\": {\n",
    "                \"description\": \"Billing documents issued by sellers or service providers to request payment for goods or services, detailing items, prices, taxes, totals, and payment terms.\"\n",
    "            },\n",
    "            \"Bank_Statement\": {\n",
    "                \"description\": \"Official statements issued by banks that summarize account activity over a period, including deposits, withdrawals, fees, and balances.\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"models\": {\"completion\": \"gpt-4.1\"},\n",
    "    \"tags\": {\"demo_type\": \"enhanced_classification\"}\n",
    "}\n",
    "\n",
    "# Create the enhanced classifier\n",
    "print(f\"üîß Creating enhanced classifier '{enhanced_classifier_id}'...\")\n",
    "response = client.begin_create_analyzer(\n",
    "    analyzer_id=enhanced_classifier_id,\n",
    "    analyzer_template=enhanced_analyzer,\n",
    ")\n",
    "\n",
    "# Wait for the classifier to be created\n",
    "print(f\"‚è≥ Waiting for classifier creation to complete...\")\n",
    "client.poll_result(response)\n",
    "print(f\"‚úÖ Enhanced classifier '{enhanced_classifier_id}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Document with Enhanced Classifier\n",
    "\n",
    "Process the document again using the enhanced classifier.\n",
    "Invoices and loan applications will now have additional fields extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../data/mixed_financial_docs.pdf\"\n",
    "print(f\"üìÑ Reading document file: {pdf_path}\")\n",
    "\n",
    "# Begin binary classification operation with enhanced classifier\n",
    "print(f\"üîç Starting binary classification with enhanced classifier '{enhanced_classifier_id}'...\")\n",
    "enhanced_analysis_response = client.begin_analyze_binary(\n",
    "    analyzer_id=enhanced_classifier_id,\n",
    "    file_location=pdf_path,\n",
    ")\n",
    "\n",
    "# Wait for classification completion\n",
    "print(f\"‚è≥ Waiting for classification to complete...\")\n",
    "enhanced_analysis_result = client.poll_result(enhanced_analysis_response)\n",
    "print(f\"‚úÖ Classification completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Enhanced Results with Extracted Fields\n",
    "\n",
    "Review the classification results alongside extracted fields from loan application documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display enhanced classification results\n",
    "if enhanced_analysis_result and \"result\" in enhanced_analysis_result:\n",
    "    result = enhanced_analysis_result[\"result\"]\n",
    "    contents = result.get(\"contents\", [])\n",
    "    \n",
    "    if contents:\n",
    "        print(\"\\nüìä Enhanced Classification Results with Field Extraction:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for idx, content_item in enumerate(contents, 1):\n",
    "            print(f\"\\nüîñ Segment {idx}:\")\n",
    "            print(f\"   Category: {content_item.get('category', 'N/A')}\")\n",
    "            print(f\"   Pages: {content_item.get('startPageNumber', 'N/A')} - {content_item.get('endPageNumber', 'N/A')}\")\n",
    "            \n",
    "            # Display extracted fields if available\n",
    "            fields = content_item.get(\"fields\", {})\n",
    "            if fields:\n",
    "                print(f\"\\n   üìã Extracted Fields:\")\n",
    "                for field_name, field_value in fields.items():\n",
    "                    field_type = field_value.get(\"type\")\n",
    "                    if field_type == \"string\":\n",
    "                        print(f\"      ‚Ä¢ {field_name}: {field_value.get('valueString')}\")\n",
    "                    elif field_type == \"number\":\n",
    "                        print(f\"      ‚Ä¢ {field_name}: {field_value.get('valueNumber')}\")\n",
    "                    elif field_type == \"date\":\n",
    "                        print(f\"      ‚Ä¢ {field_name}: {field_value.get('valueDate')}\")\n",
    "            else:\n",
    "                print(f\"   (No custom fields extracted for this category)\")\n",
    "        \n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        \n",
    "        # Display document information for the first segment\n",
    "        first_content = contents[0]\n",
    "        if first_content.get(\"kind\") == \"document\":\n",
    "            print(f\"\\nüìö Document Information:\")\n",
    "            pages = first_content.get(\"pages\")\n",
    "            if pages:\n",
    "                print(f\"Total pages in document: {len(pages)}\")\n",
    "                unit = first_content.get(\"unit\", \"units\")\n",
    "                print(f\"Page dimensions: {pages[0].get('width')} x {pages[0].get('height')} {unit}\")\n",
    "    else:\n",
    "        print(\"No contents available in enhanced analysis result\")\n",
    "else:\n",
    "    print(\"No enhanced analysis result available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Classification Results\n",
    "The classification result is saved to a JSON file for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the enhanced analysis result to a file\n",
    "saved_file_path = save_json_to_file(enhanced_analysis_result, filename_prefix=\"enhanced_classification_get_result\")\n",
    "# Print the full analysis result as a JSON string\n",
    "print(json.dumps(enhanced_analysis_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the created analyzer\n",
    "After the demo completes, the analyzer is automatically deleted to prevent resource accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the custom loan analyzer\n",
    "print(f\"üóëÔ∏è  Deleting analyzer '{loan_analyzer_id}'...\")\n",
    "client.delete_analyzer(analyzer_id=loan_analyzer_id)\n",
    "print(f\"‚úÖ Analyzer '{loan_analyzer_id}' deleted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the created classifier\n",
    "After the demo completes, the classifier is automatically deleted to prevent resource accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the enhanced classifier\n",
    "print(f\"üóëÔ∏è  Deleting classifier '{enhanced_classifier_id}'...\")\n",
    "client.delete_analyzer(analyzer_id=enhanced_classifier_id)\n",
    "print(f\"‚úÖ Classifier '{enhanced_classifier_id}' deleted successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
